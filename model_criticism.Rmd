
```{r model-criticism-setup, include=FALSE, eval=TRUE, cache=FALSE}
knitr::opts_chunk$set(eval=T, echo=T)

kable_df = function(data, digits=3, ...) {
  kableExtra::kable(
    data,
    digits = digits,
    format = 'html',
    booktabs = T,
    # longtable = F,
    linesep = "",
    ...,
  ) %>% 
    kableExtra::kable_styling(full_width = F)
}

perc = function(x, digits = 1) paste(rnd(x*100, digits = digits), '%')
```

# Model Criticism

It isn't enough to simply fit a particular model, we must also ask how well it matches the data under study, if it can predict well on new data, where it fails, and more.

## Model Fit
### Standard linear model

In the basic regression setting we and think of model fit in terms of a statistical result, or in terms of the match between our model predictions and the observed target values.

#### Statistical

In a standard linear model we can compare a model where there are no covariates to the model we care about, which may have many.  This is an almost useless test, but the results are typically reported.  Let's think about it conceptually.

$$\textrm{Total Variance} = \textrm{Model Explained Variance} + \textrm{Residual Variance}$$

So the variability in our target (TV) can be decomposed into that which we can explain with the predictor variables (MEV), and everything else that is not in our model (RV). If we have nothing in the model, then TV = RV.

Let's revisit the summary of our model.  Note the *F-statistic*.

```{r model-summary-rev}
happy_model_base_sum
```


The standard F statistic can be calculated as follows, where $p$ is the number of predictors[^ftest]:

$$F = \frac{MV/p}{RV/(N-p-1)}$$
Conceptually it is a ratio of average squared variance to average unexplained variance. We can see this more explicitly as follows, where each predictor's contribution to the total variance is provided in the `Sum Sq` column. 

```{r happy-anova}
anova(happy_model_base)
```

If we add those together and use our formula above we get:

$$F = \frac{366.62/3}{160.653/407} = 309.6$$
Which is what is reported in the summary of the model. And the p-value is just `pf(309.6, 3, 407, lower = FALSE)`, whose values can be extracted from the summary object.

```{r f-stat}
happy_model_base_sum$fstatistic
pf(309.6, 3, 407, lower.tail = FALSE)
```

Because the F-value is so large and p-value so small, the printed result doesn't give us the actual p-value. So let's demonstrate again with a worse model.

```{r f-stat-redux}
f_test = lm(happiness_score ~ generosity, happy)

summary(f_test)

pf(8.78, 1, 533, lower.tail = FALSE)
```


We can make this F-test more explicit by actually fitting a null model and making the comparison.  The following will provide the same result as before. We make sure to use the same data as in the original model, since there are missing values for some covariates.

```{r null-vs-realistic}
happy_model_null = lm(happiness_score ~ 1, data = model.frame(happy_model_base)) 
anova(happy_model_null, happy_model_base)
```

In this case our F statistic generalizes to the following, where $\textrm{Model}_1$ is the simpler model and $p$ now refers to the total number of parameters estimated (i.e. same as before + 1 for the intercept)

$$F = \frac{(\textrm{Model}_2\ RV - \textrm{Model}_1\ RV)/(p_2 - p_1)}{\textrm{Model}_2\ RV/(N-p_2-1)}$$

From the previous results, we can perform the necessary arithmetic based on this formula to get the F statistic.

```{r happy-anova-rev}
((527.27 - 160.65)/3) / (160.65/407)
```

#### $R^2$

This statistical result is mostly a straw man type of test- who actually cares if our model does statistically better than a model with nothing in it?  Let's turn instead to a different concept- the amount of variance of the target variable that is explained by our predictors.  For the standard linear model setting, this statistic is called *R-squared* ($R^2$).

Going back to our previous notions, $R^2$ is just:

$$R^2 =\textrm{Model Explained Variance}/\textrm{Total Variance}$$

This too is reported by default in our summary printout.

```{r happy-r2}
happy_model_base_sum
```

With our values from before for model and total variance, we can calculate it ourselves.

```{r r2-calc}
366.62 / 527.27
```

Here is another way.  Let's get the model predictions, and see how well they correlate with the target.

```{r r2-as-cor}
predictions = predict(happy_model_base)
target = happy_model_base$model$happiness_score
rho = cor(predictions, target)
rho
rho^2
```

Now you can see why it's called $R^2$.  It is the squared Pearson $r$ of the model expected value and the observed target variable.


##### Adjustment

One problem with $R^2$ is that it always goes up, no matter what nonsense you add to a model.  This is why we have an *adjusted $R^2$* that attempts to balance the sample size and model complexity.  For very large data and/or simpler models, the difference is negligible. But you should always report the adjusted $R^2$, as the default $R^2$ is actually upwardly biased and doesn't account for additional model complexity[^biasR2].


### Beyond OLS

People love $R^2$, so much that they will report it wherever they can, even coming up with things like 'Pseudo-$R^2$' when it proves difficult. However, outside of the OLS setting where we assume a normal distribution as the underlying data-generating mechanism, $R^2$ has little application, and so is not very useful. In some sense, for any numeric target variable we can ask how well our predictions correlate with the observed target values, but the notion of 'variance explained' doesn't easily follow us. For example, for other distributions the estimated variance is a function of the mean (e.g. our Poisson example), and so isn't constant. In other settings we have multiple sources of residual variance, and some sources  where it's not clear whether the variance should be considered as part of the model explained variance or residual variance.  For categorical targets it becomes even doesn't apply

At least for GLM for non-normal distributions, we can work with *deviance*, which is similar to the residual sum of squares in the OLS setting.  We can get a 'deviance explained' using the following approach:

1. Fit a null model, i.e. intercept only. This gives the total deviance (`tot_dev`).
2. Fit the desired model. This provides the model deviance, which will be reduced (`model_dev`)
3. Calculate $\frac{\textrm{tot_dev} -\textrm{model_dev}}{\textrm{tot_dev}}$

But this value doesn't really behave in the same manner as $R^2$.  For one, it can actually go down for a more complex model, and there is no standard adjustment, neither of which is the case with $R^2$ for the standard linear model.  At most this can serve as an approximation.  For more complicated settings you will have to rely on other means to determine model fit.

### Classification
#### Accuracy and other metrics

## Model Assumptions

There are quite a few assumptions for the standard linear model that we could talk about, but I'll focus on just a handful, ordered roughly in terms of the severity of violation. 

- Correct model
- Heteroscedasticity
- Independence of observations
- Normality

These concern bias (the first), accurate inference (most of the rest), or other statistical concepts (efficiency, consistency).  The issue with most of the assumptions you learn about is that they mostly just apply to the OLS setting.  Moreover, you can meet all the assumptions you want and still have a crappy model. Practically speaking, the effects on inference usually aren't large enough to matter, as most shouldn't be making any important decision based on a p-value, or slight differences in the boundaries of an interval.  Even then, at least for OLS settings, the solutions are often easy, for example, to obtain standard errors, or are mostly overcome by having a large amount of data.

Still, the diagnostic tools can provide clues to model failure.  As before, visualization will aid us here. 

```{r diag-plot-code, eval=F}
library(ggfortify)

autoplot(happy_model_base)
```

```{r diag-plot-show, echo=FALSE, fig.height=4}
library(ggfortify)

autoplot(
  happy_model_base,
  which = 1:2,
  colour = '#ff550040',
  smooth.colour = '#00aaff80',
  label.colour = 'gray50'
) +
  theme_clean()
```


The first plot shows the spread of the residuals vs. the model estimated values. By default, the three most extreme observations are noted.  In this plot we are looking for a lack of any conspicuous pattern, e.g. a fanning out to one side or butterfly shape.  If the variance was dependent on some of the model estimated values, we have a couple options: 
- Use a model that does not assume constant variance
- Add complexity to the model to better capture more extreme observations
- Change the assumed distribution

In this case we have it about as good as it gets.  The second plot regards the normality of the residuals.  If they are normally distributed, they would fall along the dotted line. Again, in practical application this is about as good as you're going to get.   


Another plot we can use is to note the predictions vs. the observed values, and this sort of plot would be appropriate for any model. Here I show this both as a scatterplot and a density plot.  With the first, the closer the result is to a line the better, with the latter, we can more adequately see what the model is predicting in relation to the observed values. In this case, while we're doing well, one limitation of the model is that it does not have as much spread as target, and so is not capturing the more extreme values.

```{r pp_check_base, echo=F, out.width='1200px', out.height='400px'}
init = data.frame(
  predictions = predict(happy_model_base),
  target = happy_model_base$model$happiness_score
) 

p_dense = init %>% 
  pivot_longer(everything(), names_to = 'result') %>% 
  ggplot() +
  geom_density(aes(x = value, color = result, fill = result), alpha = .25) +
  scico::scale_color_scico_d(end = .5, aesthetics = c('color', 'fill')) +
  guides(fill = guide_legend(title = ""), color = 'none') +
  theme_clean() +
  theme(
    legend.title = element_text(size = 4),
    legend.key.size = unit(2, 'mm')
  )

p_scatter = init %>% 
  ggplot() +
  geom_point(aes(x = predictions, y = target), alpha = .25) +
  lims(x = c(2, 8), y = c(2, 8)) +
  theme_clean()

# still an issue with lack 
p_scatter +
  p_dense +
  plot_layout(widths = c(3, 1)) * theme_clean()
# gridExtra::grid.arrange(p_scatter, p_dense, layout_matrix = matrix(c(1,1,1,2,2), nrow = 1 ))
```

Beyond OLS, assumptions may change, are more difficult to check, and guarantees are harder to come by.  The primary one- that you have an adequate and sufficiently complex model- still remains the most vital.  It is important to note that these assumptions regard inference, not predictive capabilities.  In addition, in many modeling scenarios we will actually induce bias to have more predictive capacity. In such settings statistical tests are of less importance, and there often may not even be an obvious test to use.  Typically we will still have some means to get the interval estimates for weights or predictions though.  



## Predictive Performance

While we can gauge predictive performance to some extent with a metric like $R^2$ in the standard linear model case, even then it almost certainly an optimistic viewpoint, and adjusted $R^2$ doesn't really deal with the underlying issue.  What is the problem?  The concern is that we are judging model performance on the very data it was fit to. Any potential deviation to the underlying data would certainly result in a different result for $R^2$, accuracy, or any metric we choose to look at.

So the better estimate of how the model is doing is to observe performance on data it hasn't seen.  This data goes by different names- *test set*, *validation set*, *holdout sample*, etc., but the basic idea is that we use some data that wasn't used in model fitting to assess performance.  We can do this in any data situation by randomly splitting into a data set for training the model, and one used for testing the model's performance.

```{r train-test}
library(tidymodels)

set.seed(1212)

happy_split = initial_split(happy, prop = 0.75)

happy_train = training(happy_split)
happy_test  = testing(happy_split) %>% drop_na()

happy_model_train = lm(
  happiness_score ~ democratic_quality + generosity + log_gdp_per_capita,
  data = happy_train
)

predictions = predict(happy_model_train, newdata = happy_test)
```

Comparing our loss on training and test (i.e. RMSE), we can see the loss is greater on the test set.  

```{r train-test-loss, echo=FALSE}
tibble(
  RMSE_train = sqrt(crossprod(residuals(happy_model_train))/nrow(happy_train)),
  RMSE_test  = sqrt(crossprod(predictions - happy_test$happiness_score)/nrow(happy_test))[,1],
  `% increase` = rnd(100*(RMSE_test/RMSE_train - 1), 1)
  ) %>% 
  kable_df()
```

For a more accurate assessment of test error, we'd need to take an average over several test sets, an approach known as *cross-validation*, something we'll talk more about later.

In general, we may do okay in scenarios where the model is simple and uses a lot of data, but even then we may find a notable increase in test error relative to training error.  For more complex models and/or with less data, the difference in training vs. test could be quite significant.  


## Model Comparison

Up until now the focus has been entirely on one model.  However, if you're trying to learn something new, you'll almost always want to have multiple plausible models to explore, rather than just confirming what you think you already know.  This can be as simple as starting with a base model and adding complexity to it, but it could also be pitting fundamentally different models against one another.  

A notable problem is that complex models should always do better than simple ones.  The question then becomes if they are doing notably better given the additional complexity.

### Example: Additional covariates

A starting point for adding model complexity is simply adding more covariates.  Let's add life expectancy and a yearly trend.  To make this model comparable to our baseline model, they need to be fit to the same data, and life expectancy has a couple missing values the others do not.  So we'll start with some data processing. I will start by standardizing some of the variables, and making year start at zero, which will represent 2008, and finally dropping missing values.  Refer to our previous section on [transforming variables][numeric variables] if you want to.

```{r happy-data-proc}
happy_processed = happy %>% 
  select(
    country,
    year,
    happiness_score,
    democratic_quality,
    generosity,
    healthy_life_expectancy_at_birth,
    log_gdp_per_capita
  ) %>% 
  mutate_at(vars(happiness_score:healthy_life_expectancy_at_birth),
            function(x) scale(x)[, 1]) %>% 
  mutate(year = year - min(year)) %>% 
  drop_na()
```


Now let's start with our baseline model again.

```{r happy-model-base}
happy_model_base = lm(
  happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, 
  data = happy_processed
)

summary(happy_model_base)
```

We can see that moving one standard deviation on democratic quality and generosity leads to similar standard deviation increases in happiness.  Moving 10 percentage points in GDP would lead to less than .1 standard deviation increase in happiness.

Now we add our life expectancy and yearly trend.

```{r happy_model_more}
happy_model_more = lm(
  happiness_score ~ democratic_quality + generosity + log_gdp_per_capita + healthy_life_expectancy_at_birth + year,
  data = happy_processed
)

summary(happy_model_more)
```

Here it would seem that life expectancy has a notable effect on happiness (shocker), but the yearly trend, while negative, is not statistically notable.  In addition, the democratic effect is no longer significant, as it would seem that it's contribution was more due to it's correlation with life expectancy. But the key question is- is this model better?  

The adjusted $R^2$ seems to indicate that we are doing slightly better with this model, but not much.  We can test if the increase is a statistically notable one. [Recall previously][Statistical] when we compared our model versus a null model to obtain a statistical test of model fit.  Since these models are *nested*, i.e. one is a simpler form of the other, we can use the more general approach we depicted to compare these models.  This ANOVA, or analysis of variance test, is essentially comparing whether the residual sum of squares (i.e. the loss) is less for one model vs. the other. In many settings it is often called a *likelihood ratio test*.

```{r anova-comparison}
anova(happy_model_base, happy_model_more)
```

The `Df` from the test denotes that we have two additional parameters, i.e. coefficients, in the more complex model. But the main thing to note is whether the model statistically reduces the RSS. So we see that this is a statistically notable improvement as well.  

I actually do not like this test though. It requires nested models, which is either not the case or can be hard to determine, and ignores various aspects of uncertainty in parameter estimates.  An approach that works in many settings is to compare AIC. AIC is a value based on the likelihood for a given model, but which adds a penalty for complexity, since otherwise any more complex model would result in a larger likelihood (or in this case, smaller negative likelihood).  In the following, $\mathcal{L}$ is the likelihood, and $\mathcal{P}$ is the number of parameters estimated for the model.

$$AIC = -2 ( \ln (\mathcal{L})) + 2 \mathcal{P}$$
```{r AIC}
AIC(happy_model_base)
```

The value itself is meaningless until we compare models, in which case the lower value is the better model.  With AIC, we don't have to have nested models, so that's a plus over the statistical test.

```{r AIC-compare}
AIC(happy_model_base, happy_model_more)
```

Again, our new model works better.  However, this still may miss out on some uncertainty in the models. To try and capture this, I will calculate interval estimates for the adjusted $R^2$ via *bootstrapping*, and then calculate an interval for their difference.  The details are beyond what I want to delve into here, but the gist is we just want a confidence interval for the difference in adjusted $R^2$.

```{r boot-$R^2$, echo=FALSE}
library(boot)

f_base = happiness_score ~ democratic_quality + generosity + log_gdp_per_capita
f_more = happiness_score ~ democratic_quality + generosity + log_gdp_per_capita + healthy_life_expectancy_at_birth + year

r2_base <- boot(
  happy_processed,
  function(data, indices)
    summary(lm(f_base, data[indices, ])
    )$adj.r.squared, 
  R = 1000
)

r2_more <- boot(
  happy_processed,
  function(data, indices)
    summary(lm(f_more, data[indices, ])
    )$adj.r.squared, 
  R = 1000
)

ci = rbind(
  quantile(r2_base$t, c(0.025,0.975)),
  quantile(r2_more$t, c(0.025,0.975))
)

tibble(
  model = c('base', 'more'), 
  r2 = c(r2_base$t0, r2_more$t0),
) %>% 
  bind_cols(as_tibble(ci)) %>% 
  kable_df()
```

```{r boot-$R^2$-diff, echo=FALSE}
diff_in_r2 = data.matrix(quantile(r2_more$t0 - r2_base$t, c(0.025,0.975)))

colnames(diff_in_r2) = 'Difference in $R^2$'

diff_in_r2 %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  kable_df()
```





### Example: Interactions

Let's start by adding interactions to our model.  Interactions allow the relationship of a predictor variable and target to vary depending on the values of another covariate.  To keep things simple, we'll add a single interaction to start- I will interact democratic quality with life expectancy.

```{r happy-interact}
happy_model_interact = lm(
  happiness_score ~ democratic_quality + generosity + log_gdp_per_capita + 
    healthy_life_expectancy_at_birth +
    democratic_quality:healthy_life_expectancy_at_birth,
  data = happy_processed
  )

summary(happy_model_interact)
```

The coefficient interpretation for variables in the interaction change.  They now only describe the effect when the variable they interact with is zero (or the reference group if it's categorical). So democratic quality has a slight positive but not statistically notable effect at the mean of life expectancy.  However, this effect increases by `r rnd(coef(happy_model_interact)['democratic_quality:healthy_life_expectancy_at_birth'], 2)` when life expectancy increases by 1 (i.e. 1 standard deviation).  The same interpretation goes for life expectancy.  It's coefficient is when democratic quality is at it's mean, and the interaction term is interpreted identically.

It seems most people (including journal reviewers) seem to have trouble understanding interactions if you just report them in a table. Furthermore, beyond the standard linear model with non-normal distributions, the coefficient for the interaction term doesn't even have the same meaning. But you know what works in every setting? Visualization!

Let's use ggeffects again. We'll plot the effect of democratic quality at the mean of life expectancy, and at one standard deviation below and above.  Since we already standardized it, this is even easier.

```{r vis-interact}
library(ggeffects)

plot(
  ggpredict(
    happy_model_interact,
    terms = c('democratic_quality', 'healthy_life_expectancy_at_birth[-1, 0, 1]')
  )
)
```

We seem to have discovered something interesting here!  Democratic quality only has a positive effect for those countries with a high life expectancy, i.e. that are already in a good place in general.  It may even be negative in countries in the contrary case. While this has to be taken with a lot of caution, it shows how exploring interactions can be fun and surprising!

Another way to plot interactions in which the variables are continuous is with a contour plot similar to this.  Here we don't have to pick arbitrary values to plot against, just note the predictions at all values of the covariates in question.

```{r contour-plot, echo=FALSE, fig.retina=5}
# library(mgcv)
# 
# happy_model_interact_ = gam(
#   happiness_score ~ democratic_quality + generosity + log_gdp_per_capita + 
#     healthy_life_expectancy_at_birth +
#     democratic_quality:healthy_life_expectancy_at_birth,
#   data = happy_processed
#   )
# 
# plot_gam_2d(
#   happy_model_interact_,
#   democratic_quality,
#   healthy_life_expectancy_at_birth,
#   n = 500,
#   palette = 'acton'
# ) +
#   guides(fill = guide_legend(title = "Expected Happiness")) +
#   theme(
#     axis.title.x = element_text(hjust = .5),
#     axis.title.y = element_text(vjust = .5),
#     legend.title = element_text(size = 8)
#   )
# 
# ggsave('img/interaction_2d.png') # svg was 55mb

# plot_gam_3d(happy_model_interact_, democratic_quality, healthy_life_expectancy_at_birth)

knitr::include_graphics('img/interaction_2d.png')
```

We see the the lowest expected happiness based on the model is with high democratic quality and low life expectancy. The best case scenario is to be high on both.


Here is our model comparison for all three models with AIC.

```{r anova-3}
AIC(happy_model_base, happy_model_more, happy_model_interact)
```

Looks like our interaction model is winning.


### Example: Additive Models

*Generalized additive models* allow our predictors to have a *wiggly* relationship with the target variable.  For more information, see [this document](https://m-clark.github.io/generalized-additive-models/), but for our purposes, that's all you really need to know. We will use the base R mgcv package because it is awesome and you don't need to install anything.  In this case, we'll allow all the covariates to have a nonlinear relationship, and we denote this with the `s()` syntax.

```{r happy-gam}
library(mgcv)

happy_model_gam = gam(
  happiness_score ~ s(democratic_quality) + s(generosity) + s(log_gdp_per_capita) + 
    s(healthy_life_expectancy_at_birth),
  data = happy_processed
)

summary(happy_model_gam)
```

The first thing you may notice is that there are no regression coefficients. This is because the effect of any of these predictors depends on their value, so trying to assess it by a single value would be problematic at best. You can guess what will help us interpret this...

```{r happy-gam-vis, eval=FALSE}
library(mgcViz)

plot.gamViz(happy_model_gam, allTerms = T)
```

```{r happy-gam-vis-show, echo=FALSE}
visibly::plot_gam(happy_model_gam)
```

Here is a brief summary of interpretation.  We generally don't have to worry about small wiggles.

- `democratic_quality`: Effect is most notable (positive and strong) for higher values. Negligible otherwise.
- `generosity`: Effect seems seems strongly positive, but only for lower values of generosity.
- `life_expectancy`: Effect is positive, but only if the country is around the mean or higher.
- `log GDP per capita`: Effect is mostly positive, but may depend on other factors not included in the model.

In terms of general model fit, the `Scale est.` is the same as the residual standard error (squared) in the other models, and is a notably lower than even the model with the interaction (`r happy_model_gam$sig2` vs. `r summary(happy_model_interact)$sig^2`). We can also see that the adjusted $R^2$ is higher as well (`r summary(happy_model_gam)$r.sq` vs. `r summary(happy_model_interact)$adj`.

Let's check our AIC now to see which model wins. It's pretty clear our wiggly model is the winner, even with the added complexity.  Note that even though we used a different function for the GAM model, the AIC is still comparable.

```{r AIC-all}
AIC(
  happy_model_base,
  happy_model_more,
  happy_model_interact,
  happy_model_gam
)
```

Performance vs. complexity


## Model Averaging

Have you ever suffered from choice overload?  Many folks who seek to understand some phenomenon via modeling do so. There are plenty of choices due to data processing, but then there may be many models to consider as well, and should be if you're doing things correctly.  But you know what? You don't have to pick a best.

Model averaging is a common technique in the Bayesian world and also with some applications of machine learning, but not as widely applied elsewhere, even though it could be. As an example if we (inversely) weight models by the AIC, we can get an average parameter that favors the better models, while not ignoring the lesser models if they aren't notably poorer.  People will use such an approach to get model averaged effects (i.e. coefficients) or predictions.  In our setting, the GAM is doing so much better, that it's weight would basically be 1.0 and zero for the others. So the model averaged predictions would be almost identical to the GAM predictions.

```{r aic-average, echo=FALSE}
model_list = list(
  happy_model_base,
  happy_model_more,
  happy_model_interact,
  happy_model_gam
)

aic_weights = AIC(
  happy_model_base,
  happy_model_more,
  happy_model_interact,
  happy_model_gam
) %>% 
  as_tibble(rownames = 'model') %>% 
  mutate(
    AICc = map_dbl(model_list, AICcmodavg::AICc),
    deltaAICc = AICc - min(AICc),
    `Rel. Like.` = exp(-.5*deltaAICc),
    weight = `Rel. Like.` / sum(`Rel. Like.`)
  )

aic_weights %>% 
  kable_df(digits = 3)
# 
# model_avg = list(
#   base = happy_model_base,
#   more = happy_model_more,
#   interact = happy_model_interact,
#   gam = happy_model_gam
# ) %>% 
#   map_df(fitted) %>%
#   mutate(avg = rowSums(. * aic_weights$weight))

# model_avg %>% 
#   head() %>% 
#   kable_df()
```

```{r avgr2, echo=FALSE}
# avg_r2 = cor(model_avg$avg, happy_model_base$model$happiness_score)^2
```


## Model Criticism Summary

## Exercises


[^ftest]: This is often depicted in different ways depending on the text and context. In our depiction, the focus is on the F statistic as a ratio of explained variance to unexplained variance, which naturally relates to the $R^2$ statistic.

[^biasR2]: The adjusted $R^2$ does not correct the bias.  It only accounts for complexity.