
```{r model-criticism-setup, include=FALSE, eval=TRUE, cache=FALSE}

```

# Model Criticism

<div class="" style="text-align: center;">
<!-- because evidently icon didn't see this one -->
<i class = 'fas fa-tools fa-5x'></i>
</div>



It isn't enough to simply fit a particular model, we must also ask how well it matches the data under study, if it can predict well on new data, where it fails, and more.  In the following we will discuss how we can better understand our model and its limitations.

## Model Fit

### Standard linear model

In the basic regression setting we can think of model fit in terms of a statistical result, or in terms of the match between our model predictions and the observed target values.  The former provides an inferential perspective, but as we will see, is limited. The latter regards a more practical result, and may provide a more nuanced or different conclusion.

#### Statistical assessment

In a standard linear model we can compare a model where there are no covariates vs. the model we actually care about, which may have many predictor variables.  This is an almost useless test, but the results are typically reported both in standard output and academic presentation.  Let's think about it conceptually- how does the variability in our target break down?

$$\textrm{Total Variance} = \textrm{Model Explained Variance} + \textrm{Residual Variance}$$

So the variability in our target (TV) can be decomposed into that which we can explain with the predictor variables (MEV), and everything else that is not in our model (RV). If we have nothing in the model, then TV = RV.

Let's revisit the summary of our model.  Note the *F-statistic*, which represents a statistical test for the model as a whole.

```{r model-summary-rev}
happy_model_base_sum
```


The standard F statistic can be calculated as follows, where $p$ is the number of predictors[^ftest]:

$$F = \frac{MV/p}{RV/(N-p-1)}$$
Conceptually it is a ratio of average squared variance to average unexplained variance. We can see this more explicitly as follows, where each predictor's contribution to the total variance is provided in the `Sum Sq` column. 

```{r happy-anova}
anova(happy_model_base)
```

If we add those together and use our formula above we get:

$$F = \frac{366.62/3}{160.653/407} = 309.6$$
Which is what is reported in the summary of the model. And the p-value is just `pf(309.6, 3, 407, lower = FALSE)`, whose values can be extracted from the summary object.

```{r f-stat}
happy_model_base_sum$fstatistic
pf(309.6, 3, 407, lower.tail = FALSE)
```

Because the F-value is so large and p-value so small, the printed result in the summary doesn't give us the actual p-value. So let's demonstrate again with a worse model, where the p-value will be higher.

```{r f-stat-redux}
f_test = lm(happiness_score ~ generosity, happy)

summary(f_test)

pf(8.78, 1, 533, lower.tail = FALSE)
```


We can make this F-test more explicit by actually fitting a null model and making the comparison.  The following will provide the same result as before. We make sure to use the same data as in the original model, since there are missing values for some covariates.

```{r null-vs-realistic}
happy_model_null = lm(happiness_score ~ 1, data = model.frame(happy_model_base)) 
anova(happy_model_null, happy_model_base)
```

In this case our F statistic generalizes to the following, where $\textrm{Model}_1$ is the simpler model and $p$ now refers to the total number of parameters estimated (i.e. same as before + 1 for the intercept)

$$F = \frac{(\textrm{Model}_2\ \textrm{RV} - \textrm{Model}_1\ \textrm{RV})/(p_2 - p_1)}{\textrm{Model}_2\ \textrm{RV}/(N-p_2-1)}$$

From the previous results, we can perform the necessary arithmetic based on this formula to get the F statistic.

```{r happy-anova-rev}
((527.27 - 160.65)/3) / (160.65/407)
```

#### $R^2$

The statistical result just shown is mostly a straw man type of test- who actually cares if our model does statistically better than a model with nothing in it?  Surely if you don't do better than nothing, then you may need to think more intently about what you are trying to model and how.  But just because you can knock the straw man down, it isn't something to get overly excited about. Let's turn instead to a different concept- the amount of variance of the target variable that is explained by our predictors.  For the standard linear model setting, this statistic is called *R-squared* ($R^2$).

Going back to our previous notions, $R^2$ is just:

$$R^2 =\textrm{Model Explained Variance}/\textrm{Total Variance}$$

This also is reported by default in our summary printout.

```{r happy-r2}
happy_model_base_sum
```

With our values from before for model and total variance, we can calculate it ourselves.

```{r r2-calc}
366.62 / 527.27
```

Here is another way.  Let's get the model predictions, and see how well they correlate with the target.

```{r r2-as-cor}
predictions = predict(happy_model_base)
target = happy_model_base$model$happiness_score
rho = cor(predictions, target)
rho
rho^2
```

Now you can see why it's called $R^2$.  It is the squared Pearson $r$ of the model expected value and the observed target variable.


##### Adjustment

One problem with $R^2$ is that it always goes up, no matter what nonsense you add to a model.  This is why we have an *adjusted $R^2$* that attempts to balance the sample size and model complexity.  For very large data and/or simpler models, the difference is negligible. But you should always report the adjusted $R^2$, as the default $R^2$ is actually upwardly biased and doesn't account for additional model complexity[^biasR2].


### Beyond OLS

People love $R^2$, so much that they will report it wherever they can, even coming up with things like 'Pseudo-$R^2$' when it proves difficult. However, outside of the OLS setting where we assume a normal distribution as the underlying data-generating mechanism, $R^2$ has little application, and so is not very useful. In some sense, for any numeric target variable we can ask how well our predictions correlate with the observed target values, but the notion of 'variance explained' doesn't easily follow us. For example, for other distributions the estimated variance is a function of the mean (e.g. Poisson, Binomial), and so isn't constant. In other settings we have multiple sources of (residual) variance, and some sources  where it's not clear whether the variance should be considered as part of the model explained variance or residual variance.  For categorical targets the notion doesn't really apply very well at all.

At least for GLM for non-normal distributions, we can work with *deviance*, which is similar to the residual sum of squares in the OLS setting.  We can get a 'deviance explained' using the following approach:

1. Fit a null model, i.e. intercept only. This gives the total deviance (`tot_dev`).
2. Fit the desired model. This provides the model unexplained deviance  (`model_dev`)
3. Calculate $\frac{\textrm{tot_dev} -\textrm{model_dev}}{\textrm{tot_dev}}$

But this value doesn't really behave in the same manner as $R^2$.  For one, it can actually go down for a more complex model, and there is no standard adjustment, neither of which is the case with $R^2$ for the standard linear model.  At most this can serve as an approximation.  For more complicated settings you will have to rely on other means to determine model fit.

### Classification

For categorical targets we must think about obtaining predictions that allow us to classify the observations into specific categories.  Not surprisingly, this will require different metrics to assess model performance.

#### Accuracy and other metrics

A very natural starting point is *accuracy*, or what percentage of our predicted class labels match the observed class labels.  However, our model will not spit out a character string, only a number. On the scale of the linear predictor it can be anything, but we will at some point transform it to the probability scale, obtaining a predicted probability for each category.  The class associated with the highest probability is the predicted class.  In the case of binary targets, this is just an <span class="func" style = "">if_else</span> statement for one class `if_else(probability >= .5, 'class A', 'class B')`.

With those predicted labels and the observed labels we create what is commonly called a *confusion matrix*, but would more sanely be called a *classification table*, *prediction table*, or just about any other name one could come up with in the first 10 seconds of trying.  Let's look at the following hypothetical result.


```{r confusionMatrix, echo=FALSE}
t1 = tibble(
  ` ` = c('Predicted = 1', 'Predicted = 0'),
  `Observed = 1` = c(41, 16),
  `Observed = 0` = c(21, 13)
)

t2 = tibble(
  ` ` = c('Predicted = 1', 'Predicted = 0'),
  `Observed = 1` = c('A', 'C'),
  `Observed = 0` = c('B', 'D')
)

kable_df(list(t1, t2), align='cc') %>% 
  kable_styling(full_width = T, bootstrap_options = 'basic')
```

```{r confmat_values, echo=FALSE}
A = 41
B = 21
C = 16
D = 13
# truepos_total = A + C
# pospred_total = A + B
# trueneg_total = B + D
# negpred_total = C + D
Total = A + B + C + D
```

In some cases we predict correctly, in other cases not.  In this 2 x 2 setting we label the cells A through D.  With things in place, consider the following the following nomenclature.

*True Positive*, *False Positive*, *True Negative*, *False Negative*: Above, these are A, B, D, and C respectively.

Now let's see what we can calculate.

*Accuracy*: Number of correct classifications out of all predictions (A + D)/Total. In the above example this would be (41 + 13)/91, about `r round((A+D)/Total,2)*100`%.

*Error Rate*: 1 - Accuracy.

*Sensitivity*: is the proportion of correctly predicted positives to all true positive events: A/(A + C).  In the above example this would be 41/57, about `r round(A/(A+C),2)*100`%. High sensitivity would suggest a low type II error rate (see below), or high statistical power. Also known as *true positive rate*.

*Specificity*: is the proportion of correctly predicted negatives to all true negative events: D/(B + D).  In the above example this would be 13/34, about `r round(D/(B+D),2)*100`%. High specificity would suggest a low type I error rate (see below). Also known as *true negative rate*.

*Positive Predictive Value* (PPV): proportion of true positives of those that are predicted positives: A/(A + B). In the above example this would be 41/62, about `r round(A/(A+B),2)*100`%.

*Negative Predictive Value* (NPV): proportion of true negatives of those that are predicted negative: D/(C + D). In the above example this would be 13/29, about `r round(D/(C+D),2)*100`%.

*Precision*:  See PPV.

*Recall*: See sensitivity.

*Lift*: Ratio of positive predictions given actual positives to the proportion of positive predictions out of the total: (A/(A + C)) / ((A + B)/Total). In the above example this would be (41/(41 + 16))/((41 + 21)/(91)), or `r round((A/(A+C))/((A+B)/Total),2)`.

*F Score* (F1 score): Harmonic mean of precision and recall: 2\*(Precision\*Recall)/(Precision+Recall). In the above example this would be 2\*(.66\*.72)/(.66+.72), about `r round(2*(.66*.72)/(.66+.72),2)`.

*Type I Error Rate* (false positive rate): proportion of true negatives that are incorrectly predicted positive: B/(B+D). In the above example this would be 21/34, about `r round(B/(B+D),2)*100`%.  Also known as *alpha*.

*Type II Error Rate* (false negative rate): proportion of true positives that are incorrectly predicted negative: C/(C+A). In the above example this would be 16/57, about `r round(C/(C+A),2)*100`%. Also known as *beta*.

*False Discovery Rate*: proportion of false positives among all positive predictions: B/(A+B). In the above example this would be 21/62, about `r round(B/(A+B),2)*100`%.  Often used in multiple comparison testing in the context of ANOVA.

*Phi coefficient*: A measure of association: (A\*D - B\*C) / (sqrt((A+C)\*(D+B)\*(A+B)\*(D+C))).  In the above example this would be `r psych::phi(matrix(c(A,C,B,D),2))`.

Several of these may also be produced on a per-class basis when there are more than two classes.  In addition, for multi-class scenarios there are other metrics commonly employed.  In general there are many, many other metrics for confusion matrices, any of which might be useful for your situation, but the above provides a starting point, and is enough for many situations.


## Model Assumptions

There are quite a few assumptions for the standard linear model that we could talk about, but I'll focus on just a handful, ordered roughly in terms of the severity of violation. 

- Correct model
- Heteroscedasticity
- Independence of observations
- Normality

These concern bias (the first), accurate inference (most of the rest), or other statistical concepts (efficiency, consistency).  The issue with most of the assumptions you learn about in your statistics course is that they mostly just apply to the OLS setting.  Moreover, you can meet all the assumptions you want and still have a crappy model. Practically speaking, the effects on inference often aren't large enough to matter in many cases, as we shouldn't be making any important decision based on a p-value, or slight differences in the boundaries of an interval.  Even then, at least for OLS and other simpler settings, the solutions to these issues are often easy, for example, to obtain correct standard errors, or are mostly overcome by having a large amount of data.

Still, the diagnostic tools can provide clues to model failure, and so have utility in that sense.  As before, visualization will aid us here. 

```{r diag-plot-code, eval=F}
library(ggfortify)

autoplot(happy_model_base)
```

```{r diag-plot-show, echo=FALSE, fig.height=4}
library(ggfortify)

autoplot(
  happy_model_base,
  which = 1:2,
  colour = '#ff550040',
  smooth.colour = '#00aaff80',
  label.colour = 'gray50'
) +
  theme_clean()
```


The first plot shows the spread of the residuals vs. the model estimated values. By default, the three most extreme observations are noted.  In this plot we are looking for a lack of any conspicuous pattern, e.g. a fanning out to one side or butterfly shape.  If the variance was dependent on some of the model estimated values, we have a couple options: 

- Use a model that does not assume constant variance
- Add complexity to the model to better capture more extreme observations
- Change the assumed distribution

In this example we have it about as good as it gets.  The second plot regards the normality of the residuals.  If they are normally distributed, they would fall along the dotted line. Again, in practical application this is about as good as you're going to get.  In the following we can see that we have some issues, where predictions are worse at low and high ends, and we may not be capturing some of the tail of the target distribution.

```{r bad_fit, echo=FALSE}
set.seed(1234)
N = 500
x = rnorm(N)
mu = 2 + .25*x - .5*x^2 + rnorm(sd=abs(x), N)
y = mu

bad_fit = lm(y ~ x)

autoplot(
  bad_fit,
  which = 1:2,
  colour = '#ff550040',
  smooth.colour = '#00aaff80',
  label.colour = 'gray50'
) +
  theme_clean()
``` 



Another plot we can use to assess model fit is simply to note the predictions vs. the observed values, and this sort of plot would be appropriate for any model. Here I show this both as a scatterplot and a density plot.  With the first, the closer the result is to a line the better, with the latter, we can more adequately see what the model is predicting in relation to the observed values. In this case, while we're doing well, one limitation of the model is that it does not have as much spread as target, and so is not capturing the more extreme values.

```{r pp_check_base, echo=F, fig.width=8}
init = data.frame(
  predictions = predict(happy_model_base),
  target = happy_model_base$model$happiness_score
) 

p_dense = init %>% 
  pivot_longer(everything(), names_to = 'result') %>% 
  ggplot() +
  geom_density(aes(x = value, color = result, fill = result), alpha = .25) +
  scico::scale_color_scico_d(end = .5, aesthetics = c('color', 'fill')) +
  guides(fill = guide_legend(title = ""), color = 'none') +
  theme_clean() +
  theme(
    legend.title = element_text(size = 4),
    legend.key.size = unit(2, 'mm')
  )

p_scatter = init %>% 
  ggplot() +
  geom_point(aes(x = predictions, y = target), alpha = .25) +
  lims(x = c(2, 8), y = c(2, 8)) +
  theme_clean()

# still an issue with lack 
p_scatter +
  p_dense +
  plot_layout(widths = c(3, 1)) * theme_clean()
# gridExtra::grid.arrange(p_scatter, p_dense, layout_matrix = matrix(c(1,1,1,2,2), nrow = 1 ))
```

Beyond the OLS setting, assumptions may change, are more difficult to check, and guarantees are harder to come by.  The primary one - that you have an adequate and sufficiently complex model - still remains the most vital.  It is important to remember that these assumptions regard inference, not predictive capabilities.  In addition, in many modeling scenarios we will actually induce bias to have more predictive capacity. In such settings statistical tests are of less importance, and there often may not even be an obvious test to use.  Typically we will still have some means to get interval estimates for weights or predictions though.  



## Predictive Performance

While we can gauge predictive performance to some extent with a metric like $R^2$ in the standard linear model case, even then it almost certainly an optimistic viewpoint, and adjusted $R^2$ doesn't really deal with the underlying issue.  What is the problem?  The concern is that we are judging model performance on the very data it was fit to. Any potential deviation to the underlying data would certainly result in a different result for $R^2$, accuracy, or any metric we choose to look at.

So the better estimate of how the model is doing is to observe performance on data it hasn't seen, using a metric that better captures how close we hit the target.  This data goes by different names- *test set*, *validation set*, *holdout sample*, etc., but the basic idea is that we use some data that wasn't used in model fitting to assess performance.  We can do this in any data situation by randomly splitting into a data set for training the model, and one used for testing the model's performance.

```{r train-test}
library(tidymodels)

set.seed(12)

happy_split = initial_split(happy, prop = 0.75)

happy_train = training(happy_split)
happy_test  = testing(happy_split) %>% drop_na()

happy_model_train = lm(
  happiness_score ~ democratic_quality + generosity + log_gdp_per_capita,
  data = happy_train
)

predictions = predict(happy_model_train, newdata = happy_test)
```

Comparing our loss on training and test (i.e. RMSE), we can see the loss is greater on the test set.  You can use a package like <span class="pack" style = "">yardstick</span> to calculate this.

```{r train-test-loss, echo=FALSE}
pita = data.frame(
  observed_train = happy_model_train$model$happiness_score,
  predict_train  = happy_model_train$fitted.values
)

pita2 = data.frame(
  observed_test = happy_test$happiness_score,
  predict_test  = predictions
)


tibble(
  RMSE_train = yardstick::rmse(pita, truth = observed_train, estimate = predict_train)$.estimate,
  RMSE_test = yardstick::rmse(pita2, truth = observed_test, estimate = predict_test)$.estimate,
  `% increase` = rnd(100*(RMSE_test/RMSE_train - 1), 1)
  ) %>% 
  kable_df()
```

While in many settings we could simply report performance metrics from the test set, for a more accurate assessment of test error, we'd do better by taking an average over several test sets, an approach known as *cross-validation*, something we'll talk more about [later][cross-validation].

In general, we may do okay in scenarios where the model is simple and uses a lot of data, but even then we may find a notable increase in test error relative to training error.  For more complex models and/or with less data, the difference in training vs. test could be quite significant.  


## Model Comparison

Up until now the focus has been entirely on one model.  However, if you're trying to learn something new, you'll almost always want to have multiple plausible models to explore, rather than just confirming what you think you already know.  This can be as simple as starting with a baseline model and adding complexity to it, but it could also be pitting fundamentally different theoretical models against one another.  

A notable problem is that complex models should always do better than simple ones.  The question often then becomes if they are doing notably better given the additional complexity.  So we'll need some way to compare models in a way that takes the complexity of the model into account.

### Example: Additional covariates

A starting point for adding model complexity is simply adding more covariates.  Let's add life expectancy and a yearly trend to our happiness model.  To make this model comparable to our baseline model, they need to be fit to the same data, and life expectancy has a couple missing values the others do not.  So we'll start with some data processing. I will start by standardizing some of the variables, and making year start at zero, which will represent 2008, and finally dropping missing values.  Refer to our previous section on [transforming variables][numeric variables] if you want to.

```{r happy-data-proc}
happy_recipe = happy %>% 
  select(
    year,
    happiness_score,
    democratic_quality,
    generosity,
    healthy_life_expectancy_at_birth,
    log_gdp_per_capita
  ) %>% 
  recipe(happiness_score ~ . ) %>% 
  step_center(all_numeric(), -log_gdp_per_capita, -year) %>% 
  step_scale(all_numeric(), -log_gdp_per_capita, -year) %>% 
  step_knnimpute(all_numeric()) %>% 
  step_naomit(everything()) %>% 
  step_center(year, means = 2005) %>% 
  prep()
  
happy_processed = happy_recipe %>% bake(happy)
```


Now let's start with our baseline model again.

```{r happy-model-base}
happy_model_base = lm(
  happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, 
  data = happy_processed
)

summary(happy_model_base)
```

We can see that moving one standard deviation on democratic quality and generosity leads to similar standard deviation increases in happiness.  Moving 10 percentage points in GDP would lead to less than .1 standard deviation increase in happiness.

Now we add our life expectancy and yearly trend.

```{r happy_model_more}
happy_model_more = lm(
  happiness_score ~ democratic_quality + generosity + log_gdp_per_capita + healthy_life_expectancy_at_birth + year,
  data = happy_processed
)

summary(happy_model_more)
```

Here it would seem that life expectancy has a notable effect on happiness (shocker), but the yearly trend, while negative, is not statistically notable.  In addition, the democratic effect is no longer significant, as it would seem that it's contribution was more due to it's correlation with life expectancy. But the key question is- is this model better?  

The adjusted $R^2$ seems to indicate that we are doing slightly better with this model, but not much (`r rnd(summary(happy_model_more)$adj, 2)` vs. `r rnd(summary(happy_model_base)$adj, 2)`).  We can test if the increase is a statistically notable one. [Recall previously][statistical assessment] when we compared our model versus a null model to obtain a statistical test of model fit.  Since these models are *nested*, i.e. one is a simpler form of the other, we can use the more general approach we depicted to compare these models.  This ANOVA, or analysis of variance test, is essentially comparing whether the residual sum of squares (i.e. the loss) is statistically less for one model vs. the other. In many settings it is often called a *likelihood ratio test*.

```{r anova-comparison}
anova(happy_model_base, happy_model_more, test = 'Chi')
```

The `Df` from the test denotes that we have two additional parameters, i.e. coefficients, in the more complex model. But the main thing to note is whether the model statistically reduces the RSS. So we see that this is a statistically notable improvement as well.  

I actually do not like this test though. It requires nested models, which in some settings is either not the case or can be hard to determine, and ignores various aspects of uncertainty in parameter estimates.  An approach that works in many settings is to compare *AIC* (Akaike Information Criterion). AIC is a value based on the likelihood for a given model, but which adds a penalty for complexity, since otherwise any more complex model would result in a larger likelihood (or in this case, smaller negative likelihood).  In the following, $\mathcal{L}$ is the likelihood, and $\mathcal{P}$ is the number of parameters estimated for the model.

$$AIC = -2 ( \ln (\mathcal{L})) + 2 \mathcal{P}$$
```{r AIC}
AIC(happy_model_base)
```

The value itself is meaningless until we compare models, in which case the lower value is the better model (because we are working with the negative log likelihood).  With AIC, we don't have to have nested models, so that's a plus over the statistical test.

```{r AIC-compare}
AIC(happy_model_base, happy_model_more)
```

Again, our new model works better.  However, this still may miss out on some uncertainty in the models. To try and capture this, I will calculate interval estimates for the adjusted $R^2$ via *bootstrapping*, and then calculate an interval for their difference.  The details are beyond what I want to delve into here, but the gist is we just want a confidence interval for the difference in adjusted $R^2$.

```{r boot-$R^2$, echo=FALSE}
library(boot)

f_base = happiness_score ~ democratic_quality + generosity + log_gdp_per_capita
f_more = happiness_score ~ democratic_quality + generosity + log_gdp_per_capita + healthy_life_expectancy_at_birth + year

r2_base <- boot(
  happy_processed,
  function(data, indices)
    summary(lm(f_base, data[indices, ])
    )$adj.r.squared, 
  R = 1000
)

r2_more <- boot(
  happy_processed,
  function(data, indices)
    summary(lm(f_more, data[indices, ])
    )$adj.r.squared, 
  R = 1000
)

ci = rbind(
  quantile(r2_base$t, c(0.025,0.975)),
  quantile(r2_more$t, c(0.025,0.975))
)

tibble(
  model = c('base', 'more'), 
  r2 = c(r2_base$t0, r2_more$t0),
) %>% 
  bind_cols(as_tibble(ci)) %>% 
  kable_df()
```

```{r boot-$R^2$-diff, echo=FALSE}
diff_in_r2 = data.matrix(quantile(r2_more$t0 - r2_base$t, c(0.025,0.975)))

colnames(diff_in_r2) = 'Difference in $R^2$'

diff_in_r2 %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  kable_df()
```

It would seem the difference in adjusted $R^2$ is not statistically different from zero.  Likewise we could do the same for AIC.

```{r boot-$AIC$, echo=FALSE}
aic_base <- boot(
  happy_processed,
  function(data, indices)
    AIC(lm(f_base, data[indices, ])
    ), 
  R = 1000
)

aic_more <- boot(
  happy_processed,
  function(data, indices)
    AIC(lm(f_more, data[indices, ])
    ), 
  R = 1000
)

ci = rbind(
  quantile(aic_base$t, c(0.025,0.975)),
  quantile(aic_more$t, c(0.025,0.975))
)

tibble(
  model = c('base', 'more'), 
  aic = c(aic_base$t0, aic_more$t0),
) %>% 
  bind_cols(as_tibble(ci)) %>% 
  kable_df()
```

```{r boot-$ARIC$-diff, echo=FALSE}
diff_in_aic = data.matrix(quantile(aic_more$t0 - aic_base$t, c(0.025,0.975)))

colnames(diff_in_aic) = 'Difference in AIC'

diff_in_aic %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  kable_df()
```

In this case, the more complex model may not be statistically better either, as the interval for the difference in AIC also contains zero, and exhibits a notably wide range.

### Example: Interactions

Let's now add interactions to our model.  Interactions allow the relationship of a predictor variable and target to vary depending on the values of another covariate.  To keep things simple, we'll add a single interaction to start- I will interact democratic quality with life expectancy.

```{r happy-interact}
happy_model_interact = lm(
  happiness_score ~ democratic_quality + generosity + log_gdp_per_capita + 
    healthy_life_expectancy_at_birth +
    democratic_quality:healthy_life_expectancy_at_birth,
  data = happy_processed
  )

summary(happy_model_interact)
```

The coefficient interpretation for variables in the interaction model changes.  For those involved in an interaction, the base coefficient now only describes the effect when the variable they interact with is zero (or is at the reference group if it's categorical). So democratic quality has a slight positive, but not statistically notable, effect at the mean of life expectancy (`r rnd(coef(happy_model_interact)['democratic_quality'])`).  However, this effect increases by `r rnd(coef(happy_model_interact)['democratic_quality:healthy_life_expectancy_at_birth'], 2)` when life expectancy increases by 1 (i.e. 1 standard deviation since we standardized).  The same interpretation goes for life expectancy.  It's base coefficient is when democratic quality is at it's mean (`r rnd(coef(happy_model_interact)['healthy_life_expectancy_at_birth'])`), and the interaction term is interpreted identically.

It seems most people (including journal reviewers) seem to have trouble understanding interactions if you just report them in a table. Furthermore, beyond the standard linear model with non-normal distributions, the coefficient for the interaction term doesn't even have the same precise meaning. But you know what helps us in every interaction setting? Visualization!

Let's use <span class="pack" style = "">ggeffects</span> again. We'll plot the effect of democratic quality at the mean of life expectancy, and at one standard deviation below and above.  Since we already standardized it, this is even easier.

```{r vis-interact}
library(ggeffects)

plot(
  ggpredict(
    happy_model_interact,
    terms = c('democratic_quality', 'healthy_life_expectancy_at_birth[-1, 0, 1]')
  )
)
```

We seem to have discovered something interesting here!  Democratic quality only has a positive effect for those countries with a high life expectancy, i.e. that are already in a good place in general.  It may even be negative in countries in the contrary case. While this has to be taken with a lot of caution, it shows how exploring interactions can be fun and surprising!

Another way to plot interactions in which the variables are continuous is with a contour plot similar to the following.  Here we don't have to pick arbitrary values to plot against, and can see the predictions at all values of the covariates in question.

```{r contour-plot, echo=FALSE, fig.retina=5}
# library(mgcv)
# 
# happy_model_interact_ = gam(
#   happiness_score ~ democratic_quality + generosity + log_gdp_per_capita + 
#     healthy_life_expectancy_at_birth +
#     democratic_quality:healthy_life_expectancy_at_birth,
#   data = happy_processed
#   )
# 
# plot_gam_2d(
#   happy_model_interact_,
#   democratic_quality,
#   healthy_life_expectancy_at_birth,
#   n = 500,
#   palette = 'acton'
# ) +
#   guides(fill = guide_legend(title = "Expected Happiness")) +
#   theme(
#     axis.title.x = element_text(hjust = .5),
#     axis.title.y = element_text(vjust = .5),
#     legend.title = element_text(size = 8)
#   )
# 
# ggsave('img/interaction_2d.png') # svg was 55mb

# plot_gam_3d(happy_model_interact_, democratic_quality, healthy_life_expectancy_at_birth)

knitr::include_graphics('img/interaction_2d.png')
```

We see the the lowest expected happiness based on the model is with high democratic quality and low life expectancy. The best case scenario is to be high on both.


Here is our model comparison for all three models with AIC.

```{r anova-3}
AIC(happy_model_base, happy_model_more, happy_model_interact)
```

Looks like our interaction model is winning.


### Example: Additive models

*Generalized additive models* allow our predictors to have a *wiggly* relationship with the target variable.  For more information, see [this document](https://m-clark.github.io/generalized-additive-models/), but for our purposes, that's all you really need to know- effects don't have to be linear even with linear models! We will use the base R <span class="pack" style = "">mgcv</span> package because it is awesome and you don't need to install anything.  In this case, we'll allow all the covariates to have a nonlinear relationship, and we denote this with the `s()` syntax.

```{r happy-gam}
library(mgcv)

happy_model_gam = gam(
  happiness_score ~ s(democratic_quality) + s(generosity) + s(log_gdp_per_capita) + 
    s(healthy_life_expectancy_at_birth),
  data = happy_processed
)

summary(happy_model_gam)
```

The first thing you may notice is that there are no regression coefficients. This is because the effect of any of these predictors depends on their value, so trying to assess it by a single value would be problematic at best. You can guess what will help us interpret this...

```{r happy-gam-vis, eval=FALSE}
library(mgcViz)

plot.gamViz(happy_model_gam, allTerms = T)
```

```{r happy-gam-vis-show, echo=FALSE}
visibly::plot_gam(happy_model_gam)
```

Here is a brief summary of interpretation.  We generally don't have to worry about small wiggles.

- `democratic_quality`: Effect is most notable (positive and strong) for higher values. Negligible otherwise.
- `generosity`: Effect seems seems strongly positive, but mostly for lower values of generosity.
- `life_expectancy`: Effect is positive, but only if the country is around the mean or higher.
- `log GDP per capita`: Effect is mostly positive, but may depend on other factors not included in the model.

In terms of general model fit, the `Scale est.` is the same as the residual standard error (squared) in the other models, and is a notably lower than even the model with the interaction (`r rnd(happy_model_gam$sig2, 2)` vs. `r rnd(summary(happy_model_interact)$sig^2, 2)`). We can also see that the adjusted $R^2$ is higher as well (`r rnd(summary(happy_model_gam)$r.sq, 2)` vs. `r rnd(summary(happy_model_interact)$adj, 2)`).  If we wanted, we can actually do wiggly interactions also!  Here is our interaction from before for the GAM case.

```{r gam-interact, echo=FALSE}
gam(
  happiness_score ~ s(democratic_quality, healthy_life_expectancy_at_birth, k = 10) + s(generosity) + s(log_gdp_per_capita),
  data = happy_processed
) %>% 
  visibly::plot_gam_3d(democratic_quality, healthy_life_expectancy_at_birth, palette = 'vik') %>% 
  plotly::hide_colorbar()
```


Let's check our AIC now to see which model wins. 

```{r AIC-all, cache=FALSE}
AIC(
  happy_model_null,
  happy_model_base,
  happy_model_more,
  happy_model_interact,
  happy_model_gam
)
```

It's pretty clear our wiggly model is the winner, even with the added complexity.  Note that even though we used a different function for the GAM model, the AIC is still comparable.




## Model Averaging

Have you ever suffered from choice overload?  Many folks who seek to understand some phenomenon via modeling do so. There are plenty of choices due to data processing, but then there may be many models to consider as well, and should be if you're doing things correctly.  But you know what? You don't have to pick a best.

Model averaging is a common technique in the Bayesian world and also with some applications of machine learning, but not as widely applied elsewhere, even though it could be. As an example if we (inversely) weight models by the AIC, we can get an average parameter that favors the better models, while not ignoring the lesser models if they aren't notably poorer.  People will use such an approach to get model averaged effects (i.e. coefficients) or predictions.  In our setting, the GAM is doing so much better, that it's weight would basically be 1.0 and zero for the others. So the model averaged predictions would be almost identical to the GAM predictions.

```{r aic-average, echo=FALSE}
model_list = list(
  happy_model_base,
  happy_model_more,
  happy_model_interact,
  happy_model_gam
)

aic_weights = AIC(
  happy_model_base,
  happy_model_more,
  happy_model_interact,
  happy_model_gam
) %>% 
  as_tibble(rownames = 'model') %>% 
  mutate(
    AICc = map_dbl(model_list, AICcmodavg::AICc),
    deltaAICc = AICc - min(AICc),
    `Rel. Like.` = exp(-.5*deltaAICc),
    weight = `Rel. Like.` / sum(`Rel. Like.`)
  )

aic_weights %>% 
  kable_df(digits = 3)
# 
# model_avg = list(
#   base = happy_model_base,
#   more = happy_model_more,
#   interact = happy_model_interact,
#   gam = happy_model_gam
# ) %>% 
#   map_df(fitted) %>%
#   mutate(avg = rowSums(. * aic_weights$weight))

# model_avg %>% 
#   head() %>% 
#   kable_df()
```

```{r avgr2, echo=FALSE}
# avg_r2 = cor(model_avg$avg, happy_model_base$model$happiness_score)^2
```


## Model Criticism Summary

Statistical significance with a single model does not provide enough of a story to tell with your data. A better assessment of performance  can be made on data the model has not seen, and can provide a better idea of the practical capabilities of it.  Furthermore, pitting various models of differing complexities will allow for better confidence in the model or set of models we ultimately deem worthy. In general, in more explanatory settings we strive to balance performance with complexity through various means.

## Model Criticism Exercises

### Exercise 0

Recall the [google app exercises][model exploration exercises], we use a standard linear model (i.e. <span class="func" style = "">lm</span>) to predict one of three target variables:

- `rating`: the user ratings of the app
- `avg_sentiment_polarity`: the average sentiment score (positive vs. negative) for the app
- `avg_sentiment_subjectivity`: the average subjectivity score (subjective vs. objective) for the app

For prediction use the following variables:

- `reviews`: number of reviews
- `type`: free vs. paid
- `size_in_MB`: size of the app in megabytes

After that we did a model with an interaction.

Either using those models, or running new ones with a different target variable, conduct the following exercises.

```{r ex0-goog, eval=FALSE}
load('data/google_apps.RData')
```

### Exercise 1

Assess the model fit and performance of your first model. Perform additional diagnostics to assess how the model is doing (e.g. plot the model to look at residuals).

```{r ex1-model-assess, eval=FALSE}
summary(model)
plot(model)
```


### Exercise 2

Compare the model with the interaction model.  Based on AIC or some other metric, which one would you choose?  Visualize the interaction model if it's the better model.

```{r ex2-model-compare, eval=FALSE}
anova(model1, model2)
AIC(model1, model2)
```



[^ftest]: This is often depicted in different ways depending on the text and context. In our depiction, the focus is on the F statistic as a ratio of explained variance to unexplained variance, which naturally relates to the $R^2$ statistic.

[^biasR2]: The adjusted $R^2$ does not correct the bias.  It only accounts for complexity.