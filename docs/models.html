<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Model Exploration | Data Processing &amp; Visualization</title>
  <meta name="description" content="The focus of this document is on common data processing and exploration techniques in R, especially as a prelude to visualization. The first part of the document will cover data structures, the dplyr and tidyverse packages, which enhance and facilitate the sorts of operations that typically arise when dealing with data, including faster I/O and grouped operations. For visualization, the focus will be on using ggplot2 and other packages that allow for interactivity. In addition, basic programming concepts and techniques are introduced. Exercises may be found in the document as well. In addition, the demonstrations of most content in Python is available via Jupyter notebooks." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Model Exploration | Data Processing &amp; Visualization" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://m-clark.github.io/data-processing-and-visualization/" />
  <meta property="og:image" content="https://m-clark.github.io/data-processing-and-visualization/img/nineteeneightyR.png" />
  <meta property="og:description" content="The focus of this document is on common data processing and exploration techniques in R, especially as a prelude to visualization. The first part of the document will cover data structures, the dplyr and tidyverse packages, which enhance and facilitate the sorts of operations that typically arise when dealing with data, including faster I/O and grouped operations. For visualization, the focus will be on using ggplot2 and other packages that allow for interactivity. In addition, basic programming concepts and techniques are introduced. Exercises may be found in the document as well. In addition, the demonstrations of most content in Python is available via Jupyter notebooks." />
  <meta name="github-repo" content="m-clark/data-processing-and-visualization/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Model Exploration | Data Processing &amp; Visualization" />
  
  <meta name="twitter:description" content="The focus of this document is on common data processing and exploration techniques in R, especially as a prelude to visualization. The first part of the document will cover data structures, the dplyr and tidyverse packages, which enhance and facilitate the sorts of operations that typically arise when dealing with data, including faster I/O and grouped operations. For visualization, the focus will be on using ggplot2 and other packages that allow for interactivity. In addition, basic programming concepts and techniques are introduced. Exercises may be found in the document as well. In addition, the demonstrations of most content in Python is available via Jupyter notebooks." />
  <meta name="twitter:image" content="https://m-clark.github.io/data-processing-and-visualization/img/nineteeneightyR.png" />



<meta name="date" content="2020-02-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="img/R.ico" type="image/x-icon" />
<link rel="prev" href="more.html"/>
<link rel="next" href="model_criticism.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/font-awesome-5.3.1/css/fontawesome-all.min.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.5/grViz.js"></script>
<link href="libs/vis-4.20.1/vis.css" rel="stylesheet" />
<script src="libs/vis-4.20.1/vis.min.js"></script>
<script src="libs/visNetwork-binding-2.0.9/visNetwork.js"></script>
<script src="libs/plotly-binding-4.9.2/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-7.0.1/css/motion.css" rel="stylesheet" />
<link href="libs/highcharts-7.0.1/css/htmlwdgtgrid.css" rel="stylesheet" />
<script src="libs/highcharts-7.0.1/highcharts.js"></script>
<script src="libs/highcharts-7.0.1/highcharts-3d.js"></script>
<script src="libs/highcharts-7.0.1/highcharts-more.js"></script>
<script src="libs/highcharts-7.0.1/modules/stock.js"></script>
<script src="libs/highcharts-7.0.1/modules/map.js"></script>
<script src="libs/highcharts-7.0.1/modules/annotations.js"></script>
<script src="libs/highcharts-7.0.1/modules/boost.js"></script>
<script src="libs/highcharts-7.0.1/modules/data.js"></script>
<script src="libs/highcharts-7.0.1/modules/drag-panes.js"></script>
<script src="libs/highcharts-7.0.1/modules/drilldown.js"></script>
<script src="libs/highcharts-7.0.1/modules/item-series.js"></script>
<script src="libs/highcharts-7.0.1/modules/offline-exporting.js"></script>
<script src="libs/highcharts-7.0.1/modules/overlapping-datalabels.js"></script>
<script src="libs/highcharts-7.0.1/modules/exporting.js"></script>
<script src="libs/highcharts-7.0.1/modules/export-data.js"></script>
<script src="libs/highcharts-7.0.1/modules/funnel.js"></script>
<script src="libs/highcharts-7.0.1/modules/heatmap.js"></script>
<script src="libs/highcharts-7.0.1/modules/treemap.js"></script>
<script src="libs/highcharts-7.0.1/modules/sankey.js"></script>
<script src="libs/highcharts-7.0.1/modules/solid-gauge.js"></script>
<script src="libs/highcharts-7.0.1/modules/streamgraph.js"></script>
<script src="libs/highcharts-7.0.1/modules/sunburst.js"></script>
<script src="libs/highcharts-7.0.1/modules/vector.js"></script>
<script src="libs/highcharts-7.0.1/modules/wordcloud.js"></script>
<script src="libs/highcharts-7.0.1/modules/xrange.js"></script>
<script src="libs/highcharts-7.0.1/modules/tilemap.js"></script>
<script src="libs/highcharts-7.0.1/modules/venn.js"></script>
<script src="libs/highcharts-7.0.1/modules/gantt.js"></script>
<script src="libs/highcharts-7.0.1/modules/timeline.js"></script>
<script src="libs/highcharts-7.0.1/modules/parallel-coordinates.js"></script>
<script src="libs/highcharts-7.0.1/plugins/grouped-categories.js"></script>
<script src="libs/highcharts-7.0.1/plugins/motion.js"></script>
<script src="libs/highcharts-7.0.1/plugins/multicolor_series.js"></script>
<script src="libs/highcharts-7.0.1/custom/reset.js"></script>
<script src="libs/highcharts-7.0.1/custom/symbols-extra.js"></script>
<script src="libs/highcharts-7.0.1/custom/text-symbols.js"></script>
<script src="libs/highchart-binding-0.7.0/highchart.js"></script>
<script src="libs/sigma-1.2.1/sigma.min.js"></script>
<script src="libs/sigma-1.2.1/plugins/sigma.layout.forceAtlas2.min.js"></script>
<script src="libs/sigma-1.2.1/plugins/sigma.layout.noverlap.min.js"></script>
<script src="libs/sigma-1.2.1/plugins/sigma.renderers.parallelEdges.min.js"></script>
<script src="libs/sigma-1.2.1/plugins/sigma.plugins.animate.min.js"></script>
<script src="libs/sigma-1.2.1/plugins/sigma.renderers.customShapes.min.js"></script>
<script src="libs/sigma-1.2.1/plugins/sigma.plugins.dragNodes.min.js"></script>
<script src="libs/sigma-1.2.1/plugins/sigma.renderers.customEdgeShapes.min.js"></script>
<script src="libs/sigma-1.2.1/plugins/sigma.plugins.relativeSize.min.js"></script>
<script src="libs/sigma-1.2.1/plugins/sigma.renderers.edgeLabels.min.js"></script>
<script src="libs/sigma-1.2.1/plugins/sigma.parsers.gexf.min.js"></script>
<script src="libs/sigma-1.2.1/plugins/sigma.exporters.svg.min.js"></script>
<script src="libs/sigma-1.2.1/plugins/sigma.plugins.neighborhoods.min.js"></script>
<script src="libs/sigma-1.2.1/plugins/sigma.plugins.filter.min.js"></script>
<script src="libs/sigma-1.2.1/plugins/sigma.renderers.snapshot.min.js"></script>
<script src="libs/sigmajs-binding-0.1.3/sigmajs.js"></script>
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/Proj4Leaflet-1.0.1/proj4-compressed.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.0.3/leaflet.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.12/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/book.css" type="text/css" />
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class='before'><a href="./">Data Processing & Visualization</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i>Intro</a><ul>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#outline"><i class="fa fa-check"></i>Outline</a><ul>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#part-1-data-processing"><i class="fa fa-check"></i>Part 1: Data Processing</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#part-2-programming-basics"><i class="fa fa-check"></i>Part 2: Programming Basics</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#part-3-modeling"><i class="fa fa-check"></i>Part 3: Modeling</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#part-4-visualization"><i class="fa fa-check"></i>Part 4: Visualization</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#part-5-presentation"><i class="fa fa-check"></i>Part 5: Presentation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#preparation"><i class="fa fa-check"></i>Preparation</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#other"><i class="fa fa-check"></i>Other</a><ul>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#python"><i class="fa fa-check"></i>Python</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#r"><i class="fa fa-check"></i>R</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part I: Information Processing</b></span></li>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html"><i class="fa fa-check"></i>Data Structures</a><ul>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html#vectors"><i class="fa fa-check"></i>Vectors</a><ul>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html#character-strings"><i class="fa fa-check"></i>Character strings</a></li>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html#factors"><i class="fa fa-check"></i>Factors</a></li>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html#logicals"><i class="fa fa-check"></i>Logicals</a></li>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html#numeric-and-integer"><i class="fa fa-check"></i>Numeric and integer</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html#matrices"><i class="fa fa-check"></i>Matrices</a><ul>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html#creating-a-matrix"><i class="fa fa-check"></i>Creating a matrix</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html#lists"><i class="fa fa-check"></i>Lists</a></li>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html#data-frames"><i class="fa fa-check"></i>Data Frames</a><ul>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html#creating-a-data-frame"><i class="fa fa-check"></i>Creating a data frame</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html#data-structure-exercises"><i class="fa fa-check"></i>Data Structure Exercises</a><ul>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html#excercise-1"><i class="fa fa-check"></i>Excercise #1</a></li>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html#exercise-2"><i class="fa fa-check"></i>Exercise #2</a></li>
<li class="chapter" data-level="" data-path="data_structures.html"><a href="data_structures.html#thinking-exercises"><i class="fa fa-check"></i>Thinking Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="io.html"><a href="io.html"><i class="fa fa-check"></i>Input/Output</a><ul>
<li class="chapter" data-level="" data-path="io.html"><a href="io.html#other-approaches"><i class="fa fa-check"></i>Other approaches</a></li>
<li class="chapter" data-level="" data-path="io.html"><a href="io.html#faster-approaches"><i class="fa fa-check"></i>Faster approaches</a></li>
<li class="chapter" data-level="" data-path="io.html"><a href="io.html#other-data"><i class="fa fa-check"></i>Other Data</a></li>
<li class="chapter" data-level="" data-path="io.html"><a href="io.html#on-the-horizon"><i class="fa fa-check"></i>On the horizon</a></li>
<li class="chapter" data-level="" data-path="io.html"><a href="io.html#big-data"><i class="fa fa-check"></i>Big Data</a></li>
<li class="chapter" data-level="" data-path="io.html"><a href="io.html#io-exercises"><i class="fa fa-check"></i>I/O Exercises</a><ul>
<li class="chapter" data-level="" data-path="io.html"><a href="io.html#exercise-1"><i class="fa fa-check"></i>Exercise 1</a></li>
<li class="chapter" data-level="" data-path="io.html"><a href="io.html#thinking-exercises-1"><i class="fa fa-check"></i>Thinking Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="indexing.html"><a href="indexing.html"><i class="fa fa-check"></i>Indexing</a><ul>
<li class="chapter" data-level="" data-path="indexing.html"><a href="indexing.html#base-r-indexing-refresher"><i class="fa fa-check"></i>Base R Indexing Refresher</a></li>
<li class="chapter" data-level="" data-path="indexing.html"><a href="indexing.html#listdata.frame-extraction"><i class="fa fa-check"></i>List/Data.frame extraction</a></li>
<li class="chapter" data-level="" data-path="indexing.html"><a href="indexing.html#indexing-exercises"><i class="fa fa-check"></i>Indexing Exercises</a><ul>
<li class="chapter" data-level="" data-path="indexing.html"><a href="indexing.html#exercise-1-1"><i class="fa fa-check"></i>Exercise 1</a></li>
<li class="chapter" data-level="" data-path="indexing.html"><a href="indexing.html#exercise-2-1"><i class="fa fa-check"></i>Exercise 2</a></li>
<li class="chapter" data-level="" data-path="indexing.html"><a href="indexing.html#exercise-3"><i class="fa fa-check"></i>Exercise 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pipes.html"><a href="pipes.html"><i class="fa fa-check"></i>Pipes</a><ul>
<li class="chapter" data-level="" data-path="pipes.html"><a href="pipes.html#using-variables-as-they-are-created"><i class="fa fa-check"></i>Using variables as they are created</a></li>
<li class="chapter" data-level="" data-path="pipes.html"><a href="pipes.html#pipes-for-visualization-more-later"><i class="fa fa-check"></i>Pipes for Visualization (more later)</a></li>
<li class="chapter" data-level="" data-path="pipes.html"><a href="pipes.html#the-dot"><i class="fa fa-check"></i>The dot</a></li>
<li class="chapter" data-level="" data-path="pipes.html"><a href="pipes.html#flexibility"><i class="fa fa-check"></i>Flexibility</a></li>
<li class="chapter" data-level="" data-path="pipes.html"><a href="pipes.html#summary"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i>Tidyverse</a><ul>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#what-is-the-tidyverse"><i class="fa fa-check"></i>What is the tidyverse?</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#what-is-tidy"><i class="fa fa-check"></i>What is tidy?</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#dplyr"><i class="fa fa-check"></i>dplyr</a><ul>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#an-example"><i class="fa fa-check"></i>An example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#running-example"><i class="fa fa-check"></i>Running example</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#selecting-columns"><i class="fa fa-check"></i>Selecting Columns</a><ul>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#helper-functions"><i class="fa fa-check"></i>Helper functions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#filtering-rows"><i class="fa fa-check"></i>Filtering Rows</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#generating-new-data"><i class="fa fa-check"></i>Generating New Data</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#grouping-and-summarizing-data"><i class="fa fa-check"></i>Grouping and Summarizing Data</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#renaming-columns"><i class="fa fa-check"></i>Renaming columns</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#merging-data"><i class="fa fa-check"></i>Merging Data</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#tidyr"><i class="fa fa-check"></i>tidyr</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#more-tidyverse"><i class="fa fa-check"></i>More Tidyverse</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#personal-opinion"><i class="fa fa-check"></i>Personal Opinion</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#tidyverse-exercises"><i class="fa fa-check"></i>tidyverse Exercises</a><ul>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#exercise-0"><i class="fa fa-check"></i>Exercise 0</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#exercise-1-2"><i class="fa fa-check"></i>Exercise 1</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#exercise-2-2"><i class="fa fa-check"></i>Exercise 2</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#exercise-3-1"><i class="fa fa-check"></i>Exercise 3</a></li>
<li class="chapter" data-level="" data-path="tidyverse.html"><a href="tidyverse.html#exercise-4"><i class="fa fa-check"></i>Exercise 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html"><i class="fa fa-check"></i>data.table</a><ul>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#grouped-operations"><i class="fa fa-check"></i>Grouped operations</a></li>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#faster"><i class="fa fa-check"></i>Faster!</a><ul>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#joins"><i class="fa fa-check"></i>Joins</a></li>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#group-by"><i class="fa fa-check"></i>Group by</a></li>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#string-matching"><i class="fa fa-check"></i>String matching</a></li>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#reading-files"><i class="fa fa-check"></i>Reading files</a></li>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#more-speed"><i class="fa fa-check"></i>More speed</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#pipe-with-data.table"><i class="fa fa-check"></i>Pipe with data.table</a></li>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#summary-1"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#faster-dplyr-alternatives"><i class="fa fa-check"></i>Faster dplyr alternatives</a></li>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#data.table-exercises"><i class="fa fa-check"></i>data.table Exercises</a><ul>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#exercise-0-1"><i class="fa fa-check"></i>Exercise 0</a></li>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#exercise-1-3"><i class="fa fa-check"></i>Exercise 1</a></li>
<li class="chapter" data-level="" data-path="data_table.html"><a href="data_table.html#exercise-2-3"><i class="fa fa-check"></i>Exercise 2</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Programming</b></span></li>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html"><i class="fa fa-check"></i>Basics</a><ul>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html#r-objects"><i class="fa fa-check"></i>R Objects</a><ul>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html#object-inspection-exploration"><i class="fa fa-check"></i>Object Inspection &amp; Exploration</a></li>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html#methods"><i class="fa fa-check"></i>Methods</a></li>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html#s4-classes"><i class="fa fa-check"></i>S4 classes</a></li>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html#others"><i class="fa fa-check"></i>Others</a></li>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html#inspecting-functions"><i class="fa fa-check"></i>Inspecting Functions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html#help-files"><i class="fa fa-check"></i>Help Files</a></li>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html#objects-exercises"><i class="fa fa-check"></i>Objects Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="iterative.html"><a href="iterative.html"><i class="fa fa-check"></i>Iterative Programming</a><ul>
<li class="chapter" data-level="" data-path="iterative.html"><a href="iterative.html#for-loops"><i class="fa fa-check"></i>For loops</a><ul>
<li class="chapter" data-level="" data-path="iterative.html"><a href="iterative.html#a-slight-speed-gain"><i class="fa fa-check"></i>A slight speed gain</a></li>
<li class="chapter" data-level="" data-path="iterative.html"><a href="iterative.html#while-alternative"><i class="fa fa-check"></i>While alternative</a></li>
<li class="chapter" data-level="" data-path="iterative.html"><a href="iterative.html#loops-summary"><i class="fa fa-check"></i>Loops summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="iterative.html"><a href="iterative.html#implicit-loops"><i class="fa fa-check"></i>Implicit Loops</a><ul>
<li class="chapter" data-level="" data-path="iterative.html"><a href="iterative.html#apply-family"><i class="fa fa-check"></i>apply family</a></li>
<li class="chapter" data-level="" data-path="iterative.html"><a href="iterative.html#apply-functions"><i class="fa fa-check"></i>Apply functions</a></li>
<li class="chapter" data-level="" data-path="iterative.html"><a href="iterative.html#purrr"><i class="fa fa-check"></i>purrr</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="iterative.html"><a href="iterative.html#looping-with-lists"><i class="fa fa-check"></i>Looping with lists</a></li>
<li class="chapter" data-level="" data-path="iterative.html"><a href="iterative.html#iterative-programming-exercises"><i class="fa fa-check"></i>Iterative Programming Exercises</a><ul>
<li class="chapter" data-level="" data-path="iterative.html"><a href="iterative.html#exercise-1-4"><i class="fa fa-check"></i>Exercise 1</a></li>
<li class="chapter" data-level="" data-path="iterative.html"><a href="iterative.html#exercise-2-4"><i class="fa fa-check"></i>Exercise 2</a></li>
<li class="chapter" data-level="" data-path="iterative.html"><a href="iterative.html#exercise-3-2"><i class="fa fa-check"></i>Exercise 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="functions.html"><a href="functions.html"><i class="fa fa-check"></i>Writing Functions</a><ul>
<li class="chapter" data-level="" data-path="functions.html"><a href="functions.html#a-starting-point"><i class="fa fa-check"></i>A starting point</a></li>
<li class="chapter" data-level="" data-path="functions.html"><a href="functions.html#dry"><i class="fa fa-check"></i>DRY</a></li>
<li class="chapter" data-level="" data-path="functions.html"><a href="functions.html#conditionals"><i class="fa fa-check"></i>Conditionals</a></li>
<li class="chapter" data-level="" data-path="functions.html"><a href="functions.html#anonymous-functions"><i class="fa fa-check"></i>Anonymous functions</a></li>
<li class="chapter" data-level="" data-path="functions.html"><a href="functions.html#writing-functions-exercises"><i class="fa fa-check"></i>Writing Functions Exercises</a><ul>
<li class="chapter" data-level="" data-path="functions.html"><a href="functions.html#excercise-1-1"><i class="fa fa-check"></i>Excercise 1</a></li>
<li class="chapter" data-level="" data-path="functions.html"><a href="functions.html#excercise-1b"><i class="fa fa-check"></i>Excercise 1b</a></li>
<li class="chapter" data-level="" data-path="functions.html"><a href="functions.html#exercise-2-5"><i class="fa fa-check"></i>Exercise 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html"><i class="fa fa-check"></i>More</a><ul>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#code-style"><i class="fa fa-check"></i>Code Style</a><ul>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#why-does-your-code-exist"><i class="fa fa-check"></i>Why does your code exist?</a></li>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#assignment"><i class="fa fa-check"></i>Assignment</a></li>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#code-length"><i class="fa fa-check"></i>Code length</a></li>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#spacing"><i class="fa fa-check"></i>Spacing</a></li>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#naming-things"><i class="fa fa-check"></i>Naming things</a></li>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#other-1"><i class="fa fa-check"></i>Other</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#vectorization"><i class="fa fa-check"></i>Vectorization</a><ul>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#boolean-indexing"><i class="fa fa-check"></i>Boolean Indexing</a></li>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#vectorized-operations"><i class="fa fa-check"></i>Vectorized operations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#regular-expressions"><i class="fa fa-check"></i>Regular Expressions</a><ul>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#typical-uses"><i class="fa fa-check"></i>Typical Uses</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#code-style-exercises"><i class="fa fa-check"></i>Code Style Exercises</a><ul>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#exercise-1-5"><i class="fa fa-check"></i>Exercise 1</a></li>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#exercise-2-6"><i class="fa fa-check"></i>Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#vectorization-exercises"><i class="fa fa-check"></i>Vectorization Exercises</a><ul>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#exercise-1-6"><i class="fa fa-check"></i>Exercise 1</a></li>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#exercise-2-7"><i class="fa fa-check"></i>Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#regex-exercises"><i class="fa fa-check"></i>Regex Exercises</a><ul>
<li class="chapter" data-level="" data-path="more.html"><a href="more.html#exercise-1-7"><i class="fa fa-check"></i>Exercise 1</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part III: Modeling</b></span></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i>Model Exploration</a><ul>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#model-taxonomy"><i class="fa fa-check"></i>Model Taxonomy</a></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#linear-models"><i class="fa fa-check"></i>Linear models</a></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#estimation"><i class="fa fa-check"></i>Estimation</a><ul>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#loss"><i class="fa fa-check"></i>Loss</a></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#optimization"><i class="fa fa-check"></i>Optimization</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#fitting-models"><i class="fa fa-check"></i>Fitting Models</a><ul>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#using-matrices"><i class="fa fa-check"></i>Using matrices</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#summarizing-models"><i class="fa fa-check"></i>Summarizing Models</a></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#variable-transformations"><i class="fa fa-check"></i>Variable Transformations</a><ul>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#numeric-variables"><i class="fa fa-check"></i>Numeric variables</a></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#categorical-variables"><i class="fa fa-check"></i>Categorical variables</a></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#scales-indices-and-dimension-reduction"><i class="fa fa-check"></i>Scales, indices, and dimension reduction</a></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#dont-discretize"><i class="fa fa-check"></i>Don’t discretize</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#variable-importance"><i class="fa fa-check"></i>Variable Importance</a></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#extracting-output"><i class="fa fa-check"></i>Extracting Output</a><ul>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#package-support"><i class="fa fa-check"></i>Package support</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#visualization"><i class="fa fa-check"></i>Visualization</a></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#extensions-to-the-standard-linear-model"><i class="fa fa-check"></i>Extensions to the Standard Linear Model</a><ul>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#different-types-of-targets"><i class="fa fa-check"></i>Different types of targets</a></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#correlated-data"><i class="fa fa-check"></i>Correlated data</a></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#other-extensions"><i class="fa fa-check"></i>Other extensions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#model-exploration-summary"><i class="fa fa-check"></i>Model Exploration Summary</a></li>
<li class="chapter" data-level="" data-path="models.html"><a href="models.html#exercises"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model_criticism.html"><a href="model_criticism.html"><i class="fa fa-check"></i>Model Criticism</a><ul>
<li class="chapter" data-level="" data-path="model_criticism.html"><a href="model_criticism.html#model-fit"><i class="fa fa-check"></i>Model Fit</a><ul>
<li class="chapter" data-level="" data-path="model_criticism.html"><a href="model_criticism.html#standard-linear-model"><i class="fa fa-check"></i>Standard linear model</a></li>
<li class="chapter" data-level="" data-path="model_criticism.html"><a href="model_criticism.html#beyond-ols"><i class="fa fa-check"></i>Beyond OLS</a></li>
<li class="chapter" data-level="" data-path="model_criticism.html"><a href="model_criticism.html#classification"><i class="fa fa-check"></i>Classification</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model_criticism.html"><a href="model_criticism.html#model-assumptions"><i class="fa fa-check"></i>Model Assumptions</a></li>
<li class="chapter" data-level="" data-path="model_criticism.html"><a href="model_criticism.html#predictive-performance"><i class="fa fa-check"></i>Predictive Performance</a></li>
<li class="chapter" data-level="" data-path="model_criticism.html"><a href="model_criticism.html#model-comparison"><i class="fa fa-check"></i>Model Comparison</a><ul>
<li class="chapter" data-level="" data-path="model_criticism.html"><a href="model_criticism.html#example-additional-covariates"><i class="fa fa-check"></i>Example: Additional covariates</a></li>
<li class="chapter" data-level="" data-path="model_criticism.html"><a href="model_criticism.html#example-interactions"><i class="fa fa-check"></i>Example: Interactions</a></li>
<li class="chapter" data-level="" data-path="model_criticism.html"><a href="model_criticism.html#example-additive-models"><i class="fa fa-check"></i>Example: Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model_criticism.html"><a href="model_criticism.html#model-averaging"><i class="fa fa-check"></i>Model Averaging</a></li>
<li class="chapter" data-level="" data-path="model_criticism.html"><a href="model_criticism.html#model-criticism-summary"><i class="fa fa-check"></i>Model Criticism Summary</a></li>
<li class="chapter" data-level="" data-path="model_criticism.html"><a href="model_criticism.html#exercises-1"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html"><i class="fa fa-check"></i>Machine Learning</a><ul>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html#concepts"><i class="fa fa-check"></i>Concepts</a><ul>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html#loss-1"><i class="fa fa-check"></i>Loss</a></li>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html#bias-variance-tradeoff"><i class="fa fa-check"></i>Bias-variance Tradeoff</a></li>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html#regularization"><i class="fa fa-check"></i>Regularization</a></li>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html#cross-validation"><i class="fa fa-check"></i>Cross-validation</a></li>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html#optimization-1"><i class="fa fa-check"></i>Optimization</a></li>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html#tuning-parameters"><i class="fa fa-check"></i>Tuning parameters</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html#techniques"><i class="fa fa-check"></i>Techniques</a><ul>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html#regularized-regression"><i class="fa fa-check"></i>Regularized regression</a></li>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html#random-forests"><i class="fa fa-check"></i>Random Forests</a></li>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html#neural-networks"><i class="fa fa-check"></i>Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html#interpreting-the-black-box"><i class="fa fa-check"></i>Interpreting the black box</a></li>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html#machine-learning-summary"><i class="fa fa-check"></i>Machine Learning Summary</a></li>
<li class="chapter" data-level="" data-path="ml.html"><a href="ml.html#exercises-2"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="part"><span><b>Part IV: Visualization</b></span></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html"><i class="fa fa-check"></i>ggplot2</a><ul>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#layers"><i class="fa fa-check"></i>Layers</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#piping"><i class="fa fa-check"></i>Piping</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#aesthetics"><i class="fa fa-check"></i>Aesthetics</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#geoms"><i class="fa fa-check"></i>Geoms</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#examples"><i class="fa fa-check"></i>Examples</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#stats"><i class="fa fa-check"></i>Stats</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#scales"><i class="fa fa-check"></i>Scales</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#facets"><i class="fa fa-check"></i>Facets</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#multiple-plots"><i class="fa fa-check"></i>Multiple plots</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#fine-control"><i class="fa fa-check"></i>Fine control</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#themes"><i class="fa fa-check"></i>Themes</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#extensions"><i class="fa fa-check"></i>Extensions</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#python-1"><i class="fa fa-check"></i>Python</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#summary-ggplot2"><i class="fa fa-check"></i>Summary ggplot2</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#ggplot2-exercises"><i class="fa fa-check"></i>ggplot2 Exercises</a><ul>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#exercise-0-2"><i class="fa fa-check"></i>Exercise 0</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#exercise-1-8"><i class="fa fa-check"></i>Exercise 1</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#exercise-2-8"><i class="fa fa-check"></i>Exercise 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html"><i class="fa fa-check"></i>Interactive Visualization</a><ul>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#packages"><i class="fa fa-check"></i>Packages</a></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#piping-for-visualization"><i class="fa fa-check"></i>Piping for Visualization</a></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#htmlwidgets"><i class="fa fa-check"></i>htmlwidgets</a></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#plotly"><i class="fa fa-check"></i>Plotly</a><ul>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#modes"><i class="fa fa-check"></i>Modes</a></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#ggplotly"><i class="fa fa-check"></i>ggplotly</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#highcharter"><i class="fa fa-check"></i>Highcharter</a></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#graph-networks"><i class="fa fa-check"></i>Graph networks</a><ul>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#visnetwork"><i class="fa fa-check"></i>visNetwork</a></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#sigmajs"><i class="fa fa-check"></i>sigmajs</a></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#plotly-1"><i class="fa fa-check"></i>Plotly</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#leaflet"><i class="fa fa-check"></i>leaflet</a></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#dt"><i class="fa fa-check"></i>DT</a></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#shiny"><i class="fa fa-check"></i>Shiny</a><ul>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#dash"><i class="fa fa-check"></i>Dash</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#interactive-and-visual-data-exploration"><i class="fa fa-check"></i>Interactive and Visual Data Exploration</a></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#interactive-visualization-exercises"><i class="fa fa-check"></i>Interactive Visualization Exercises</a><ul>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#exercise-0-3"><i class="fa fa-check"></i>Exercise 0</a></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#exercise-1-9"><i class="fa fa-check"></i>Exercise 1</a></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#exercise-2-9"><i class="fa fa-check"></i>Exercise 2</a></li>
<li class="chapter" data-level="" data-path="interactive.html"><a href="interactive.html#exercise-3-3"><i class="fa fa-check"></i>Exercise 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html"><i class="fa fa-check"></i>Thinking Visually</a><ul>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#information"><i class="fa fa-check"></i>Information</a><ul>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#your-audience-isnt-dumb"><i class="fa fa-check"></i>Your audience isn’t dumb</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#clarity-is-key"><i class="fa fa-check"></i>Clarity is key</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#avoid-clutter"><i class="fa fa-check"></i>Avoid clutter</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#color-isnt-optional"><i class="fa fa-check"></i>Color isn’t optional</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#think-interactively"><i class="fa fa-check"></i>Think interactively</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#color"><i class="fa fa-check"></i>Color</a><ul>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#viridis"><i class="fa fa-check"></i>Viridis</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#scientific-colors"><i class="fa fa-check"></i>Scientific colors</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#rcolorbrewer"><i class="fa fa-check"></i>RColorBrewer</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#contrast"><i class="fa fa-check"></i>Contrast</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#scaling-size"><i class="fa fa-check"></i>Scaling Size</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#transparency"><i class="fa fa-check"></i>Transparency</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#accessibility"><i class="fa fa-check"></i>Accessibility</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#file-types"><i class="fa fa-check"></i>File Types</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#summary-2"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#a-casual-list-of-things-to-avoid"><i class="fa fa-check"></i>A casual list of things to avoid</a><ul>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#pie"><i class="fa fa-check"></i>Pie</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#histograms"><i class="fa fa-check"></i>Histograms</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#using-3d-without-adding-any-communicative-value"><i class="fa fa-check"></i>Using 3D without adding any communicative value</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#using-too-many-colors"><i class="fa fa-check"></i>Using too many colors</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#using-valenced-colors-when-data-isnt-applicable"><i class="fa fa-check"></i>Using valenced colors when data isn’t applicable</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#showing-maps-that-just-display-population"><i class="fa fa-check"></i>Showing maps that just display population</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#biplots"><i class="fa fa-check"></i>Biplots</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#thinking-visually-exercises"><i class="fa fa-check"></i>Thinking Visually Exercises</a><ul>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#exercise-1-10"><i class="fa fa-check"></i>Exercise 1</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#exercise-2-10"><i class="fa fa-check"></i>Exercise 2</a></li>
<li class="chapter" data-level="" data-path="thinking_vis.html"><a href="thinking_vis.html#thinking-exercises-2"><i class="fa fa-check"></i>Thinking exercises</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Wrap-up</b></span></li>
<li class="chapter" data-level="" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li class='after'">
   <a href="https://m-clark.github.io/" aria-label='Author Website Link'>
      <img src="img/mc_logo.png" style="width:25%; padding:0px 0; display:block; margin: 0 auto;" alt="MC logo">
   </a>
</li>
<li class='after'">
   <div style='text-align:center'>
      <a href="https://github.com/m-clark/" aria-label='GitHub Link'>
         <i class="fab fa-github fa-2x" aria-hidden="true"></i>
      </a>
   </div>
</li>
<li class='after'">
   <div style='text-align:center'>
      <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" aria-label="Creative commons CCBYSA link">
         <i class="fab fa-creative-commons fa-lg"></i> 
         <i class="fab fa-creative-commons-by fa-lg"></i> 
         <i class="fab fa-creative-commons-sa fa-lg"></i>
      </a>
   </div>
</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Processing &amp; Visualization</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model-exploration-1" class="section level1">
<h1>Model Exploration</h1>
<div style="text-align: center">
<i class="fas  fa-chart-line fa-5x " style="color:#990024;"></i>
</div>
<p>The following section shows how to get started with modeling in R generally, with a focus on concepts, tools, and syntax, rather than trying to understand the specifics of a given model. We first dive into model exploration, getting a sense of the basic mechanics behind our modeling tools, and contemplating standard results. We’ll then shift our attention to understanding the strengths and limitations of our models. We’ll then change from classical methods to explore machine learning techniques. The goal of these chapters is to provide an overview of concepts and ways to think about modeling.</p>
<div id="model-taxonomy" class="section level2">
<h2>Model Taxonomy</h2>
<p>We can begin with a taxonomy that broadly describes two classes of models:</p>
<ul>
<li><em>Supervised</em></li>
<li><em>Unsupervised</em></li>
<li>Some combination</li>
</ul>
<p>For supervised settings, there is a target or set of target variables which we aim to predict with a set of predictor variables or covariates. This is far and away the most common case, and the one we will focus on here. It is very common in machine learning parlance to further distinguish <em>regression</em> and <em>classification</em> among supervised models, but what they actually mean to distinguish is numeric target variables from categorical ones (it’s all regression).</p>
<p>In the case of unsupervised models, the data itself is the target, and this setting includes techniques such as principal components analysis, factor analysis, cluster analytic approaches, topic modeling, and many others. A key goal for many such methods is <em>dimension reduction</em>, either of the columns or rows. For example, we may have many items of a survey we wish to group together into a few concepts, or cluster thousands of observations into a few simple categories.</p>
<p>We can also broadly describe two primary goals of modeling:</p>
<ul>
<li><em>Prediction</em></li>
<li><em>Explanation</em></li>
</ul>
<p>Different models will provide varying amounts of predictive and explanatory (or inferential) power. In some settings, prediction is almost entirely the goal, with little need to understand the underlying details of the relation of inputs to outputs. For example, in a model that predicts words to suggest when typing, we don’t really need to know nor much care about the details except to be able to improve those suggestions. In scientific studies however, we may be much more interested in the (potentially causal) relations among the variables under study.</p>
<p>While these are sometimes competing goals, it is definitely not the case that they are mutually exclusive. For example, a fully interpretable model, statistically speaking, may have no predictive capability, and so is fairly useless in practical terms. Often, very predictive models offer little understanding. But sometimes we can luck out and have both a highly predictive model as well as one that is highly interpretable.</p>
</div>
<div id="linear-models" class="section level2">
<h2>Linear models</h2>
<p>Most models you see in published reports are <em>linear models</em> of varying kinds, and form the basis on which to build more complex forms. In such models we distinguish a target variable we want to understand from the variables which we will use to understand it. Note that these come with different names depending on the goal of the study, discipline, and other factors. The following table denotes common nomenclature across many disciplines.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Type
</th>
<th style="text-align:left;">
Names
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;vertical-align: middle !important;" rowspan="5">
Target
</td>
<td style="text-align:left;">
Dependent variable
</td>
</tr>
<tr>
<td style="text-align:left;">
Response
</td>
</tr>
<tr>
<td style="text-align:left;">
Outcome
</td>
</tr>
<tr>
<td style="text-align:left;">
Output
</td>
</tr>
<tr>
<td style="text-align:left;">
Y
</td>
</tr>
<tr>
<td style="text-align:left;vertical-align: middle !important;" rowspan="5">
Prediction
</td>
<td style="text-align:left;">
Independent variable
</td>
</tr>
<tr>
<td style="text-align:left;">
Predictor
</td>
</tr>
<tr>
<td style="text-align:left;">
Covariate
</td>
</tr>
<tr>
<td style="text-align:left;">
Input
</td>
</tr>
<tr>
<td style="text-align:left;">
X
</td>
</tr>
</tbody>
</table>
<p>A typical way to depict a linear regression model is as follows:</p>
<p><span class="math display">\[y = b_0 + b_1\cdot x_1 + b_2\cdot x_2 + ... +  + b_p\cdot x_p + \epsilon\]</span></p>
<p>In the above, <span class="math inline">\(b_0\)</span> is the intercept, and the other <span class="math inline">\(b_*\)</span> are the regression coefficients that represent the relationship of the predictors <span class="math inline">\(x\)</span> to the target variable <span class="math inline">\(y\)</span>. The <span class="math inline">\(\epsilon\)</span> represents the <em>error</em> or <em>residual</em>. We don’t have perfect prediction, and that represents the difference between what we can guess with our predictor relationships to the target and what we actually observe with it.</p>
<p>In R, we specify a linear model as follows. Conveniently enough, we use a function, <code>lm</code>, that stands for linear model. There are various inputs, typically starting with the formula. In the formula, The target variable is first, followed by the predictor variables, separated by a tilde (<code>~</code>). Additional predictor variables are added with a plus sign (<code>+</code>). In this example, <code>y</code> is our target, and the predictors are <code>x</code> and <code>z</code>.</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="models.html#cb322-1"></a><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>z)</span></code></pre></div>
<p>We can still use linear models to investigate nonlinear relationships. For example, in the following, we can add a quadratic term or an interaction, yet the model is still linear in the parameters. All of the following are standard linear regression models.</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="models.html#cb323-1"></a><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>z <span class="op">+</span><span class="st"> </span>x<span class="op">:</span>z)</span>
<span id="cb323-2"><a href="models.html#cb323-2"></a><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>x_squared)     <span class="co"># a better way: lm(y ~ poly(x, degree = 2))</span></span></code></pre></div>
<p>In the models above, <code>x</code> has a potentially nonlinear relationship with <code>y</code>, either by varying its (linear) relationship depending on values of z (the first case) or itself (the second). In general, the manner in which nonlinear relationships may be explored in linear models is quite flexible.</p>
<p>An example of a <em>nonlinear model</em> would be population growth models, like exponential or logistic growth curves. You can use functions like <span class="func" style="">nls</span> or <span class="func" style="">nlme</span> for such models, but should have a specific theoretical reason to do so, and even then, flexible models such as <a href="https://m-clark.github.io/generalized-additive-models/">GAMs</a> might be better than assuming a functional form.</p>
</div>
<div id="estimation" class="section level2">
<h2>Estimation</h2>
<p>One thing to understand with predictive models of any kind is how we estimate the parameters of interest, e.g. coefficients/weights, variance, and more. To start with, we must have some sort of goal that choosing a particular set of values for the parameters achieves, and then find some way to reach that goal efficiently.</p>
<div id="loss" class="section level3">
<h3>Loss</h3>
<p>The goal of many estimation approaches is the reduction of <em>loss</em>, conceptually defined as the difference between the model predictions and the observed data, i.e. prediction error. In an introductory methods course, many are introduced to <em>ordinary least squares</em> as a means to estimate the coefficients for a linear regression model. In this scenario, we are seeking to come up with estimates of the coefficients that minimize the (squared) difference between the observed target value and the fitted value based on the parameter estimates. The loss in this case is defined as the sum of the squared errors. Formally we can state it as follows.</p>
<p><span class="math display">\[\mathcal{Loss} = \Sigma(y - \hat{y})^2\]</span></p>
<p>We can see how this works more clearly with some simple conceptual code. In what follows, we create a [function][writing-functions], allows us to move [row by row][for-loops] through the data, calculating both our prediction based on the given model parameters- <span class="math inline">\(\hat{y}\)</span>, and the difference between that and our target variable <span class="math inline">\(y\)</span>. We sum these squared differences to get a total. In practice such a function is called the loss function, cost function, or objective function.</p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="models.html#cb324-1"></a>ls_loss &lt;-<span class="st"> </span><span class="cf">function</span>(X, y, beta) {</span>
<span id="cb324-2"><a href="models.html#cb324-2"></a>  </span>
<span id="cb324-3"><a href="models.html#cb324-3"></a>  <span class="co"># initialize the objects</span></span>
<span id="cb324-4"><a href="models.html#cb324-4"></a>  loss  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">nrow</span>(X))</span>
<span id="cb324-5"><a href="models.html#cb324-5"></a>  y_hat =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">nrow</span>(X))</span>
<span id="cb324-6"><a href="models.html#cb324-6"></a>  </span>
<span id="cb324-7"><a href="models.html#cb324-7"></a>  <span class="co"># for each row, calculate y_hat and square the difference with y</span></span>
<span id="cb324-8"><a href="models.html#cb324-8"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(X)) {</span>
<span id="cb324-9"><a href="models.html#cb324-9"></a>    y_hat[n] =<span class="st"> </span><span class="kw">sum</span>(X[n, ] <span class="op">*</span><span class="st"> </span>beta)</span>
<span id="cb324-10"><a href="models.html#cb324-10"></a>    loss[n]  =<span class="st"> </span>(y[n] <span class="op">-</span><span class="st"> </span>y_hat[n]) <span class="op">^</span><span class="st"> </span><span class="dv">2</span></span>
<span id="cb324-11"><a href="models.html#cb324-11"></a>  }</span>
<span id="cb324-12"><a href="models.html#cb324-12"></a>  </span>
<span id="cb324-13"><a href="models.html#cb324-13"></a>  <span class="kw">sum</span>(loss)  </span>
<span id="cb324-14"><a href="models.html#cb324-14"></a>}</span></code></pre></div>
<p>Now we need some data. Let’s construct some data so that we know the true underlying values for the regression coefficients. Feel free to change the sample size <code>N</code> or the coefficient values.</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="models.html#cb325-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)           <span class="co"># for reproducibility</span></span>
<span id="cb325-2"><a href="models.html#cb325-2"></a>N =<span class="st"> </span><span class="dv">100</span></span>
<span id="cb325-3"><a href="models.html#cb325-3"></a>X =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">rnorm</span>(N))  <span class="co"># a model matrix; first column represents the intercept</span></span>
<span id="cb325-4"><a href="models.html#cb325-4"></a>y =<span class="st"> </span><span class="dv">5</span> <span class="op">*</span><span class="st"> </span>X[, <span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="fl">.5</span> <span class="op">*</span><span class="st"> </span>X[, <span class="dv">2</span>] <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(N)  <span class="co"># a target with some noise; truth is y = 5 +.5*x</span></span>
<span id="cb325-5"><a href="models.html#cb325-5"></a></span>
<span id="cb325-6"><a href="models.html#cb325-6"></a>df =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> y, <span class="dt">x =</span> X[, <span class="dv">2</span>])</span></code></pre></div>
<p>Now let’s make some guesses for the coefficients, and see what the corresponding sum of the squared errors, i.e. the loss, would be.</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="models.html#cb326-1"></a><span class="kw">ls_loss</span>(X, y, <span class="dt">beta =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))    <span class="co"># guess 1</span></span></code></pre></div>
<pre><code>[1] 2467.106</code></pre>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="models.html#cb328-1"></a><span class="kw">ls_loss</span>(X, y, <span class="dt">beta =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))    <span class="co"># guess 2</span></span></code></pre></div>
<pre><code>[1] 1702.547</code></pre>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="models.html#cb330-1"></a><span class="kw">ls_loss</span>(X, y, <span class="dt">beta =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="fl">.25</span>))  <span class="co"># guess 3</span></span></code></pre></div>
<pre><code>[1] 179.2952</code></pre>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="models.html#cb332-1"></a>model =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, df)            <span class="co"># fit the model and obtain parameter estimates using OLS</span></span>
<span id="cb332-2"><a href="models.html#cb332-2"></a><span class="kw">coef</span>(model)                      <span class="co"># best guess given the data </span></span></code></pre></div>
<pre><code>(Intercept)           x 
  4.8971969   0.4475284 </code></pre>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="models.html#cb334-1"></a><span class="kw">sum</span>(<span class="kw">residuals</span>(model)<span class="op">^</span><span class="dv">2</span>)          <span class="co"># least squares loss</span></span></code></pre></div>
<pre><code>[1] 92.34413</code></pre>
<p>We see that in our third guess we reduce the loss quite a bit relative to our first guess. This makes sense because a value of 4 for the intercept and .25 for the coefficient for <code>x</code> are not as relatively far from the true values. However, we can also see that they are not the best we could have done. In addition, with more data, our estimated coefficients would get closer to true values.</p>
<p>In some relatively rare cases, a known approach is available and we do not have to search for the best estimates, but simply have to perform the correct steps that will result in them. For example, the following matrix operations will produce the best estimates for linear regression, which also happen to be the maximum likelihood estimates.</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="models.html#cb336-1"></a><span class="kw">solve</span>(<span class="kw">crossprod</span>(X)) <span class="op">%*%</span><span class="st"> </span><span class="kw">crossprod</span>(X, y)  <span class="co"># &#39;normal equations&#39;</span></span></code></pre></div>
<pre><code>          [,1]
[1,] 4.8971969
[2,] 0.4475284</code></pre>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="models.html#cb338-1"></a><span class="kw">coef</span>(model)</span></code></pre></div>
<pre><code>(Intercept)           x 
  4.8971969   0.4475284 </code></pre>
<p>Most of the time we don’t have such luxury, or even if we did, the computations might be too great for the size of our data.</p>
<p>Many statistical modeling techniques use <em>maximum likelihood</em> in some form or fashion, including Bayesian approaches, so you would do well to understand the basics. In this case, instead of minimizing the loss, we use an approach to maximize the probability of the observations of the target variable given the estimates of the parameters of the model (e.g. the coefficients in a regression)<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a>.</p>
<p>The following shows how this would look for estimating a single value like a mean for a set of observations from a specific distribution<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a>. In this case, the true underlying value that maximizes the likelihood is 5, but we typically don’t know this. We see that as our guesses for the mean would get closer to 5, the likelihood of the observed values increases. Our final guess based on the observed data won’t be exactly 5, but with enough data and an appropriate model for that data, we should get close.</p>
<p><img src="data-processing-and-visualization_files/figure-html/maximum-likelihood-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>Again, some simple conceptual code can help us. The next bit of code follows a similar approach to what we had with least squares regression, but the goal is instead to maximize the likelihood of the observed data. In this example, I fix the estimated variance, but in practice we’d need to estimate that parameter as well. As probabilities are typically very small, we work with them on the log scale.</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="models.html#cb340-1"></a>max_like &lt;-<span class="st"> </span><span class="cf">function</span>(X, y, beta, <span class="dt">sigma =</span> <span class="dv">1</span>) {</span>
<span id="cb340-2"><a href="models.html#cb340-2"></a>  </span>
<span id="cb340-3"><a href="models.html#cb340-3"></a>  likelihood =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">nrow</span>(X))</span>
<span id="cb340-4"><a href="models.html#cb340-4"></a>  y_hat =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">nrow</span>(X))</span>
<span id="cb340-5"><a href="models.html#cb340-5"></a>  </span>
<span id="cb340-6"><a href="models.html#cb340-6"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(X)) {</span>
<span id="cb340-7"><a href="models.html#cb340-7"></a>    y_hat[n] &lt;-<span class="st"> </span><span class="kw">sum</span>(X[n, ] <span class="op">*</span><span class="st"> </span>beta)</span>
<span id="cb340-8"><a href="models.html#cb340-8"></a>    likelihood[n] =<span class="st"> </span><span class="kw">dnorm</span>(y[n],  <span class="dt">mean =</span> y_hat[n], <span class="dt">sd =</span> sigma, <span class="dt">log =</span> <span class="ot">TRUE</span>)</span>
<span id="cb340-9"><a href="models.html#cb340-9"></a>  }</span>
<span id="cb340-10"><a href="models.html#cb340-10"></a>  </span>
<span id="cb340-11"><a href="models.html#cb340-11"></a>  <span class="kw">sum</span>(likelihood)  </span>
<span id="cb340-12"><a href="models.html#cb340-12"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="models.html#cb341-1"></a><span class="kw">max_like</span>(X, y, <span class="dt">beta =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))    <span class="co"># guess 1</span></span></code></pre></div>
<pre><code>[1] -1327.593</code></pre>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="models.html#cb343-1"></a><span class="kw">max_like</span>(X, y, <span class="dt">beta =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))    <span class="co"># guess 2</span></span></code></pre></div>
<pre><code>[1] -1022.18</code></pre>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="models.html#cb345-1"></a><span class="kw">max_like</span>(X, y, <span class="dt">beta =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="fl">.25</span>))  <span class="co"># guess 3</span></span></code></pre></div>
<pre><code>[1] -300.6741</code></pre>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="models.html#cb347-1"></a><span class="kw">logLik</span>(model)</span></code></pre></div>
<pre><code>&#39;log Lik.&#39; -137.9115 (df=3)</code></pre>
<p>To better understand maximum likelihood, it might help to think of our model from a data generating perspective, rather than in terms of ‘errors’. In the standard regression setting, we think of a single observation as follows:</p>
<p><span class="math display">\[\mu = b_0 + b_1*x_1 + ... + b_p*x_p\]</span></p>
<p>Or with matrix notation (consider it shorthand if not familiar):</p>
<p><span class="math display">\[\mu = X\beta\]</span></p>
<p>Now we display how <span class="math inline">\(y\)</span> is generated:</p>
<p><span class="math display">\[y \sim \mathcal{N}(\mathrm{mean} = \mu, \mathrm{sd} = \sigma)\]</span></p>
<p>In words, this means that our target observation <span class="math inline">\(y\)</span> is assumed to be normally distributed with some mean and some standard deviation/variance. The mean <span class="math inline">\(\mu\)</span> is a function, or simply weighted sum, of our covariates <span class="math inline">\(X\)</span>. The unknown parameters we have to estimate are the <span class="math inline">\(\beta\)</span>, i.e. weights, and standard deviation <span class="math inline">\(\sigma\)</span> (or variance <span class="math inline">\(\sigma^2\)</span>).</p>
<p>One more note regarding estimation, it is good to distinguish models from estimation procedures. The following shows the more specific to the more general for both models and estimation procedures respectively.</p>
<div style="width:500px; margin:0 auto; font-family:Roboto; ">
<div id="htmlwidget-2a358d522d1f42144f36" style="width:500px;height:250px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-2a358d522d1f42144f36">{"x":{"diagram":"digraph models {\ngraph [rankdir = LR  bgcolor=transparent]\n\nnode [shape = rectangle style=filled fillcolor=\"#990024BF\" color=gray80 width=.75]\n\nnode [fontcolor=white fontname=Roboto fixedsize=true fontsize=\"10%\"]\nLM; GLM; GLMM; GAMM;\n\nedge [color=gray50 style=filled]\nLM -> GLM; \nGLM -> GLMM; \nGLMM -> GAMM;\n\n// title\nlabelloc=\"t\";\nlabel=\"Models\\n\\n\";\nfontname=Roboto;\nfontcolor=gray50;\nfontsize=\"15%\"\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<div style="width:500px; margin:0 auto; font-family:Roboto; ">
<div id="htmlwidget-ddee11d9a5ad4b380912" style="width:500px;height:250px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-ddee11d9a5ad4b380912">{"x":{"diagram":"digraph models {\ngraph [rankdir = LR  bgcolor=transparent]\n\nnode [shape = rectangle style=filled fillcolor=\"#990024BF\" color=gray80 width=.75]\n\nnode [fontcolor=white fontname=Roboto fixedsize=true fontsize=\"10%\"]\nOLS; WLS; GLS; GEE; GMM;\n\nedge [color=gray50 style=filled]\nOLS -> WLS; \nWLS -> GLS; \nGLS -> GEE;\nGEE -> GMM;\n\n// title\nlabelloc=\"t\";\nlabel=\"Estimation Procedures\\n\\n\";\nfontname=Roboto;\nfontcolor=gray50;\nfontsize=\"20%\"\n\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<table class="table table" style="width: auto !important; margin-left: auto; margin-right: auto; font-size: 10px; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Label
</th>
<th style="text-align:left;">
Name
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
LM
</td>
<td style="text-align:left;">
Linear Model
</td>
</tr>
<tr>
<td style="text-align:left;">
GLM
</td>
<td style="text-align:left;">
Generalized Linear Model
</td>
</tr>
<tr>
<td style="text-align:left;">
GLMM
</td>
<td style="text-align:left;">
Generalized Linear Mixed Model
</td>
</tr>
<tr>
<td style="text-align:left;">
GAMM
</td>
<td style="text-align:left;">
Generalized Linear Mixed Model
</td>
</tr>
<tr>
<td style="text-align:left;">
OLS
</td>
<td style="text-align:left;">
Ordinary Least Squares
</td>
</tr>
<tr>
<td style="text-align:left;">
WLS
</td>
<td style="text-align:left;">
Weighted Least Squares
</td>
</tr>
<tr>
<td style="text-align:left;">
GLS
</td>
<td style="text-align:left;">
Generalized Least Squares
</td>
</tr>
<tr>
<td style="text-align:left;">
GEE
</td>
<td style="text-align:left;">
Generalized Estimating Equations
</td>
</tr>
<tr>
<td style="text-align:left;">
GMM
</td>
<td style="text-align:left;">
Generalized Method of Moments
</td>
</tr>
</tbody>
</table>
</div>
<div id="optimization" class="section level3">
<h3>Optimization</h3>
<p>So we know the goal, but how do we get to it? In practice, we typically use <em>optimization</em> methods to iteratively search for the best estimates for the parameters of a given model. The functions we explored above provide a goal- to minimize loss (however defined- least squares for continuous, classification error for binary, etc.) or maximize the likelihood (or posterior probability in the Bayesian context). Whatever the goal, an optimizing <em>algorithm</em> will typically be used to find the estimates that reach that goal. Some approaches are very general, some are better for certain types of modeling problems. These algorithms continue to make guesses until some criterion has been reached (<em>convergence</em>)<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a>.</p>
<p>You generally don’t need to know the details to use these algorithms to fit models, but knowing a little bit about the optimization process and available options may prove useful to deal with more complex data scenarios, where convergence can be difficult. Some packages will even have documentation specifically dealing with convergence issues. In the more predictive models previously discussed, knowing more about the optimization algorithm may speedup the time it takes to train the model, or smooth out the variability in the process.</p>
<p><strong>Blurb about MCMC</strong></p>
</div>
</div>
<div id="fitting-models" class="section level2">
<h2>Fitting Models</h2>
<p>With practically every modern modeling package in R, the two components required to fit a model are the model formula, and a data frame that contains the variables specified in that formula. Consider the following models. In general the syntax is the same or identical, with special considerations for the type of model. The data argument is not included.</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="models.html#cb349-1"></a><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>z)                                          <span class="co"># standard linear model/OLS</span></span>
<span id="cb349-2"><a href="models.html#cb349-2"></a><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>z, <span class="dt">family =</span> <span class="st">&#39;binomial&#39;</span>)                    <span class="co"># logistic regression with binary response</span></span>
<span id="cb349-3"><a href="models.html#cb349-3"></a><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>z <span class="op">+</span><span class="st"> </span><span class="kw">offset</span>(<span class="kw">log</span>(q)), <span class="dt">family =</span> <span class="st">&#39;poisson&#39;</span>)    <span class="co"># count/rate model</span></span>
<span id="cb349-4"><a href="models.html#cb349-4"></a>pscl<span class="op">::</span><span class="kw">hurdle</span>(y <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>z, <span class="dt">dist =</span> <span class="st">&quot;negbin&quot;</span>)               <span class="co"># hurdle model with negative binomial response</span></span>
<span id="cb349-5"><a href="models.html#cb349-5"></a>lme4<span class="op">::</span><span class="kw">glmer</span>(y <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>group), <span class="dt">family =</span> <span class="st">&#39;binomial&#39;</span>)  <span class="co"># generalized linear mixed model</span></span>
<span id="cb349-6"><a href="models.html#cb349-6"></a>mgcv<span class="op">::</span><span class="kw">gam</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">s</span>(x))                                    <span class="co"># generalized additive model</span></span>
<span id="cb349-7"><a href="models.html#cb349-7"></a>survival<span class="op">::</span><span class="kw">coxph</span>(<span class="kw">Surv</span>(<span class="dt">time =</span> t, <span class="dt">event =</span> q) <span class="op">~</span><span class="st"> </span>x)         <span class="co"># Cox Proportional Hazards Regression</span></span>
<span id="cb349-8"><a href="models.html#cb349-8"></a></span>
<span id="cb349-9"><a href="models.html#cb349-9"></a><span class="co"># Bayesian</span></span>
<span id="cb349-10"><a href="models.html#cb349-10"></a>brms<span class="op">::</span><span class="kw">brm</span>(</span>
<span id="cb349-11"><a href="models.html#cb349-11"></a>  y <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>x <span class="op">|</span><span class="st"> </span>group), </span>
<span id="cb349-12"><a href="models.html#cb349-12"></a>  <span class="dt">family =</span> <span class="st">&#39;zero_one_inflated_beta&#39;</span>, </span>
<span id="cb349-13"><a href="models.html#cb349-13"></a>  <span class="dt">prior =</span> priors</span>
<span id="cb349-14"><a href="models.html#cb349-14"></a>)</span></code></pre></div>
<p>For examples of many other types of models, see this <a href="https://m-clark.github.io/R-models/">document</a>.</p>
<p>Let’s run an example. We’ll use the world happiness dataset<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a>. This is country level data based on surveys taken at various years, and the scores are averages or proportions, along with other values like GDP.</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="models.html#cb350-1"></a><span class="kw">library</span>(tidyverse)  <span class="co"># load if you haven&#39;t already</span></span>
<span id="cb350-2"><a href="models.html#cb350-2"></a></span>
<span id="cb350-3"><a href="models.html#cb350-3"></a><span class="kw">load</span>(<span class="st">&#39;data/world_happiness.RData&#39;</span>)</span>
<span id="cb350-4"><a href="models.html#cb350-4"></a></span>
<span id="cb350-5"><a href="models.html#cb350-5"></a><span class="co"># glimpse(happy)</span></span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:right;">
N
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
SD
</th>
<th style="text-align:right;">
Min
</th>
<th style="text-align:right;">
Q1
</th>
<th style="text-align:right;">
Median
</th>
<th style="text-align:right;">
Q3
</th>
<th style="text-align:right;">
Max
</th>
<th style="text-align:right;">
Missing
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
year
</td>
<td style="text-align:right;">
1704
</td>
<td style="text-align:right;">
2012.33
</td>
<td style="text-align:right;">
3.69
</td>
<td style="text-align:right;">
2005.00
</td>
<td style="text-align:right;">
2009.00
</td>
<td style="text-align:right;">
2012.00
</td>
<td style="text-align:right;">
2015.00
</td>
<td style="text-align:right;">
2018.00
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
life_ladder
</td>
<td style="text-align:right;">
1704
</td>
<td style="text-align:right;">
5.44
</td>
<td style="text-align:right;">
1.12
</td>
<td style="text-align:right;">
2.66
</td>
<td style="text-align:right;">
4.61
</td>
<td style="text-align:right;">
5.34
</td>
<td style="text-align:right;">
6.27
</td>
<td style="text-align:right;">
8.02
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
log_gdp_per_capita
</td>
<td style="text-align:right;">
1676
</td>
<td style="text-align:right;">
9.22
</td>
<td style="text-align:right;">
1.19
</td>
<td style="text-align:right;">
6.46
</td>
<td style="text-align:right;">
8.30
</td>
<td style="text-align:right;">
9.41
</td>
<td style="text-align:right;">
10.19
</td>
<td style="text-align:right;">
11.77
</td>
<td style="text-align:right;">
28
</td>
</tr>
<tr>
<td style="text-align:left;">
social_support
</td>
<td style="text-align:right;">
1691
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.12
</td>
<td style="text-align:right;">
0.29
</td>
<td style="text-align:right;">
0.75
</td>
<td style="text-align:right;">
0.83
</td>
<td style="text-align:right;">
0.90
</td>
<td style="text-align:right;">
0.99
</td>
<td style="text-align:right;">
13
</td>
</tr>
<tr>
<td style="text-align:left;">
healthy_life_expectancy_at_birth
</td>
<td style="text-align:right;">
1676
</td>
<td style="text-align:right;">
63.11
</td>
<td style="text-align:right;">
7.58
</td>
<td style="text-align:right;">
32.30
</td>
<td style="text-align:right;">
58.30
</td>
<td style="text-align:right;">
65.00
</td>
<td style="text-align:right;">
68.30
</td>
<td style="text-align:right;">
76.80
</td>
<td style="text-align:right;">
28
</td>
</tr>
<tr>
<td style="text-align:left;">
freedom_to_make_life_choices
</td>
<td style="text-align:right;">
1675
</td>
<td style="text-align:right;">
0.73
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:right;">
0.26
</td>
<td style="text-align:right;">
0.64
</td>
<td style="text-align:right;">
0.75
</td>
<td style="text-align:right;">
0.85
</td>
<td style="text-align:right;">
0.99
</td>
<td style="text-align:right;">
29
</td>
</tr>
<tr>
<td style="text-align:left;">
generosity
</td>
<td style="text-align:right;">
1622
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.16
</td>
<td style="text-align:right;">
-0.34
</td>
<td style="text-align:right;">
-0.12
</td>
<td style="text-align:right;">
-0.02
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
0.68
</td>
<td style="text-align:right;">
82
</td>
</tr>
<tr>
<td style="text-align:left;">
perceptions_of_corruption
</td>
<td style="text-align:right;">
1608
</td>
<td style="text-align:right;">
0.75
</td>
<td style="text-align:right;">
0.19
</td>
<td style="text-align:right;">
0.04
</td>
<td style="text-align:right;">
0.70
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.88
</td>
<td style="text-align:right;">
0.98
</td>
<td style="text-align:right;">
96
</td>
</tr>
<tr>
<td style="text-align:left;">
positive_affect
</td>
<td style="text-align:right;">
1685
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
0.11
</td>
<td style="text-align:right;">
0.36
</td>
<td style="text-align:right;">
0.62
</td>
<td style="text-align:right;">
0.72
</td>
<td style="text-align:right;">
0.80
</td>
<td style="text-align:right;">
0.94
</td>
<td style="text-align:right;">
19
</td>
</tr>
<tr>
<td style="text-align:left;">
negative_affect
</td>
<td style="text-align:right;">
1691
</td>
<td style="text-align:right;">
0.27
</td>
<td style="text-align:right;">
0.08
</td>
<td style="text-align:right;">
0.08
</td>
<td style="text-align:right;">
0.21
</td>
<td style="text-align:right;">
0.25
</td>
<td style="text-align:right;">
0.31
</td>
<td style="text-align:right;">
0.70
</td>
<td style="text-align:right;">
13
</td>
</tr>
<tr>
<td style="text-align:left;">
confidence_in_national_government
</td>
<td style="text-align:right;">
1530
</td>
<td style="text-align:right;">
0.48
</td>
<td style="text-align:right;">
0.19
</td>
<td style="text-align:right;">
0.07
</td>
<td style="text-align:right;">
0.33
</td>
<td style="text-align:right;">
0.46
</td>
<td style="text-align:right;">
0.61
</td>
<td style="text-align:right;">
0.99
</td>
<td style="text-align:right;">
174
</td>
</tr>
<tr>
<td style="text-align:left;">
democratic_quality
</td>
<td style="text-align:right;">
1558
</td>
<td style="text-align:right;">
-0.14
</td>
<td style="text-align:right;">
0.88
</td>
<td style="text-align:right;">
-2.45
</td>
<td style="text-align:right;">
-0.79
</td>
<td style="text-align:right;">
-0.23
</td>
<td style="text-align:right;">
0.65
</td>
<td style="text-align:right;">
1.58
</td>
<td style="text-align:right;">
146
</td>
</tr>
<tr>
<td style="text-align:left;">
delivery_quality
</td>
<td style="text-align:right;">
1559
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.98
</td>
<td style="text-align:right;">
-2.14
</td>
<td style="text-align:right;">
-0.71
</td>
<td style="text-align:right;">
-0.22
</td>
<td style="text-align:right;">
0.70
</td>
<td style="text-align:right;">
2.18
</td>
<td style="text-align:right;">
145
</td>
</tr>
<tr>
<td style="text-align:left;">
gini_index_world_bank_estimate
</td>
<td style="text-align:right;">
643
</td>
<td style="text-align:right;">
0.37
</td>
<td style="text-align:right;">
0.08
</td>
<td style="text-align:right;">
0.24
</td>
<td style="text-align:right;">
0.30
</td>
<td style="text-align:right;">
0.35
</td>
<td style="text-align:right;">
0.43
</td>
<td style="text-align:right;">
0.63
</td>
<td style="text-align:right;">
1061
</td>
</tr>
<tr>
<td style="text-align:left;">
happiness_score
</td>
<td style="text-align:right;">
554
</td>
<td style="text-align:right;">
5.41
</td>
<td style="text-align:right;">
1.13
</td>
<td style="text-align:right;">
2.69
</td>
<td style="text-align:right;">
4.51
</td>
<td style="text-align:right;">
5.31
</td>
<td style="text-align:right;">
6.32
</td>
<td style="text-align:right;">
7.63
</td>
<td style="text-align:right;">
1150
</td>
</tr>
<tr>
<td style="text-align:left;">
dystopia_residual
</td>
<td style="text-align:right;">
554
</td>
<td style="text-align:right;">
2.06
</td>
<td style="text-align:right;">
0.55
</td>
<td style="text-align:right;">
0.29
</td>
<td style="text-align:right;">
1.72
</td>
<td style="text-align:right;">
2.06
</td>
<td style="text-align:right;">
2.44
</td>
<td style="text-align:right;">
3.84
</td>
<td style="text-align:right;">
1150
</td>
</tr>
</tbody>
</table>
<p>The happiness score itself ranges from 2.7 to 7.6, with a mean of 5.4 and standard deviation of 1.1.</p>
<p>Fitting a model with R is trivial, and at a minimum requires the two key ingredients mentioned before, the formula and data.</p>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="models.html#cb351-1"></a>happy_model_base =<span class="st"> </span><span class="kw">lm</span>(</span>
<span id="cb351-2"><a href="models.html#cb351-2"></a>  happiness_score <span class="op">~</span><span class="st"> </span>democratic_quality <span class="op">+</span><span class="st"> </span>generosity <span class="op">+</span><span class="st"> </span>log_gdp_per_capita,</span>
<span id="cb351-3"><a href="models.html#cb351-3"></a>  <span class="dt">data =</span> happy</span>
<span id="cb351-4"><a href="models.html#cb351-4"></a>)</span></code></pre></div>
<div id="using-matrices" class="section level3">
<h3>Using matrices</h3>
<p>Many packages still allow for matrix input instead of specifying a model formula, or even require it (but shouldn’t). This means separating data into a model (or design) matrix, and the vector or matrix of the target variable(s). For example, if we needed a speed boost and weren’t concerned about some typical output we could use <span class="func" style="">lm.fit</span>.</p>
<p>First we need to create the required components.</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="models.html#cb352-1"></a>X =<span class="st"> </span><span class="kw">model.matrix</span>(</span>
<span id="cb352-2"><a href="models.html#cb352-2"></a>  happiness_score <span class="op">~</span><span class="st"> </span>democratic_quality <span class="op">+</span><span class="st"> </span>generosity <span class="op">+</span><span class="st"> </span>log_gdp_per_capita, </span>
<span id="cb352-3"><a href="models.html#cb352-3"></a>  <span class="dt">data =</span> happy</span>
<span id="cb352-4"><a href="models.html#cb352-4"></a>)</span>
<span id="cb352-5"><a href="models.html#cb352-5"></a></span>
<span id="cb352-6"><a href="models.html#cb352-6"></a><span class="kw">head</span>(X)</span></code></pre></div>
<pre><code>   (Intercept) democratic_quality  generosity log_gdp_per_capita
8            1         -1.8443636  0.08909068           7.500539
9            1         -1.8554263  0.05136492           7.497038
10           1         -1.8865659 -0.11219829           7.497755
19           1          0.2516293 -0.08441135           9.302960
20           1          0.2572919 -0.02068741           9.337532
21           1          0.2999450 -0.03264282           9.376145</code></pre>
<p>Note the column of ones in the model matrix <code>X</code>. This represents our intercept, but may not mean much to you unless you understand <a href="http://matrixmultiplication.xyz/">matrix multiplication</a>. The other columns are just as they are in the data. Note also that the missing values have been removed.</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="models.html#cb354-1"></a><span class="kw">nrow</span>(happy)</span></code></pre></div>
<pre><code>[1] 1704</code></pre>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="models.html#cb356-1"></a><span class="kw">nrow</span>(X)</span></code></pre></div>
<pre><code>[1] 411</code></pre>
<p>We can now fit the model as follows. The target variable must contain the same number of observations as in the model matrix.</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="models.html#cb358-1"></a>y =<span class="st"> </span>happy<span class="op">$</span>happiness_score[<span class="kw">as.integer</span>(<span class="kw">rownames</span>(X))]</span>
<span id="cb358-2"><a href="models.html#cb358-2"></a>happy_model_matrix =<span class="st"> </span><span class="kw">lm.fit</span>(X, y)</span>
<span id="cb358-3"><a href="models.html#cb358-3"></a><span class="kw">summary</span>(happy_model_matrix)</span></code></pre></div>
<pre><code>              Length Class  Mode   
coefficients    4    -none- numeric
residuals     411    -none- numeric
effects       411    -none- numeric
rank            1    -none- numeric
fitted.values 411    -none- numeric
assign          4    -none- numeric
qr              5    qr     list   
df.residual     1    -none- numeric</code></pre>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="models.html#cb360-1"></a><span class="kw">coef</span>(happy_model_matrix)</span></code></pre></div>
<pre><code>       (Intercept) democratic_quality         generosity log_gdp_per_capita 
        -1.0104775          0.1703734          1.1608465          0.6934213 </code></pre>
<p>In my experience, it is generally a bad sign if a package requires that you create the model matrix rather than doing so itself. I typically find that such packages tend to skip out on many other things like using typical methods like <span class="func" style="">predict</span>, <span class="func" style="">coef</span>, etc., making them even more difficult to work with. In general, the only real time you should need to use model matrices is when you are creating your own modeling package, doing simulations, utilizing model speed-ups, or otherwise know why you need them.</p>
</div>
</div>
<div id="summarizing-models" class="section level2">
<h2>Summarizing Models</h2>
<p>Once we have a model, we’ll want to summarize the results of it. Most modeling packages have a summary method we can apply, which will provide parameter estimates, some notion of model fit, inferential statistics, and other output.</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="models.html#cb362-1"></a>happy_model_base_sum =<span class="st"> </span><span class="kw">summary</span>(happy_model_base)</span></code></pre></div>
<pre><code>
Call:
lm(formula = happiness_score ~ democratic_quality + generosity + 
    log_gdp_per_capita, data = happy)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.75376 -0.45585 -0.00307  0.46013  1.69925 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        -1.01048    0.31436  -3.214 0.001412 ** 
democratic_quality  0.17037    0.04588   3.714 0.000233 ***
generosity          1.16085    0.19548   5.938 6.18e-09 ***
log_gdp_per_capita  0.69342    0.03335  20.792  &lt; 2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.6283 on 407 degrees of freedom
  (1293 observations deleted due to missingness)
Multiple R-squared:  0.6953,    Adjusted R-squared:  0.6931 
F-statistic: 309.6 on 3 and 407 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The summary provides several pieces of information: the coefficients or weights (<code>Estimate</code>), the standard errors, the t-statistic (which is just the coefficient divided by the standard error), and the corresponding p-value. While we can see that all these covariates are statistically significant, which doesn’t tell us much, but serves as a starting point. The main thing to look at are the actual coefficients. For example, with regard to the effect of log GDP, moving one percentage point on GDP<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a> results in roughly 0.7 units of happiness. Knowing the scale of the outcome can help us understand the magnitude of the effect in a general sense. Before we showed that the standard deviation of the happiness scale was 1.1. So this would typically represent a notable effect in terms of standard deviation units- moving a percentage point on GDP would result in a 0.6 standard deviation increase in state-level happiness.</p>
<p>One thing we must also have in order to understand our results is to get a sense of the uncertainty in the effects. The following provides confidence intervals for each of the coefficients.</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="models.html#cb364-1"></a><span class="kw">confint</span>(happy_model_base)</span></code></pre></div>
<pre><code>                         2.5 %     97.5 %
(Intercept)        -1.62845472 -0.3925003
democratic_quality  0.08018814  0.2605586
generosity          0.77656244  1.5451306
log_gdp_per_capita  0.62786210  0.7589806</code></pre>
<p>Now we have a sense of the range of plausible values for the coefficients. The value we actually estimate is the best guess given our circumstances, but slight changes in the data, the way we collect it, the time we collect it, etc., all would result in a slightly different result. The confidence interval provides a range of what we could expect given the uncertainty.</p>
</div>
<div id="variable-transformations" class="section level2">
<h2>Variable Transformations</h2>
<p>Transforming variables can provide a few benefits, whether applied to the target, covariates, or both, and should regularly be used for most models. Some of these benefits include<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a>:</p>
<ul>
<li>Interpretable intercepts</li>
<li>More comparable covariate effects</li>
<li>Faster estimation</li>
<li>Easier convergence</li>
<li>Help with heteroscedasticity</li>
</ul>
<p>For example, merely centering predictor variables, i.e. subtracting the mean, provides a more interpretable intercept that will fall within the actual range of the target variable, telling us what the value of the target variable is when the covariates are at their means (or reference value if categorical).</p>
<div id="numeric-variables" class="section level3">
<h3>Numeric variables</h3>
<p>The following table shows the interpretation of two extremely common transformations applied to numeric variables- logging and scaling (i.e. standardizing to mean zero, standard deviation one).</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
target
</th>
<th style="text-align:left;">
predictor
</th>
<th style="text-align:left;">
interpretation
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
y
</td>
<td style="text-align:left;">
x
</td>
<td style="text-align:left;">
<span class="math inline">\(\Delta y = \beta\Delta x\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
y
</td>
<td style="text-align:left;">
log(x)
</td>
<td style="text-align:left;">
<span class="math inline">\(\Delta y \approx (\beta/100)\%\Delta x\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
log(y)
</td>
<td style="text-align:left;">
x
</td>
<td style="text-align:left;">
<span class="math inline">\(\%\Delta y \approx 100\cdot \beta\%\Delta x\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
log(y)
</td>
<td style="text-align:left;">
log(x)
</td>
<td style="text-align:left;">
<span class="math inline">\(\%\Delta y = \beta\%\Delta x\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
y
</td>
<td style="text-align:left;">
scale(x)
</td>
<td style="text-align:left;">
<span class="math inline">\(\Delta y = \beta\sigma\Delta x\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
scale(y)
</td>
<td style="text-align:left;">
x
</td>
<td style="text-align:left;">
<span class="math inline">\(\sigma\Delta y = \beta\Delta x\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
scale(y)
</td>
<td style="text-align:left;">
scale(x)
</td>
<td style="text-align:left;">
<span class="math inline">\(\sigma\Delta y = \beta\sigma\Delta x\)</span>
</td>
</tr>
</tbody>
</table>
<p>For example, to start with the normal situation, a one-unit change in <span class="math inline">\(x\)</span>, i.e. <span class="math inline">\(\Delta x =1\)</span>, leads to <span class="math inline">\(\beta\)</span> unit change in <span class="math inline">\(y\)</span>. If we log the target variable <span class="math inline">\(y\)</span>, the interpretation of the coefficient for <span class="math inline">\(x\)</span> is that a one-unit change in <span class="math inline">\(x\)</span> leads to an (approximately) 100<span class="math inline">\(\cdot\)</span><span class="math inline">\(\beta\)</span>% change in <span class="math inline">\(y\)</span>. The 100 changes the result from a proportion to percentage change. More concretely, if <span class="math inline">\(\beta\)</span> was .5, a unit change in <span class="math inline">\(x\)</span> leads to (roughly) a 50% change in <span class="math inline">\(y\)</span>. If both were logged, a percentage change in <span class="math inline">\(x\)</span> leads to a <span class="math inline">\(\beta\)</span> percentage change in y<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a>. These percentage change interpretations are called <a href="https://en.wikipedia.org/wiki/Elasticity_(economics)">elasticities</a> in econometrics and areas trained similarly.</p>
<p>It is very common to use <em>standardized</em> variables as well, also called normalizing, or simply scaling. If <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> are both standardized, a one unit (i.e. one standard deviation) change in <span class="math inline">\(x\)</span> leads to a <span class="math inline">\(\beta\)</span> standard deviation change in <span class="math inline">\(y\)</span>. Again, if <span class="math inline">\(\beta\)</span> was .5, a standard deviation change in <span class="math inline">\(x\)</span> leads to a half standard deviation change in <span class="math inline">\(y\)</span>.</p>
<p>Another common transformation, particularly in machine learning, is the <em>min-max normalization</em>, changing variables to range from some minimum to some maximum, usually zero to one.</p>
</div>
<div id="categorical-variables" class="section level3">
<h3>Categorical variables</h3>
<p>For categorical variables, we can employ what is called <em>effects coding</em> to test for specific types of group differences. Far and away the most common approach is called <em>dummy coding</em> or <em>one-hot encoding</em><a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a>. For example:</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="models.html#cb366-1"></a><span class="kw">library</span>(recipes)</span>
<span id="cb366-2"><a href="models.html#cb366-2"></a></span>
<span id="cb366-3"><a href="models.html#cb366-3"></a>nafta =<span class="st"> </span>happy <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb366-4"><a href="models.html#cb366-4"></a><span class="st">  </span><span class="kw">filter</span>(country <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;United States&#39;</span>, <span class="st">&#39;Canada&#39;</span>, <span class="st">&#39;Mexico&#39;</span>))</span>
<span id="cb366-5"><a href="models.html#cb366-5"></a></span>
<span id="cb366-6"><a href="models.html#cb366-6"></a>dummy =<span class="st"> </span>nafta  <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb366-7"><a href="models.html#cb366-7"></a><span class="st">  </span><span class="kw">recipe</span>(<span class="op">~</span><span class="st"> </span>country) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb366-8"><a href="models.html#cb366-8"></a><span class="st">  </span><span class="kw">step_dummy</span>(country, <span class="dt">one_hot =</span> <span class="ot">TRUE</span>) <span class="co"># make variables for all factor levels</span></span>
<span id="cb366-9"><a href="models.html#cb366-9"></a></span>
<span id="cb366-10"><a href="models.html#cb366-10"></a><span class="kw">prep</span>(dummy) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb366-11"><a href="models.html#cb366-11"></a><span class="st">  </span><span class="kw">bake</span>(nafta) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb366-12"><a href="models.html#cb366-12"></a><span class="st">  </span><span class="kw">print</span>(<span class="dt">n =</span> <span class="dv">20</span>)</span></code></pre></div>
<pre><code># A tibble: 39 x 3
   country_Canada country_Mexico country_United.States
            &lt;dbl&gt;          &lt;dbl&gt;                 &lt;dbl&gt;
 1              1              0                     0
 2              1              0                     0
 3              1              0                     0
 4              1              0                     0
 5              1              0                     0
 6              1              0                     0
 7              1              0                     0
 8              1              0                     0
 9              1              0                     0
10              1              0                     0
11              1              0                     0
12              1              0                     0
13              1              0                     0
14              0              1                     0
15              0              1                     0
16              0              1                     0
17              0              1                     0
18              0              1                     0
19              0              1                     0
20              0              1                     0
# … with 19 more rows</code></pre>
<p>We see that the first few observations are Canada, and the next few Mexico. This is almost never required for R modeling packages, but sometimes can be useful to do explicitly. If your modeling package cannot handle factor variables, and thus requires explicit coding, you’ll know very quickly.</p>
<p>Let’s run a regression as follows:</p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="models.html#cb368-1"></a>model_dummy =<span class="st"> </span><span class="kw">lm</span>(happiness_score <span class="op">~</span><span class="st"> </span>country, <span class="dt">data =</span> nafta)</span>
<span id="cb368-2"><a href="models.html#cb368-2"></a></span>
<span id="cb368-3"><a href="models.html#cb368-3"></a><span class="kw">summary</span>(model_dummy)</span></code></pre></div>
<pre><code>
Call:
lm(formula = happiness_score ~ country, data = nafta)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.26960 -0.07453 -0.00615  0.06322  0.42920 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)           7.36887    0.09633  76.493 5.64e-14 ***
countryMexico        -0.61107    0.13624  -4.485  0.00152 ** 
countryUnited States -0.34337    0.13624  -2.520  0.03275 *  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.1927 on 9 degrees of freedom
  (27 observations deleted due to missingness)
Multiple R-squared:  0.692, Adjusted R-squared:  0.6236 
F-statistic: 10.11 on 2 and 9 DF,  p-value: 0.004994</code></pre>
<p>In this case, the coefficient represents the difference in means on the target variable between the reference group and the group in question. In this case, the U.S. is -0.34 less on the happy score than the reference country (Canada).</p>
<p>Other codings are possible, and these would allow for specific group comparisons or types of comparisons. This is sometimes called <em>contrast coding</em>. For example, we could compare Canada vs. both the U.S. and Mexico. By giving Canada twice the weight of the other two we can get this result. I also add a coding that will just compare Mexico vs. the U.S. The actual weights used are aribtrary but in this case should sum to zero.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
group
</th>
<th style="text-align:right;">
canada_vs_other
</th>
<th style="text-align:right;">
mexico_vs_us
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Canada
</td>
<td style="text-align:right;">
-0.667
</td>
<td style="text-align:right;">
0.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Mexico
</td>
<td style="text-align:right;">
0.333
</td>
<td style="text-align:right;">
-0.5
</td>
</tr>
<tr>
<td style="text-align:left;">
United States
</td>
<td style="text-align:right;">
0.333
</td>
<td style="text-align:right;">
0.5
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; border:0;" colspan="100%">
<sup></sup> weights sum to zero, but are arbitrary
</td>
</tr>
</tfoot>
</table>
<p>Adding such coding to a factor variable allows the corresonding models to use it in constructing the model matrix, rather than dummy coding. See the group means and calculate the results by hand for yourself.</p>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="models.html#cb370-1"></a>nafta =<span class="st"> </span>nafta <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb370-2"><a href="models.html#cb370-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">country_fac =</span> <span class="kw">factor</span>(country))</span>
<span id="cb370-3"><a href="models.html#cb370-3"></a></span>
<span id="cb370-4"><a href="models.html#cb370-4"></a><span class="kw">contrasts</span>(nafta<span class="op">$</span>country_fac) =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span><span class="op">/</span><span class="dv">3</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="kw">c</span>(<span class="dv">0</span>,<span class="op">-</span>.<span class="dv">5</span>,.<span class="dv">5</span>)), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb370-5"><a href="models.html#cb370-5"></a></span>
<span id="cb370-6"><a href="models.html#cb370-6"></a><span class="kw">summary</span>(<span class="kw">lm</span>(happiness_score <span class="op">~</span><span class="st"> </span>country_fac, <span class="dt">data =</span> nafta))</span></code></pre></div>
<pre><code>
Call:
lm(formula = happiness_score ~ country_fac, data = nafta)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.26960 -0.07453 -0.00615  0.06322  0.42920 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   7.05072    0.05562 126.769 6.01e-16 ***
country_fac1 -0.47722    0.11799  -4.045  0.00291 ** 
country_fac2  0.26770    0.13624   1.965  0.08100 .  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.1927 on 9 degrees of freedom
  (27 observations deleted due to missingness)
Multiple R-squared:  0.692, Adjusted R-squared:  0.6236 
F-statistic: 10.11 on 2 and 9 DF,  p-value: 0.004994</code></pre>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="models.html#cb372-1"></a>nafta <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb372-2"><a href="models.html#cb372-2"></a><span class="st">  </span><span class="kw">group_by</span>(country) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb372-3"><a href="models.html#cb372-3"></a><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">happy =</span> <span class="kw">mean</span>(happiness_score, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))</span></code></pre></div>
<pre><code># A tibble: 3 x 2
  country       happy
  &lt;chr&gt;         &lt;dbl&gt;
1 Canada         7.37
2 Mexico         6.76
3 United States  7.03</code></pre>
<p>For example, we can see that for this balanced data set, the corresponding coefficient is the halfway point, i.e. average, of the U.S. and Mexico coefficients from dummy coding: -0.611 + -0.343 / 2 = -0.477.</p>
<p>In other circumstances, we can use <em>categorical embeddings</em> to reduce a very large number of categorical levels to a smaller number of numeric variables. This is very commonly employed in deep learning.</p>
</div>
<div id="scales-indices-and-dimension-reduction" class="section level3">
<h3>Scales, indices, and dimension reduction</h3>
<p>It is often the case that we have several correlated variables/items which do not all need to go into the model. For example, instead of using all items in a psychological scale, we can use the scale score, however defined, which is often just a <em>sum score</em> of the underlying items. Often people will create an index by using a <em>principal components analysis</em>, which can be thought of as a means to create a weighted sum score, or set of scores. Some (especially binary) items may tend toward the creation of a single variable that simply notes whether any of those collection of variables was present or not.</p>
<div id="two-step-approaches" class="section level4">
<h4>Two-step approaches</h4>
<p>Some might do a preliminary analysis, such as a <em>cluster analysis</em> or <em>factor analysis</em>, to create new target or predictor variables. In the former we reduce several variables to a single categorical label. Factor analysis does the same but results in a more expressive continuous metric. While fine to use, the corresponding results are measured with error, so treating the categories or factor scores as you would observed variables will typically result in optimistic results when you later include them in a subsequent analysis like a linear regression. Though this difference is probably slight in most applications, keen reviewers would probably point out the model shortcoming.</p>
</div>
</div>
<div id="dont-discretize" class="section level3">
<h3>Don’t discretize</h3>
<p>Little pains advanced modelers more than seeing results where a nice expressive continuous metric butchered into two categories. There is almost never a reason to do this. There are reasons to collapse rare labels of a categorical variable, so that the new variable has fewer but more frequent categories, but even that can cause problems, and doesn’t really overcome the fact that you simply didn’t have enough data to begin with.</p>
</div>
</div>
<div id="variable-importance" class="section level2">
<h2>Variable Importance</h2>
<p>In many circumstances, one of the modeling goals is to determine which predictor variable is most important out of the collection used in the model. However, determining relative variable importance is at best an approximation with some methods, and a fairly hopeless endeavor with others. For just basic linear regression there are many methods that would not necessarily come to the same conclusions. Some believe that standardizing variables, i.e. putting them on the same scale (e.g. mean 0 sd 1), is enough, but it is not, and doesn’t help with comparison to categorical inputs. If you’re model is not strong, it doesn’t make much sense to even worry about which is the best of a bad lot.</p>
<p>Another reason that ‘importance’ is a problematic endeavor is that a statistical result doesn’t speak to practical action, nor does it speak to the fact that small effects may be very important. Sex may be an important driver in social science model, but we cannot do anything about a person’s sex for many outcomes that may be of interest. With health outcomes, any effects might be worthy of attention, however small, if they could practically increase the likelihood of survival.</p>
<p>Even if you can come up with a metric you like, you would still need some measure of uncertainty around that to make a claim that one predictor is statistically better than another, and the only real approach to do that is some computationally expensive procedure that you will likely have to put together by hand.</p>
<p>As an example, for standard linear regression there are many methods that decompose <span class="math inline">\(R^2\)</span> into relative contributions by the covariates. The tools to do so have to re-run the model in many ways to produce these estimates (see the <span class="pack" style="">relaimpo</span> package for example) but you would have to use bootstrapping or similar to get interval estimates for those measures of importance. Certain techniques like random forests have a natural way to provide variable importance metrics, but providing inference on them would similarly be very computationally expensive.</p>
<p>In the end though, I think it is probably best to assume that any effect that seems statistically distinct from zero might be worthy of attention, and can be regarded for its own sake. You can reduce</p>
</div>
<div id="extracting-output" class="section level2">
<h2>Extracting Output</h2>
<p>The better you get at modeling, the more often you are going to need to get at certain parts of the model output easily. For example, we can extract t</p>
<p>Why would you want to do this? A simple example would be to compare effects across different settings. We can collect the values, put them in a data frame, and then to a table or visualization.</p>
<p>Typical modeling methods you might want to use:</p>
<ul>
<li><span class="func" style="">summary</span>: print results in a legible way</li>
<li><span class="func" style="">plot</span>: plot something about the model (e.g. diagnostic plots)</li>
<li><span class="func" style="">predict</span>: make predictions, possibly on new data</li>
<li><span class="func" style="">confint</span>: get confidence intervals for parameters</li>
<li><span class="func" style="">coef</span>: extract coefficients</li>
<li><span class="func" style="">fitted</span>: extract fitted values</li>
<li><span class="func" style="">residuals</span>: extract residuals</li>
<li><span class="func" style="">AIC</span>: extract AIC</li>
</ul>
<p>Here is an example of using the <span class="func" style="">predict</span> and <span class="func" style="">coef</span> methods.</p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="models.html#cb374-1"></a><span class="kw">predict</span>(happy_model_base, <span class="dt">newdata =</span> happy <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>))</span></code></pre></div>
<pre><code>       1        2        3        4        5 
3.838179 3.959046 3.928180 4.004129 4.171624 </code></pre>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="models.html#cb376-1"></a><span class="kw">coef</span>(happy_model_base)</span></code></pre></div>
<pre><code>       (Intercept) democratic_quality         generosity log_gdp_per_capita 
        -1.0104775          0.1703734          1.1608465          0.6934213 </code></pre>
<p>Also, it’s useful to assign the summary results to an object, so that you can extract things that are also useful but would not be in the model object. We did this before, so now let’s take a look.</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="models.html#cb378-1"></a><span class="kw">str</span>(happy_model_base_sum, <span class="dv">1</span>)</span></code></pre></div>
<pre><code>List of 12
 $ call         : language lm(formula = happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, data = happy)
 $ terms        :Classes &#39;terms&#39;, &#39;formula&#39;  language happiness_score ~ democratic_quality + generosity + log_gdp_per_capita
  .. ..- attr(*, &quot;variables&quot;)= language list(happiness_score, democratic_quality, generosity, log_gdp_per_capita)
  .. ..- attr(*, &quot;factors&quot;)= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ...
  .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2
  .. ..- attr(*, &quot;term.labels&quot;)= chr [1:3] &quot;democratic_quality&quot; &quot;generosity&quot; &quot;log_gdp_per_capita&quot;
  .. ..- attr(*, &quot;order&quot;)= int [1:3] 1 1 1
  .. ..- attr(*, &quot;intercept&quot;)= int 1
  .. ..- attr(*, &quot;response&quot;)= int 1
  .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; 
  .. ..- attr(*, &quot;predvars&quot;)= language list(happiness_score, democratic_quality, generosity, log_gdp_per_capita)
  .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:4] &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot;
  .. .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;happiness_score&quot; &quot;democratic_quality&quot; &quot;generosity&quot; &quot;log_gdp_per_capita&quot;
 $ residuals    : Named num [1:411] -0.405 -0.572 0.057 -0.426 -0.829 ...
  ..- attr(*, &quot;names&quot;)= chr [1:411] &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;19&quot; ...
 $ coefficients : num [1:4, 1:4] -1.01 0.17 1.161 0.693 0.314 ...
  ..- attr(*, &quot;dimnames&quot;)=List of 2
 $ aliased      : Named logi [1:4] FALSE FALSE FALSE FALSE
  ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;(Intercept)&quot; &quot;democratic_quality&quot; &quot;generosity&quot; &quot;log_gdp_per_capita&quot;
 $ sigma        : num 0.628
 $ df           : int [1:3] 4 407 4
 $ r.squared    : num 0.695
 $ adj.r.squared: num 0.693
 $ fstatistic   : Named num [1:3] 310 3 407
  ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;value&quot; &quot;numdf&quot; &quot;dendf&quot;
 $ cov.unscaled : num [1:4, 1:4] 0.2504 0.0229 -0.0139 -0.0264 0.0229 ...
  ..- attr(*, &quot;dimnames&quot;)=List of 2
 $ na.action    : &#39;omit&#39; Named int [1:1293] 1 2 3 4 5 6 7 11 12 13 ...
  ..- attr(*, &quot;names&quot;)= chr [1:1293] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
 - attr(*, &quot;class&quot;)= chr &quot;summary.lm&quot;</code></pre>
<p>If we want the adjusted <span class="math inline">\(R^2\)</span> or root mean squared error (RMSE, i.e. average error<a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a>), they aren’t readily available in the model object, but in the summary object, we can pluck them as we would any other <a href="data_structures.html#lists">list object</a>.</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="models.html#cb380-1"></a>happy_model_base_sum<span class="op">$</span>adj.r.squared</span></code></pre></div>
<pre><code>[1] 0.6930647</code></pre>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="models.html#cb382-1"></a>happy_model_base_sum[[<span class="st">&#39;sigma&#39;</span>]]</span></code></pre></div>
<pre><code>[1] 0.6282718</code></pre>
<div id="package-support" class="section level3">
<h3>Package support</h3>
<p>There are many packages available to get at model results. One of the most widely used is broom, which has tidy and other functions that can apply in different ways to different models.</p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="models.html#cb384-1"></a><span class="kw">library</span>(broom)</span>
<span id="cb384-2"><a href="models.html#cb384-2"></a><span class="kw">tidy</span>(happy_model_base)</span></code></pre></div>
<pre><code># A tibble: 4 x 5
  term               estimate std.error statistic  p.value
  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1 (Intercept)          -1.01     0.314      -3.21 1.41e- 3
2 democratic_quality    0.170    0.0459      3.71 2.33e- 4
3 generosity            1.16     0.195       5.94 6.18e- 9
4 log_gdp_per_capita    0.693    0.0333     20.8  5.93e-66</code></pre>
<p>Some packages will produce tables for a model object that are more or less ready for publication. However, unless you know it’s in the exact style you need, you’re probably better off dealing with it yourself. For example, you can use tidy and do minor cleanup to get the table ready for publication.</p>
</div>
</div>
<div id="visualization" class="section level2">
<h2>Visualization</h2>
<blockquote>
<p>Models require visualization to be understood.</p>
</blockquote>
<p>If you aren’t using visualization as a fundamental part of your model exploration, you’re likely leaving a lot of that exploration behind, and not communicating the results as well as you could to the broadest audience possible. When adding nonlinear effects, interactions, and more, visualization is a must. Thankfully there are many packages to help you get data you need to visualize effects.</p>
<p>We start with the <span class="pack" style="">emmeans</span> package. In the following example we have a country effect and which to get the mean happiness scores per country. We then visualize the results. Here we can see that Mexico is lowest on average.</p>
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb386-1"><a href="models.html#cb386-1"></a>happy_model_nafta =<span class="st"> </span><span class="kw">lm</span>(happiness_score <span class="op">~</span><span class="st"> </span>country <span class="op">+</span><span class="st"> </span>year, <span class="dt">data =</span> nafta)</span>
<span id="cb386-2"><a href="models.html#cb386-2"></a></span>
<span id="cb386-3"><a href="models.html#cb386-3"></a><span class="kw">library</span>(emmeans)</span>
<span id="cb386-4"><a href="models.html#cb386-4"></a>country_means =<span class="st"> </span><span class="kw">emmeans</span>(happy_model_nafta, <span class="op">~</span><span class="st"> </span>country)</span>
<span id="cb386-5"><a href="models.html#cb386-5"></a>country_means</span></code></pre></div>
<pre><code> country       emmean    SE df lower.CL upper.CL
 Canada          7.37 0.064  8     7.22     7.52
 Mexico          6.76 0.064  8     6.61     6.91
 United States   7.03 0.064  8     6.88     7.17

Confidence level used: 0.95 </code></pre>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb388-1"><a href="models.html#cb388-1"></a><span class="kw">plot</span>(country_means)</span></code></pre></div>
<p><img src="data-processing-and-visualization_files/figure-html/emmeans-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>We can also test for pairwise differences between the countries, and there’s no reason not to visualize that also. In the following, after adjustment Mexico and U.S. might not differ on mean happiness, but the other comparisons are statistically notable.</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb389-1"><a href="models.html#cb389-1"></a>pw_comparisons =<span class="st"> </span><span class="kw">contrast</span>(country_means, <span class="dt">method =</span> <span class="st">&#39;pairwise&#39;</span>, <span class="dt">adjust =</span> <span class="st">&#39;bonferroni&#39;</span>)</span>
<span id="cb389-2"><a href="models.html#cb389-2"></a>pw_comparisons</span></code></pre></div>
<pre><code> contrast               estimate     SE df t.ratio p.value
 Canada - Mexico           0.611 0.0905  8  6.751  0.0004 
 Canada - United States    0.343 0.0905  8  3.793  0.0159 
 Mexico - United States   -0.268 0.0905  8 -2.957  0.0547 

P value adjustment: bonferroni method for 3 tests </code></pre>
<div class="sourceCode" id="cb391"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb391-1"><a href="models.html#cb391-1"></a><span class="kw">plot</span>(pw_comparisons)</span></code></pre></div>
<p><img src="data-processing-and-visualization_files/figure-html/emmeans-pairwise-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>The following example uses <span class="pack" style="">ggeffects</span>. First, we run a model with an interaction of country and year (we’ll talk more about interactions later). Then we get predictions for the year by country, and subsequently visualize. We can see that the trend, while negative for all countries, is more pronounced as we move south.</p>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="models.html#cb392-1"></a>happy_model_nafta =<span class="st"> </span><span class="kw">lm</span>(happiness_score <span class="op">~</span><span class="st"> </span>country<span class="op">*</span>year, <span class="dt">data =</span> nafta)</span>
<span id="cb392-2"><a href="models.html#cb392-2"></a></span>
<span id="cb392-3"><a href="models.html#cb392-3"></a><span class="kw">library</span>(ggeffects)</span>
<span id="cb392-4"><a href="models.html#cb392-4"></a>preds =<span class="st"> </span><span class="kw">ggpredict</span>(happy_model_nafta, <span class="dt">terms =</span> <span class="kw">c</span>(<span class="st">&#39;year&#39;</span>, <span class="st">&#39;country&#39;</span>))</span>
<span id="cb392-5"><a href="models.html#cb392-5"></a></span>
<span id="cb392-6"><a href="models.html#cb392-6"></a><span class="kw">plot</span>(preds)</span></code></pre></div>
<p><img src="data-processing-and-visualization_files/figure-html/ggeffects-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>Whenever you move to generalized linear models or other more complicated settings, visualization is even more required, so it’s best to have some tools at your disposal.</p>
</div>
<div id="extensions-to-the-standard-linear-model" class="section level2">
<h2>Extensions to the Standard Linear Model</h2>
<div id="different-types-of-targets" class="section level3">
<h3>Different types of targets</h3>
<p>In many data situations, we do not have a continuous target variable, or may want to use a different distribution to get a better fit, or adhere to some theoretical perspective. For example, count data is not continuous and often notably skewed, so assuming a normal symmetric distribution may not work as well. From a data generating perspective we can use the Poisson distribution<a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a> for the target variable instead.</p>
<p><span class="math display">\[\ln{\mu} =  X\beta\]</span>
<span class="math display">\[\mu = e^{X\beta}\]</span>
<span class="math display">\[y \sim \mathcal{Pois}(\mu)\]</span>
Conceptually nothing has really changed from what we were doing with the standard linear model, except for the distribution. We still have a mean function determined by our predictors, and this is what we’re typically mainly interested in from a theoretical perspective. We do have an added step, a transformation of the mean (now usually called the <em>linear predictor</em>). Poisson naturally works with the log of the target, but rather than do that explicitly, we instead exponentiate the linear predictor. The <em>link function</em><a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a>, which links is the natural log, with the <em>inverse link/mean function</em> exponentiation.</p>
<p>In code we can demonstrate this as follows.</p>
<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb393-1"><a href="models.html#cb393-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb393-2"><a href="models.html#cb393-2"></a>N =<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb393-3"><a href="models.html#cb393-3"></a>beta =<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb393-4"><a href="models.html#cb393-4"></a>x =<span class="st"> </span><span class="kw">rnorm</span>(N)</span>
<span id="cb393-5"><a href="models.html#cb393-5"></a>mu =<span class="st"> </span><span class="kw">exp</span>(beta[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">2</span>]<span class="op">*</span>x)</span>
<span id="cb393-6"><a href="models.html#cb393-6"></a>y =<span class="st"> </span><span class="kw">rpois</span>(N, mu)</span>
<span id="cb393-7"><a href="models.html#cb393-7"></a></span>
<span id="cb393-8"><a href="models.html#cb393-8"></a><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">family =</span> poisson)</span></code></pre></div>
<pre><code>
Call:  glm(formula = y ~ x, family = poisson)

Coefficients:
(Intercept)            x  
      2.009        0.994  

Degrees of Freedom: 999 Total (i.e. Null);  998 Residual
Null Deviance:      13240 
Residual Deviance: 1056     AIC: 4831</code></pre>
<p><span class="math display">\[\ln{\frac{\mu}{1-\mu}} =  X\beta\]</span>
<span class="math display">\[\mu = \frac{1}{1+e^{-X\beta}}\]</span>
<span class="math display">\[y \sim \mathcal{Binom}(\mathrm{prob}=\mu, \mathrm{size} = 1)\]</span>
Here our link function is called the <em>logit</em>, and it’s inverse takes our linear predictor and puts it on the probability scale.</p>
<p>Again, some code can help drive this home.</p>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb395-1"><a href="models.html#cb395-1"></a>mu =<span class="st"> </span><span class="kw">plogis</span>(beta[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">2</span>]<span class="op">*</span>x)</span>
<span id="cb395-2"><a href="models.html#cb395-2"></a>y =<span class="st"> </span><span class="kw">rbinom</span>(N, <span class="dt">size =</span> <span class="dv">1</span>, mu)</span>
<span id="cb395-3"><a href="models.html#cb395-3"></a></span>
<span id="cb395-4"><a href="models.html#cb395-4"></a><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">family =</span> binomial)</span></code></pre></div>
<pre><code>
Call:  glm(formula = y ~ x, family = binomial)

Coefficients:
(Intercept)            x  
      2.141        1.227  

Degrees of Freedom: 999 Total (i.e. Null);  998 Residual
Null Deviance:      852.3 
Residual Deviance: 708.8    AIC: 712.8</code></pre>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb397-1"><a href="models.html#cb397-1"></a><span class="co"># extension to count/proportional model</span></span>
<span id="cb397-2"><a href="models.html#cb397-2"></a><span class="co"># mu = plogis(beta[1] + beta[2]*x)</span></span>
<span id="cb397-3"><a href="models.html#cb397-3"></a><span class="co"># total = rpois(N, lambda = 5)</span></span>
<span id="cb397-4"><a href="models.html#cb397-4"></a><span class="co"># events = rbinom(N, size = total, mu)</span></span>
<span id="cb397-5"><a href="models.html#cb397-5"></a><span class="co"># nonevents = total - events</span></span>
<span id="cb397-6"><a href="models.html#cb397-6"></a><span class="co"># </span></span>
<span id="cb397-7"><a href="models.html#cb397-7"></a><span class="co"># glm(cbind(events, nonevents) ~ x, family = binomial)</span></span></code></pre></div>
<p>You’ll have noticed that when we fit these models we used <span class="func" style="">glm</span> instead of <span class="func" style="">lm</span>. The normal linear model is a special case of <em>generalized linear models</em>, which includes a specific class of distributions- normal, poisson, binomial, gamma, beta and more- collectively referred to as the <a href="https://en.wikipedia.org/wiki/Exponential_family">exponential family</a>. While this family can cover a lot of ground, you do not have to restrict yourself to it, and many R modeling packages will provide easy access to more. The main point is that you have tools to deal with continuous, binary, count, ordinal, and other types of data.</p>
</div>
<div id="correlated-data" class="section level3">
<h3>Correlated data</h3>
<p>Often in standard regression modeling situations we have data that is correlated, like when we observe multiple observations for individuals (i.e. longitudinal studies), or observations are clustered within geographic units. There are many ways to analyze all kinds of correlated data in the form of clustered data, time series, spatial data and similar. In terms of understanding the mean function and data generating distribution for our target variable, as we did in our previous models, not much changes. However, we will want to utilize estimation techniques that take this correlation into account. Examples of such models include:</p>
<ul>
<li>Mixed models (e.g. random intercepts, ‘multilevel’ models)</li>
<li>Time series models (autoregressive)</li>
<li>Spatial models (e.g. conditional autoregressive)</li>
</ul>
<p>As demonstration is beyond the scope of this document, the main point here is awareness. But see these on <a href="https://m-clark.github.io/mixed-models-with-R/">mixed models</a> and <a href="https://m-clark.github.io/generalized-additive-models/">generalized additive models</a>.</p>
</div>
<div id="other-extensions" class="section level3">
<h3>Other extensions</h3>
<p>There are many types of models that will take one well beyond the standard linear model. In some cases, the focus is multivariate, trying to model many targets at once. Other models will even be domain-specific, tailored to a very narrow type of problem. Whatever the scenario, having a good understanding of the models we’ve been discussing will likely help you navigate these new waters much more easily.</p>
</div>
</div>
<div id="model-exploration-summary" class="section level2">
<h2>Model Exploration Summary</h2>
<p>At this point you should have a good idea of how to get started exploring models with R. Generally what you will explore will be based on theory, or merely curiosity. Specific packages while make certain types of models easy to pull off, without much change to the syntax. Almost invariably, you will need to process the data to make it more amenable to analysis and/or more interpretable. After model fitting, summaries and visualizations go a long way toward understanding the part of the world you are exploring.</p>
</div>
<div id="exercises" class="section level2">
<h2>Exercises</h2>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="18">
<li id="fn18"><p>To be consistent with the many tools that minimize different types of losses, we usually minimize the <em>negative log likelihood</em>, rather than maximize the raw likelihood.<a href="models.html#fnref18" class="footnote-back">↩︎</a></p></li>
<li id="fn19"><p>In this case, a Poisson distribution.<a href="models.html#fnref19" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p>For example, if the next best guess for a regression coefficient results in only a difference to many decimal places from the previous one, you probably don’t care and can stop your search. Reaching your desired stopping point is known as <em>convergence</em>. In other cases, you may simply set a maximum number of iterations.<a href="models.html#fnref20" class="footnote-back">↩︎</a></p></li>
<li id="fn21"><p>The World Happiness Report is a survey of the state of global happiness that ranks countries by how happy their citizens perceive themselves to be. Almost all the information here is gleaned from the report and appendices. This regards the report data from 2008-2018 included in the 2019 report.<a href="models.html#fnref21" class="footnote-back">↩︎</a></p></li>
<li id="fn22"><p>In econometrics, if both target and predictor variable are logged, the coefficient is referred to as an <a href="https://stats.idre.ucla.edu/sas/faq/how-can-i-interpret-log-transformed-variables-in-terms-of-percent-change-in-linear-regression/">elasticity</a>.<a href="models.html#fnref22" class="footnote-back">↩︎</a></p></li>
<li id="fn23"><p>Note that none of the benefits regard normality. Transforming variables is not done to meet the normality assumption regarding residuals, and would rarely help in that regard.<a href="models.html#fnref23" class="footnote-back">↩︎</a></p></li>
<li id="fn24"><p>The log transformations in the table are approximations that allow an eyeballable interpretation. Typically we use exponentiated coefficients for a more exact interpretation. For example, if y is logged, a one-unit change in x leads to a <span class="math inline">\(100*(e^B-1)\)</span>% change in y. See <a href="https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-a-regression-model-when-some-variables-are-log-transformed/">this link</a> for more on interpreting logged variables.<a href="models.html#fnref24" class="footnote-back">↩︎</a></p></li>
<li id="fn25"><p>Some distinguish one-hot from dummy coding in that the former creates a binary variable for all <span class="math inline">\(C\)</span> categorical levels while dummy coding only creates <span class="math inline">\(C-1\)</span> variables, leaving the reference group out. However, the reference group is only left out at model fitting, there’s no reason not to create all C variables, as it requires no additional effort and you might want to change the reference group.<a href="models.html#fnref25" class="footnote-back">↩︎</a></p></li>
<li id="fn26"><p>For standard regression models, the divisor for calculating the RMSE is the residual degrees of freedom, but more generally it is just N, the sample size, as many modern models may have more parameters than observations.<a href="models.html#fnref26" class="footnote-back">↩︎</a></p></li>
<li id="fn27"><p>For historical reasons, Poisson is usually denoted with mean <span class="math inline">\(\lambda\)</span>, and with the binomial the probability is often denoted with <span class="math inline">\(\pi\)</span>, but consistency is desired here.<a href="models.html#fnref27" class="footnote-back">↩︎</a></p></li>
<li id="fn28"><p>Before we distinguished models from <a href="models.html#estimation">estimation procedures</a>, and it’s also important to distinguish models from link functions. In some disciplines (mostly one and those trained in its methods), you’ll see some refer to a ‘logit model’, for example. However that wouldn’t tell you if the model regards a target that is binary, categorical, ordered categorical, count or has values between zero and one, which cover several types of possible distributions.<a href="models.html#fnref28" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="more.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model_criticism.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["twitter", "facebook", "weibo", "instapaper"],
"google": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"depth": 2,
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
