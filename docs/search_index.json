[
["index.html", "Data Processing &amp; Visualization", " Data Processing &amp; Visualization Michael Clark https://m-clark.github.io/data-processing-and-visualization/ 2017-11-25 "],
["01_intro.html", "Intro Outline Preparation Other", " Intro This document is the basis for multiple workshops, whose commmon goal is to provide some tools, tips, packages etc. that make data processing and visualization in R easier. It is oriented toward those who have had some exposure to R in an applied fashion, but would also be useful to someone coming to R from another programming language. It is not an introduction to R, nor does this document have anything to do with statistical analysis (yet). The goal here is primarily to instill awareness, specifically of tools that will make your data exploration easier, and to understand some of the why behind the tools, so that one can better implement them. Outline Part 1: Data Processing Understanding Base R Approaches to Data Processing Overview of Data Structures Input/Output Vectorization and Apply functions Getting Acquainted with Other Approaches to Data Processing Pipes, and how to use them tidyverse data.table Misc. Part 2: Visualization Thinking Visually in progress Information Color Contrast and more… ggplot2 Aesthetics Layers Themes and more… Adding Interactivity Package demos Shiny Future… ? Part 3: Programming Basics Part 4: Presentation Part 5: Modeling Preparation To follow along with some of the examples, create a new RStudio project (File/New Project/New Directory/New Project/…). After that, download the data zip file and unzip the contents to your project folder. You will then have easy access to much of the data used in the examples (that isn’t already accessible via packages, base R, etc.). Some are .RData files, while others might be raw .csv. Zipped Data File Other Color coding in text: emphasis package function object/class link Some key packages used in the following demonstrations and exercises: tidyverse (several packages), data.table, ggplot2movies Many others are also used, feel free to install as we come across them. Here are a few. DT, highcharter, magrittr, maps, mgcv, plotly, quantmod, readr, visNetwork "],
["02_dataStructures.html", "Part I: Data Processing Data Structures", " Part I: Data Processing Data Structures R has several core data structures, and we’ll take a look at each. Vectors Factors Lists Matrices/arrays Data frames The more you know about R data structures, the more you’ll know how to use them, the more you’ll know why things go wrong if they do, and the further you’ll be able to go with your data. Vectors Vectors form the basis of R data structures. Two main types are atomic and lists, but we’ll talk about lists separately. Here is an R vector. The elements of the vector are numeric values. x = c(1, 3, 2, 5, 4) x [1] 1 3 2 5 4 All elements of an atomic vector are the same type. Example types include: character numeric (double) integer logical In addition, there are special kinds of values like NA (‘not available’ i.e. missing), NULL, NaN (not a number), Inf (infinite) and so forth. You can use typeof to examine an object’s type, or use an is function, e.g. is.logical, to check if an object is a specific type. Character strings When dealing with text, objects of class character are what you’d typically be dealing with. x = c(&#39;... Of Your Fake Dimension&#39;, &#39;Ephemeron&#39;, &#39;Dryswch&#39;, &#39;Isotasy&#39;, &#39;Memory&#39;) x Not much to it, but be aware there is no real limit to what is represented as a character vector. For example, in a data frame, you could have a column where each entry is one of the works of Shakespeare. Factors An important type of vector is a factor. Factors are used to represent categorical data structures. Although not exactly precise, one can think of factors as integers with labels. So the underlying representation of a variable for sex is 1:2 with labels ‘Male’ and ‘Female’. They are a special class with attributes, or metadata, that contains the information about the levels. x = factor(rep(letters[1:3], e=10)) x [1] a a a a a a a a a a b b b b b b b b b b c c c c c c c c c c Levels: a b c attributes(x) $levels [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; $class [1] &quot;factor&quot; The underlying representation is numeric, but it is important to remember that factors are categorical. Thus, they can’t be used as numbers would be, as the following demonstrates. x_num = as.numeric(x) # convert to a numeric object sum(x_num) [1] 60 sum(x) Error in Summary.factor(structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, : &#39;sum&#39; not meaningful for factors Strings vs. Factors The main thing to note is that factors are generally a statistical phenomenon, and are required to do statistical things with data that would otherwise be a simple character string. If you know the relatively few levels the data can take, you’ll generally want to use factors, or at least know that statistical packages and methods may require them. In addition, factors allow you to easily overcome the silly default alphabetical ordering of category levels in some very popular visualization packages. For other things, such as text analysis, you’ll almost certainly want character strings instead, and in many cases it will be required. It’s also worth noting that a lot of base R and other behavior will coerce strings to factors. This made a lot more sense in the early days of R, but is not really necessary these days. Some packages to note to help you with processing strings and factors: forcats stringr Matrices With multiple dimensions, we are dealing with arrays. Matrices are 2-d arrays, and extremely commonly used for scientific computing1. The vectors making up a matrix must all be of the same type. For example, all values in a matrix might be numeric, or all character strings. Creating a matrix Creating a matrix can be done in a variety of ways. # create vectors x = 1:4 y = 5:8 z = 9:12 rbind(x, y, z) # row bind [,1] [,2] [,3] [,4] x 1 2 3 4 y 5 6 7 8 z 9 10 11 12 cbind(x, y, z) # column bind x y z [1,] 1 5 9 [2,] 2 6 10 [3,] 3 7 11 [4,] 4 8 12 matrix(c(x, y, z), nrow=3, ncol=4, byrow=TRUE) [,1] [,2] [,3] [,4] [1,] 1 2 3 4 [2,] 5 6 7 8 [3,] 9 10 11 12 Lists Lists in R are highly flexible objects, and probably the most commonly used for data science. Unlike vectors, whose elements must be of the same type, lists can contain anything as their elements, even other lists. Here is a list. We use the list function to create it. x = list(1, &quot;apple&quot;, list(3, &quot;cat&quot;)) x [[1]] [1] 1 [[2]] [1] &quot;apple&quot; [[3]] [[3]][[1]] [1] 3 [[3]][[2]] [1] &quot;cat&quot; We often want to loop some function over a list. for (elem in x) print(class(elem)) [1] &quot;numeric&quot; [1] &quot;character&quot; [1] &quot;list&quot; Lists can, and often do, have named elements. x = list(&quot;a&quot; = 25, &quot;b&quot; = -1, &quot;c&quot; = 0) x[&quot;b&quot;] $b [1] -1 Almost all standard models in base R and other packages return an object that is a list. Knowing how to work with a list will allow you to easily access the contents of the model object for further processing. Python has similar structures, lists and dictionaries, where the latter works similarly to R’s named list. Data Frames Data frames are a very commonly used data structure. Elements of a data frame can be different types, and this is because the data.frame class is actually just a list. As such, everything about lists applies to them. But they can also be indexed by row or column as well, just like matrices. There are other very common types of object classes associated with packages that are both a data.frame and some other type of structure (e.g. tibbles in the tidyverse). Usually your data frame will come directly from import or manipulation of other R objects (e.g. matrices). However, you should know how to create one from scratch. Creating a data frame The following will create a data frame with two columns, a and b. mydf = data.frame(a = c(1,5,2), b = c(3,8,1)) Much to the disdain of the tidyverse, we can add row names also. rownames(mydf) = paste0(&#39;row&#39;, 1:3) mydf a b row1 1 3 row2 5 8 row3 2 1 Everything about lists applies to data.frames, so we can add, select, and remove elements of a data frame just like lists. However, we’ll visit this later, because we’ll have much more flexibility with data frames than we would lists. Data Structure Exercises Excercise #1 Create an object that is a matrix and/or a data.frame, and inspect its class or structure (use the class or str functions) on the object you just created. Exercise #2 Create a list of 3 elements, the first of which contains character strings, the second numbers, and the third, the data.frame or matrix you just created. Thinking Exercises How is a factor different from a character vector? How is a data.frame the same as and different from a matrix? How is a data.frame the same as and different from a list? Despite what the tidyverse would have you believe.↩ "],
["03_io.html", "Input/Output", " Input/Output Until you get comfortable getting data into R, you’re not going to use it as much as you would. You should at least be able to read in common data formats like comma/tab-separated, Excel, etc. Standard methods of reading in tabular data include the following functions: read.table read.csv readLines Base R also comes with the foreign package for reading in other types of files, especially other statistical packages. However, while you may see it still in use, it’s not as useful as what’s found in other packages. Reading in data is usually a one-off event, such that you’ll never need to use the package again after the data is loaded. In that case, you might use the following approach, so that you don’t need to attach the whole package. readr::read_csv(&#39;fileloc/filename.csv&#39;) You can use that for any package, which can help avoid naming conflicts by not loading a bunch of different packages. Furthermore, if you need packages that do have a naming conflict, using this approach will ensure the function from the package you want will be used. Other approaches There are some better and faster ways to read in data than the base R approach. A package for reading in foreign statistical files is haven, which has functions like read_spss and read_dta for SPSS and Stata files respectively. The package readxl is a clean way to read Excel files that doesn’t require any outside packages or languages. The package rio: uses haven, readxl etc., but with just two functions for everything: import, export (also convert). Faster approaches For faster versions of base R functions, readr has read_csv, read_delim, and others. These make assumptions about what type each vector is after an initial scan of the data, then proceed accordingly. If you don’t have ‘big’ data, the subsequent speed gain won’t help much, however, such an approahc actually can be used as a diagnostic to pick up potential data entry errors, as warnings are given when unexpected observations occur. The data.table package provides a faster version read.table, and is typically faster than readr approaches (fread). Other Data Be aware that R can handle practically any type of data you want to throw at it. Some examples include: JSON SQL XML YAML MongoDB NETCDF text (e.g. a novel) shapefiles (e.g. for geographic data) Google spreadsheets And many, many others. On the horizon feather is designed to make reading/writing data frames efficient, and the really nice thing about it is that it works in both Python and R. It’s still in early stages of development on the R side though. Big Data You may come across the situation where your data cannot be held in memory. One of the first things to be aware of for data processing is that you may not need to have the data all in memory at once. Before shifting to a hardware solution, consider if the following is possible. Chunking: reading and processing the data in chunks Line at a time: dealing with individual lines of data Other data formats: for example SQL databases (sqldf package, src_dbi in dplyr) However, it may be that the end result is still too large. In that case you’ll have to consider a cluster-based or distributed data situation. Of course R will have tools for that as well. DBI sparklyr RHadoop And more. I/O Exercises Exercise 1 Use readr and haven to read the following files. Use the url just like you would any filename. The latter is a Stata file. You can use the RStudio’s menu approach to import the file if you want. ‘https://raw.githubusercontent.com/m-clark/data-processing-and-visualization/master/data/cars.csv’ ‘https://raw.githubusercontent.com/m-clark/data-processing-and-visualization/master/data/presvote.dta’ Thinking Exercises Why might you use read_csv from the readr package rather than read.csv in base R? What is your definition of ‘big’ data? "],
["04_indexing.html", "Indexing", " Indexing What follows is a refresher. Presumably you’ve had enough R exposure to be aware of some of this. However, much of data processing regards data frames, or other tables of mixed data types, so more time will be spent on slicing and dicing of data frames instead. Even so, it would be impossible to use R effectively without knowing how to handle basic data types. Base R Indexing Refresher Slicing vectors letters[4:6] [1] &quot;d&quot; &quot;e&quot; &quot;f&quot; letters[c(13,10,3)] [1] &quot;m&quot; &quot;j&quot; &quot;c&quot; Slicing matrices/data.frames myMatrix[1, 2:3] Label-based indexing mydf[&#39;row1&#39;, &#39;b&#39;] Position-based indexing mydf[1, 2] Mixed indexing mydf[&#39;row1&#39;, 2] If the row/column value is empty, all rows/columns are retained. mydf[&#39;row1&#39;,] mydf[,&#39;b&#39;] Non-contiguous mydf[c(1,3),] Boolean mydf[mydf$a &gt;= 2,] List/Data.frame extraction [ : grab a slice of elements/columns [[ : grab specific elements/columns $ : grab specific elements/columns @: extract slot for S4 objects my_list_or_df[2:4] my_list_or_df[[&#39;name&#39;]] my_list_or_df$name Indexing Exercises This following is a refresher of base R indexing only. Here is a matrix, a data.frame and a list. mymatrix = matrix(rnorm(100), 10, 10) mydf = cars mylist = list(mymatrix, thisdf = mydf) Exercise 1 For the matrix, in separate operations, take a slice of rows, a selection of columns, and a single element. Exercise 2 For the data.frame, grab a column in 3 different ways. Exercise 3 For the list grab an element by number and by name. "],
["05_vectorapp.html", "Vectorization &amp; Apply", " Vectorization &amp; Apply Boolean Indexing Assume x is a vector of numbers. How would we create an index representing any value greater than 2? x = c(-1, 2, 10, -5) idx = x &gt; 2 idx [1] FALSE FALSE TRUE FALSE x[idx] [1] 10 Logicals are objects with values of TRUE or FALSE, like the idx variable above. While sometimes we want to deal with the logical object as an end, it is extremely commonly used as an index in data processing. Note that how we don’t have to create an explicit index object first (though often you should), as R indexing is ridiculously flexible. Here are more examples, not necessarily recommended, but just to demonstrate the flexibility of Boolean indexing. x[x &gt; 2] x[x != 3] x[ifelse(x &gt; 2 &amp; x !=10, TRUE, FALSE)] x[{y = idx; y}] x[resid(lm(y ~ x)) &gt; 0] Vectorized operations Boolean indexing allows us to take vectorized approaches to dealing with data. Consider the following unfortunately coded loop: for (i in 1:nrow(mydf)) { check = mydf$x[i] &gt; 2 if (check==TRUE) { mydf$y[i] = &#39;Yes&#39; } else { mydf$y[i] = &#39;No&#39; } } Compare2: mydf$y = &#39;No&#39; mydf$y[mydf$x &gt; 2] = &#39;Yes&#39; This gets us the same thing, and would be much faster than the looped approach. Boolean indexing is an example of a vectorized operation. The whole vector is considered, rather than each element individually. The result is that any preprocessing is done once rather than the n iterations of the loop. In R, this will always faster. Example: Log all values in a matrix. mymatrix_log = log(mymatrix) This is way faster than looping over elements, rows or columns. Here we’ll let the apply function stand in for our loop, logging the elements of each column. mymatrix = matrix(runif(100), 10, 10) identical(apply(mymatrix, 2, log), log(mymatrix)) [1] TRUE library(microbenchmark) microbenchmark(apply(mymatrix, 2, log), log(mymatrix)) Unit: microseconds expr min lq mean median uq max neval apply(mymatrix, 2, log) 47.730 48.931 57.83179 49.531 50.432 200.528 100 log(mymatrix) 3.002 3.302 3.71348 3.302 3.603 16.811 100 Many vectorized functions already exist in R. They are often written in C, Fortran etc., and so even faster. Not all programming languages lean toward vectorized operations, and may not see much speed gain from it. In R however, you’ll want to prefer it. Even without, it’s cleaner/clearer code, another reason to use the approach. Apply functions A family of functions comes with R that allows for a succinct way of looping. Common functions in this family include: apply arrays, matrices, data.frames lapply, sapply, vapply lists, data.frames, vectors tapply grouped operations (table apply) mapply multivariate version of sapply replicate similar to sapply As an example we’ll consider standardizing variables, i.e. taking a set of numbers, subtracting the mean, and dividing by the standard deviation. This results in a variable with mean of 0 and standard deviation of 1. Let’s start with a loop approach. for (i in 1:ncol(mydf)){ x = mydf[,i] for (j in 1:length(x)){ x[j] = (x[j] - mean(x))/sd(x) } } The above would be a really bad way to use R. It goes over each column individually, then over each value of the column. Conversely, apply will take a matrix or data frame, and apply a function over the margin, row or column, you want to loop over. The first argument is the data you’re considering, the margin is the second argument (1 for rows, 2 for columns), and the function you want to apply to those rows is the third argument. The following example is much cleaner compared to the loop, and now you’d have a function you can use elsewhere if needed. stdize &lt;- function(x) { (x-mean(x)) / sd(x) } apply(mydf, 2, stdize) # 1 for rows, 2 for columnwise application Many of the other apply functions work similarly, taking an object and a function to do the work on the object (possibly implicit), possibly with other arguments if necessary. Timings The previous demonstrates how to use apply. However, there is a scale function in base R that uses a more vectorized approach under the hood. The following demonstrates various approaches to standardizing the columns of the matrix, even using a parallelized approach. As you’ll see, the base R function requires very little code and beats the others. mymat = matrix(rnorm(100000), ncol=1000) stdize &lt;- function(x) { (x-mean(x)) / sd(x) } doubleloop = function() { for (i in 1:ncol(mymat)) { x = mymat[, i] for (j in 1:length(x)) { x[j] = (x[j] - mean(x)) / sd(x) } } } singleloop = function() { for (i in 1:ncol(mymat)) { x = mymat[, i] x = (x - mean(x)) / sd(x) } } library(parallel) cl = makeCluster(8) clusterExport(cl, c(&#39;stdize&#39;, &#39;mymat&#39;)) doParallel::registerDoParallel(cl) test = microbenchmark::microbenchmark(doubleloop=doubleloop(), singleloop=singleloop(), apply=apply(mymat, 2, stdize), parApply=parApply(cl, mymat, 2, stdize), vectorized=scale(mymat), times=25) stopCluster(cl) test Unit: milliseconds expr min lq mean median uq max neval doubleloop 2882.148089 2904.739076 2938.21281 2946.57065 2952.25532 3013.29805 25 singleloop 28.989806 29.512434 33.08177 31.17037 32.42606 84.79185 25 apply 32.167901 33.350945 33.96146 34.13204 34.64055 36.18983 25 parApply 17.691611 18.213638 21.91323 19.63893 21.12156 73.60683 25 vectorized 8.085578 8.491433 10.25583 10.66990 11.02532 12.62173 25 Apply functions It is important to be familiar with and regularly use the apply family for efficient data processing. A summary of benefits includes: Cleaner/simpler code Environment kept clear of unnecessary objects Potentially more reproducible more likely to use generalizable functions Might be faster Parallelizable Note that apply functions are NOT necessarily faster than explicit loops. single loop over columns was as fast as apply replicate and mapply are especially slow However, the apply family can ALWAYS can potentially be faster than standard R loops do to para parallelization. With base R’s parallel package, there are parallel versions of the apply family, e.g.parApply, parLapply etc. Apply functions and similar approaches should be a part of your regular R experience. We’ll talk about other options that may have even more benefits, but you need to know the basics in order to use those. I use R every day, and very rarely use explicit loops. Note that there is no speed difference for a for loop vs. using while. And if you must use an explicit loop, create an empty object of the dimension/form you need, and then fill it in via the loop. This will be notably faster. I pretty much never use an explicit double loop, as a little more thinking about the problem will usually provide a more efficient path to solving the problem. Vector/Apply Exercises Exercise 1 With the following matrix, use apply and the sum function to get row or column sums. x = matrix(1:9, 3, 3) Exercise 2 With the following list, use lapply and sapply and the sum function to get sums for the elements. There is no margin to specify with on a list, so just supply the list and the sum function. x = list(1:3, 4:6, 7:9) sapply is actually just a wrapper for lapply. If you supply the argument simplified=F, it is identical. Otherwise, it attempts to return a vector or matrix. For those familiar with ifelse, that would be applicable, but is not the point of the example.↩ "],
["06_pipes.html", "Pipes", " Pipes More detail on much of the following is provided here. Pipes are operators that send what comes before the pipe to what comes after. There are many different pipes, and some packages that use their own. However, the vast majority of packages use the same pipe: %&gt;% Here, we’ll focus on their use with the dplyr package, and the tidyverse more generally. Pipes are also utilized heavily in visualization. Example: mydf %&gt;% select(var1, var2) %&gt;% filter(var1 == &#39;Yes&#39;) %&gt;% summary Start with a data.frame %&gt;% select columns from it %&gt;% filter/subset it %&gt;% get a summary Using variables as they are created One nice thing about pipelines is that we can use variables as soon as they are created, with out having to break out separate objects/steps. mydf %&gt;% mutate(newvar1 = var1 + var2, newvar2 = newvar1/var3) %&gt;% summarise(newvar2avg = mean(newvar2)) Pipes for Visualization (more later) The following provides a means to think about pipes for visualization. It’s just a generic example for now, but we’ll see more later. basegraph %&gt;% points %&gt;% lines %&gt;% layout The dot Most functions are not ‘pipe-aware’ by default. In the following we try to send our data frame to lm for a regression. mydf %&gt;% lm(y ~ x) # error Other pipes could work in this situation, e.g. %$% in magrittr. But generally, when you come upon this situation, you can use a dot to represent the object before the pipe. mydf %&gt;% lm(y ~ x, data=.) # . == mydf Flexibility Piping is not just for data.frames, lists and vectors can be used as well, and would be the primary object for the purrr family of functions we’ll discuss later. The following starts with a character vector. Sends it to a recursive function (named ..). .. is created on-the-fly, and has a single argument (.). After the function is created, it’s used on ., which represents the string before the pipe. Result: pipes between the words3. c(&#39;Ceci&#39;, &quot;n&#39;est&quot;, &#39;pas&#39;, &#39;une&#39;, &#39;pipe!&#39;) %&gt;% { .. &lt;- . %&gt;% if (length(.) == 1) . else paste(.[1], &#39;%&gt;%&#39;, ..(.[-1])) ..(.) } [1] &quot;Ceci %&gt;% n&#39;est %&gt;% pas %&gt;% une %&gt;% pipe!&quot; Put that in your pipe and smoke it René Magritte! Summary Pipes are best used interactively, though you can use them within functions as well, and they are extremely useful for data exploration. Nowadays, more and more packages are being made that are ‘pipe-aware’, especially many visualization packages. See the magrittr package for more pipes. That was a very complicated way to do this paste(c('Ceci', &quot;n'est&quot;, 'pas', 'une', 'pipe!'), collapse=' %&gt;% ').↩ "],
["07_tidyverse.html", "Tidyverse", " Tidyverse What is the tidyverse? The tidyverse consists of a few key packages: ggplot2: data visualization dplyr: data manipulation tidyr: data tidying readr: data import purrr: functional programming, e.g. alternate approaches to apply tibble: tibbles, a modern re-imagining of data frames And of course the tidyverse package itself, which will load all of the above in a way that will avoid naming conflicts library(tidyverse) Loading tidyverse: ggplot2 Loading tidyverse: tibble Loading tidyverse: tidyr Loading tidyverse: readr Loading tidyverse: purrr Loading tidyverse: dplyr Conflicts with tidy packages ------------------------- filter(): dplyr, stats lag(): dplyr, stats In addition, there are other packages like lubridate, rvest, stringr and others in the hadleyverse that are also greatly useful. What is tidy? Tidy data refers to data arranged in a way that makes data processing, analysis, and visualization simpler. In a tidy data set: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Think long before wide. dplyr dplyr provides a grammar of data manipulation (like ggplot2 does for visualization). It is the next iteration of plyr, but there is no longer any need for plyr really. It’s focused on tools for working with data frames, with over 100 functions that might be of specific use to you. It has three main goals: Make the most important data manipulation tasks easier. Do them faster. Use the same interface to work with data frames, data tables or a database. Some key operations include: select: grab columns select helpers: one_of, starts_with, num_range etc. filter/slice: grab rows group_by: grouped operations mutate/transmute: create new variables summarize: summarize/aggregate do: arbitrary operations There are various (SQL-like) join/merge functions: inner_join, left_join etc. And there are a lot of little things like: n, n_distinct, nth, n_groups, count, recode, between In addition, there is no need to quote variable names. An example Let’s say we want to select from our data the following variables: Start with the ID variable The variables X1:X10, which are not all together, and there are many more X columns The variables var1 and var2, which are the only variables with var in their name Any variable that starts with XYZ How might we go about this? There several base R approaches that we could go with, but often they will be tedious, or require multiple objects to be created just to get the columns you want. Let’s start with the worst choice. newData = oldData[,c(1,2,3,4, etc.)] Using numeric indexes, or rather magic numbers, is not conducive to readability or reproducibility. If anything changes about the data columns, the numbers may no longer be applicable, and you’d have to redo the line again. We could name the variables explicitly. newData = oldData[,c(&#39;ID&#39;,&#39;X1&#39;, &#39;X2&#39;, etc.)] This would be fine if there are only a handful. But if you’re trying to reduce a 1000 column data set to several dozen it’s tedious, and generally not pretty regardless. A more advanced alternative regards a two-step approach with regular expressions. This requires that you know something about regex (and you should), but it is difficult to read/understand by those who don’t, and often by even yourself if it’s more complicated. In any case, you first will need to create an object that represents the column names first, otherwise it looks unwieldy if used within brackets or a function like subset. cols = c(&#39;ID&#39;, paste0(&#39;X&#39;, 1:10), &#39;var1&#39;, &#39;var2&#39;, grep(colnames(oldData), &#39;^XYZ&#39;, value=T)) newData = oldData[,cols] # or via subset newData = subset(oldData, select = cols) Now consider there is even more to do. What if you also want observations where Z is Yes, Q is No, and only the observations with the top 50 values of var2, ordered by var1 (descending)? Probably the more straightforward way in R to do so would be something like the following, where each part is broken out and we continuously write over the object as we modify it. # three operations and overwriting or creating new objects if we want clarity newData = newData[oldData$Z == &#39;Yes&#39; &amp; oldData$Q == &#39;No&#39;,] newData = newData[order(newData$var2, decreasing=T)[1:50],] newData = newData[order(newData$var1, decreasing=T),] And this is for fairly straightforward operations. Now consider doing all of the previous in one piped operation. The dplyr package will allow us to do something like the following. newData = oldData %&gt;% filter(Z == &#39;Yes&#39;, Q == &#39;No&#39;) %&gt;% select(num_range(&#39;X&#39;, 1:10), contains(&#39;var&#39;), starts_with(&#39;XYZ&#39;)) %&gt;% top_n(n=50, var2) %&gt;% arrange(desc(var1)) Even if it hadn’t been explained before, you might have been able to guess a little as to what was going on. The code is fairly succinct, we don’t have to keep referencing objects repeatedly, and no intermediary objects are created. dplyr and piping is an alternative. You can do all this sort of stuff with base R, for example, with functions like with, within, subset, transform, etc. Though the initial base R approach depicted is fairly concise, in general, it still can potentially be: more verbose less legible less amenable to additional data changes requires esoteric knowledge (e.g. regular expressions) often requires creation of new objects (even if we just want to explore) often slower, possibly greatly Running example The following data was scraped initially scraped from the web as follows. It is data from the NBA basketball league for the 2016-2017 season with things like player names, points per game, field goal percentage etc. We’ll use it as an example to demonstrate various functionality found within dplyr. library(rvest) url = &quot;http://www.basketball-reference.com/leagues/NBA_2017_totals.html&quot; bball = read_html(url) %&gt;% html_nodes(&quot;#totals_stats&quot;) %&gt;% html_table %&gt;% data.frame save(bball, file=&#39;data/bball.RData&#39;) However you can just load it into your workspace. Note that when initially downloaded, the data is all character strings. We’ll fix this later. load(&#39;data/bball.RData&#39;) glimpse(bball[,1:5]) Observations: 619 Variables: 5 $ Rk &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;13&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;, &quot;16&quot;, &quot;17&quot;, &quot;18&quot;, &quot;19&quot;, &quot;20&quot;, &quot;Rk&quot;, &quot;21&quot;, &quot;22&quot;, &quot;23&quot;, &quot;24&quot;, &quot;25&quot;, &quot;26&quot;, &quot;27&quot;, &quot;28&quot;, &quot;29&quot;, &quot;29&quot;, &quot;... $ Player &lt;chr&gt; &quot;Alex Abrines&quot;, &quot;Quincy Acy&quot;, &quot;Quincy Acy&quot;, &quot;Quincy Acy&quot;, &quot;Steven Adams&quot;, &quot;Arron Afflalo&quot;, &quot;Alexis Ajinca&quot;, &quot;Cole Aldrich&quot;, &quot;LaMarcus Aldridge&quot;, &quot;Lavoy Allen&quot;, &quot;Tony Allen&quot;, &quot;Al-Farouq Aminu&quot;, &quot;Chris ... $ Pos &lt;chr&gt; &quot;SG&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;C&quot;, &quot;SG&quot;, &quot;C&quot;, &quot;C&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;SG&quot;, &quot;SF&quot;, &quot;C&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SG&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;C&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;Pos&quot;, &quot;C&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;SG&quot;, &quot;PG&quot;, &quot;SG&quot;, &quot;PG&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;... $ Age &lt;chr&gt; &quot;23&quot;, &quot;26&quot;, &quot;26&quot;, &quot;26&quot;, &quot;23&quot;, &quot;31&quot;, &quot;28&quot;, &quot;28&quot;, &quot;31&quot;, &quot;27&quot;, &quot;35&quot;, &quot;26&quot;, &quot;38&quot;, &quot;34&quot;, &quot;23&quot;, &quot;23&quot;, &quot;23&quot;, &quot;23&quot;, &quot;28&quot;, &quot;22&quot;, &quot;32&quot;, &quot;34&quot;, &quot;31&quot;, &quot;28&quot;, &quot;Age&quot;, &quot;30&quot;, &quot;29&quot;, &quot;27&quot;, &quot;23&quot;, &quot;20&quot;, &quot;34&quot;, &quot;32&quot;, &quot;24&quot;, &quot;... $ Tm &lt;chr&gt; &quot;OKC&quot;, &quot;TOT&quot;, &quot;DAL&quot;, &quot;BRK&quot;, &quot;OKC&quot;, &quot;SAC&quot;, &quot;NOP&quot;, &quot;MIN&quot;, &quot;SAS&quot;, &quot;IND&quot;, &quot;MEM&quot;, &quot;POR&quot;, &quot;CLE&quot;, &quot;LAC&quot;, &quot;TOT&quot;, &quot;DAL&quot;, &quot;PHI&quot;, &quot;SAS&quot;, &quot;HOU&quot;, &quot;MIL&quot;, &quot;NYK&quot;, &quot;SAS&quot;, &quot;HOU&quot;, &quot;DEN&quot;, &quot;Tm&quot;, &quot;NOP&quot;, &quot;ORL&quot;, &quot;MIA&quot;, &quot;NYK&quot;... Selecting Columns Often you do not need the entire data set. While this is easily handled in base R (as shown earlier), it can be more clear to use select in dplyr. Now we won’t have to create separate objects, use quotes or $, etc. bball %&gt;% select(Player, Tm, Pos) %&gt;% head Player Tm Pos 1 Alex Abrines OKC SG 2 Quincy Acy TOT PF 3 Quincy Acy DAL PF 4 Quincy Acy BRK PF 5 Steven Adams OKC C 6 Arron Afflalo SAC SG What if we want to drop some variables? bball %&gt;% select(-Player, -Tm, -Pos) %&gt;% head Rk Age G GS MP FG FGA FG. X3P X3PA X3P. X2P X2PA X2P. eFG. FT FTA FT. ORB DRB TRB AST STL BLK TOV PF PTS 1 1 23 68 6 1055 134 341 .393 94 247 .381 40 94 .426 .531 44 49 .898 18 68 86 40 37 8 33 114 406 2 2 26 38 1 558 70 170 .412 37 90 .411 33 80 .413 .521 45 60 .750 20 95 115 18 14 15 21 67 222 3 2 26 6 0 48 5 17 .294 1 7 .143 4 10 .400 .324 2 3 .667 2 6 8 0 0 0 2 9 13 4 2 26 32 1 510 65 153 .425 36 83 .434 29 70 .414 .542 43 57 .754 18 89 107 18 14 15 19 58 209 5 3 23 80 80 2389 374 655 .571 0 1 .000 374 654 .572 .571 157 257 .611 281 332 613 86 89 78 146 195 905 6 4 31 61 45 1580 185 420 .440 62 151 .411 123 269 .457 .514 83 93 .892 9 116 125 78 21 6 42 104 515 Helper functions Sometimes, we have a lot of variables to select, and if they have a common naming scheme, this can be very easy. bball %&gt;% select(Player, contains(&quot;3P&quot;), ends_with(&quot;RB&quot;)) %&gt;% arrange(desc(TRB)) %&gt;% head Player X3P X3PA X3P. ORB DRB TRB 1 Player 3P 3PA 3P% ORB DRB TRB 2 Player 3P 3PA 3P% ORB DRB TRB 3 Player 3P 3PA 3P% ORB DRB TRB 4 Player 3P 3PA 3P% ORB DRB TRB 5 Player 3P 3PA 3P% ORB DRB TRB 6 Player 3P 3PA 3P% ORB DRB TRB The select also has helper functions to make selecting columns even easier. I probably don’t even need to explain what’s being done above, and this is the power of the tidyverse way. Here is the list of helper functions to be aware of: starts_with: starts with a prefix ends_with: ends with a prefix contains: contains a literal string matches: matches a regular expression num_range: a numerical range like x01, x02, x03. one_of: variables in character vector. everything: all variables. Filtering Rows There are repeated header rows in this data4, so we need to drop them. This is also why everything was character string when we first imported it, because having any character strings in a column coerces the entire column to be character, since all elements need to be of the same type. Character string is chosen over others because anything can be converted to a string, but not everything can be a number. Filtering by rows requires the basic indexing knowledge we talked about before, especially Boolean indexing. In the following, Rk, or rank, is really just a row id, but if it equals ‘Rk’ instead of a number, we know we’re dealing with a header row, so we’ll drop it. bball = bball %&gt;% filter(Rk != &quot;Rk&quot;) filter returns rows with matching conditions. slice allows for a numeric indexing approach5. Say we want too look at forwards (SF or PF) over the age of 35. The following will do this, and since some players play on multiple teams, we’ll want only the unique information on the variables of interest. The function distinct allows us to do this. bball %&gt;% filter(Age &gt; 35, Pos == &quot;SF&quot; | Pos == &quot;PF&quot;) %&gt;% distinct(Player, Pos, Age) Player Pos Age 1 Matt Barnes SF 36 2 Vince Carter SF 40 3 Nick Collison PF 36 4 Mike Dunleavy SF 36 5 Richard Jefferson SF 36 6 Dahntay Jones SF 36 7 James Jones SF 36 8 Mike Miller SF 36 9 Dirk Nowitzki PF 38 10 Paul Pierce SF 39 11 Luis Scola PF 36 12 Metta World Peace SF 37 Maybe we want just the first 10 rows… bball %&gt;% slice(1:10) # A tibble: 10 x 30 Rk Player Pos Age Tm G GS MP FG FGA FG. X3P X3PA X3P. X2P X2PA X2P. eFG. FT FTA FT. ORB DRB TRB AST STL BLK TOV PF PTS &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1 Alex Abrines SG 23 OKC 68 6 1055 134 341 .393 94 247 .381 40 94 .426 .531 44 49 .898 18 68 86 40 37 8 33 114 406 2 2 Quincy Acy PF 26 TOT 38 1 558 70 170 .412 37 90 .411 33 80 .413 .521 45 60 .750 20 95 115 18 14 15 21 67 222 3 2 Quincy Acy PF 26 DAL 6 0 48 5 17 .294 1 7 .143 4 10 .400 .324 2 3 .667 2 6 8 0 0 0 2 9 13 4 2 Quincy Acy PF 26 BRK 32 1 510 65 153 .425 36 83 .434 29 70 .414 .542 43 57 .754 18 89 107 18 14 15 19 58 209 5 3 Steven Adams C 23 OKC 80 80 2389 374 655 .571 0 1 .000 374 654 .572 .571 157 257 .611 281 332 613 86 89 78 146 195 905 6 4 Arron Afflalo SG 31 SAC 61 45 1580 185 420 .440 62 151 .411 123 269 .457 .514 83 93 .892 9 116 125 78 21 6 42 104 515 7 5 Alexis Ajinca C 28 NOP 39 15 584 89 178 .500 0 4 .000 89 174 .511 .500 29 40 .725 46 131 177 12 20 22 31 77 207 8 6 Cole Aldrich C 28 MIN 62 0 531 45 86 .523 0 0 45 86 .523 .523 15 22 .682 51 107 158 25 25 23 17 85 105 9 7 LaMarcus Aldridge PF 31 SAS 72 72 2335 500 1049 .477 23 56 .411 477 993 .480 .488 220 271 .812 172 351 523 139 46 88 98 158 1243 10 8 Lavoy Allen PF 27 IND 61 5 871 77 168 .458 0 1 .000 77 167 .461 .458 23 33 .697 105 114 219 57 18 24 29 78 177 We can use filtering even with variables just created. bball %&gt;% unite(&quot;posTeam&quot;, Pos, Tm) %&gt;% # create a new variable filter(posTeam == &quot;PF_SAS&quot;) %&gt;% # use it for filtering select(Player, posTeam, Age) %&gt;% # use it for selection arrange(desc(Age)) # order Player posTeam Age 1 David Lee PF_SAS 33 2 LaMarcus Aldridge PF_SAS 31 3 Davis Bertans PF_SAS 24 Generating New Data One of the most common data processing tasks beyond subsetting the data is generating new variables. The function mutate takes a vector and returns one of the same dimension. In addition, there is mutate_at, mutate_if, and mutate_all to help with specific scenarios. To demonstrate, we’ll use mutate_at to make appropriate columns numeric, i.e. everything except Player, Pos, and Tm. It takes two inputs, variables and functions to apply. As there are multiple variables and (potentially) multiple functions, we use the vars and funs functions to denote them. bball = bball %&gt;% mutate_at(vars(-Player, -Pos, -Tm), funs(as.numeric)) glimpse(bball[,1:7]) Observations: 595 Variables: 7 $ Rk &lt;dbl&gt; 1, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 13, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 29, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,... $ Player &lt;chr&gt; &quot;Alex Abrines&quot;, &quot;Quincy Acy&quot;, &quot;Quincy Acy&quot;, &quot;Quincy Acy&quot;, &quot;Steven Adams&quot;, &quot;Arron Afflalo&quot;, &quot;Alexis Ajinca&quot;, &quot;Cole Aldrich&quot;, &quot;LaMarcus Aldridge&quot;, &quot;Lavoy Allen&quot;, &quot;Tony Allen&quot;, &quot;Al-Farouq Aminu&quot;, &quot;Chris ... $ Pos &lt;chr&gt; &quot;SG&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;C&quot;, &quot;SG&quot;, &quot;C&quot;, &quot;C&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;SG&quot;, &quot;SF&quot;, &quot;C&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SG&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;C&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;C&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;SG&quot;, &quot;PG&quot;, &quot;SG&quot;, &quot;PG&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;S... $ Age &lt;dbl&gt; 23, 26, 26, 26, 23, 31, 28, 28, 31, 27, 35, 26, 38, 34, 23, 23, 23, 23, 28, 22, 32, 34, 31, 28, 30, 29, 27, 23, 20, 34, 32, 24, 36, 36, 36, 26, 31, 28, 28, 30, 27, 23, 20, 28, 30, 22, 19, 23, 21, 24, ... $ Tm &lt;chr&gt; &quot;OKC&quot;, &quot;TOT&quot;, &quot;DAL&quot;, &quot;BRK&quot;, &quot;OKC&quot;, &quot;SAC&quot;, &quot;NOP&quot;, &quot;MIN&quot;, &quot;SAS&quot;, &quot;IND&quot;, &quot;MEM&quot;, &quot;POR&quot;, &quot;CLE&quot;, &quot;LAC&quot;, &quot;TOT&quot;, &quot;DAL&quot;, &quot;PHI&quot;, &quot;SAS&quot;, &quot;HOU&quot;, &quot;MIL&quot;, &quot;NYK&quot;, &quot;SAS&quot;, &quot;HOU&quot;, &quot;DEN&quot;, &quot;NOP&quot;, &quot;ORL&quot;, &quot;MIA&quot;, &quot;NYK&quot;, &quot;MEM... $ G &lt;dbl&gt; 68, 38, 6, 32, 80, 61, 39, 62, 72, 61, 71, 61, 12, 30, 75, 51, 24, 72, 72, 80, 74, 19, 80, 41, 31, 78, 68, 52, 33, 67, 35, 79, 74, 54, 20, 60, 52, 77, 3, 75, 73, 77, 22, 56, 74, 38, 43, 23, 3, 67, 67,... $ GS &lt;dbl&gt; 6, 1, 0, 1, 80, 45, 15, 0, 72, 5, 66, 25, 0, 0, 10, 2, 8, 14, 72, 80, 74, 0, 80, 7, 19, 20, 55, 13, 1, 0, 6, 79, 18, 13, 5, 19, 0, 77, 1, 2, 64, 77, 1, 6, 0, 1, 0, 1, 0, 6, 67, 27, 1, 16, 66, 54, 54, ... Now that the data is correctly specified, the following demonstrates how we can use the standard mutate function to create composites of existing variables. bball = bball %&gt;% mutate(trueShooting = PTS / (2 * (FGA + (.44 * FTA))), effectiveFG = (FG + (.5 * X3P)) / FGA, shootingDif = trueShooting - FG.) summary(select(bball, shootingDif)) # select and others don&#39;t have to be piped to use shootingDif Min. :-0.46809 1st Qu.: 0.05202 Median : 0.09072 Mean : 0.08555 3rd Qu.: 0.11760 Max. : 0.39787 NA&#39;s :2 Grouping and Summarizing Data A very common task is to look at group-based statistics, and we can use group_by and summarize to help us in this regard6. Base R has things like aggregate,by, and tapply for this, but they should not be used, as this approach is much more straightforward, flexible, and faster. For this, I’m going to start putting together several things we’ve demonstrated thus far. Ultimately we’ll create a variable called trueShooting, which represents ‘true shooting percentage’, and get an average for each position. bball %&gt;% mutate(trueShooting = PTS / (2 * (FGA + (.44 * FTA))), effectiveFG = (FG + (.5 * X3P)) / FGA, shootingDif = trueShooting - FG.) %&gt;% select(Player, Tm, Pos, MP, trueShooting, effectiveFG, PTS) %&gt;% group_by(Pos) %&gt;% summarize(meanTrueShooting = mean(trueShooting, na.rm = TRUE)) # A tibble: 6 x 2 Pos meanTrueShooting &lt;chr&gt; &lt;dbl&gt; 1 C 0.5646389 2 PF 0.5164029 3 PF-C 0.5093448 4 PG 0.5105353 5 SF 0.5295641 6 SG 0.5154494 We can use do on grouped data to go further. This function can be used to create a new list-column in the data, the elements of which can be anything, even the results of an analysis for each group. As such, we can use tidyr’s unnest to get back to a standard data frame. To demonstrate, the following will group data by position, then get the correlation between field-goal percentage and free-throw shooting percentage. For some reason, one individual was labeled as ‘PF-C’, so we change them to just ‘C’. On your own, I recommend just looking at it to the do line first, and compare to this result. bball %&gt;% mutate(Pos = if_else(Pos==&#39;PF-C&#39;, &#39;C&#39;, Pos)) %&gt;% group_by(Pos) %&gt;% do(FgFt_Corr=cor(.$FG., .$FT., use=&#39;complete&#39;)) %&gt;% unnest(FgFt_Corr) # A tibble: 5 x 2 Pos FgFt_Corr &lt;chr&gt; &lt;dbl&gt; 1 C -0.43286280 2 PF -0.08935286 3 PG 0.12087949 4 SF 0.02330231 5 SG -0.01293017 As a reminder, data frames are lists. As such, anything can go into the ‘columns’. library(nycflights13) carriers = group_by(flights, carrier) group_size(carriers) [1] 18460 32729 714 54635 48110 54173 685 3260 342 26397 32 58665 20536 5162 12275 601 mods = do(carriers, model = lm(arr_delay ~ dep_time, data = .)) # reminder that data frames are lists mods %&gt;% summarize(rsq = summary(model)$r.squared) %&gt;% head # A tibble: 6 x 1 rsq &lt;dbl&gt; 1 0.05135040 2 0.05041396 3 0.08277597 4 0.02407390 5 0.03472122 6 0.08360260 You can use group_by on more than one variable, e.g. group_by(var1, var2) Merging Data Merging data is yet another very common data task, as data often comes from multiple sources. In order to do this, we need some common identifier among the sources by which to join them. The following is a list of dplyr join functions. inner_join: return all rows from x where there are matching values in y, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned. left_join: return all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned. right_join: return all rows from y, and all columns from x and y. Rows in y with no match in x will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned. semi_join: return all rows from x where there are matching values in y, keeping just columns from x. It differs from an inner join because an inner join will return one row of x for each matching row of y, where a semi join will never duplicate rows of x. anti_join: return all rows from x where there are not matching values in y, keeping just columns from x. full_join: return all rows and all columns from both x and y. Where there are not matching values, returns NA for the one missing. Probably the most common is a left join, where we kind of have one primary data set, and are adding data from another source to it. The following is a simple demonstration. band_members # A tibble: 3 x 2 Name Band &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise 2 Francis Pixies 3 Bubba The New Year band_instruments # A tibble: 3 x 2 Name Instrument &lt;chr&gt; &lt;chr&gt; 1 Seth Synthesizer 2 Francis Guitar 3 Bubba Guitar left_join(band_members, band_instruments) Joining, by = &quot;Name&quot; # A tibble: 3 x 3 Name Band Instrument &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise Synthesizer 2 Francis Pixies Guitar 3 Bubba The New Year Guitar Merges can get quite complex, and involve multiple data sources. In many cases you may have to do a lot of processing before getting to the merge, but dplyr’s joins will help quite a bit. tidyr The tidyr can be thought of as a specialized subset of dplyr’s functionality. Some of its functions for manipulating data you’ll want to be familiar with are: gather: wide to long spread: long to wide unite: paste together multiple columns into one separate: complement of unite unnest: unnest ‘list columns’ The following example shows how we take a ‘wide-form’ data set, where multiple columns represent different stock prices, and turn it into two columns, one representing stock name, and one for the price. library(tidyr) stocks &lt;- data.frame( time = as.Date(&#39;2009-01-01&#39;) + 0:9, X = rnorm(10, 0, 1), Y = rnorm(10, 0, 2), Z = rnorm(10, 0, 4) ) stocks %&gt;% head time X Y Z 1 2009-01-01 -0.2719193 0.5491413 4.3865986 2 2009-01-02 -0.2591219 -1.8420745 0.1846092 3 2009-01-03 -1.1934447 -0.7980515 -6.0348261 4 2009-01-04 2.0726259 0.1962904 -1.2891491 5 2009-01-05 -0.9536876 2.7113528 3.6755673 6 2009-01-06 2.3763177 -1.0983255 0.8223697 stocks %&gt;% gather(stock, price, -time) %&gt;% head time stock price 1 2009-01-01 X -0.2719193 2 2009-01-02 X -0.2591219 3 2009-01-03 X -1.1934447 4 2009-01-04 X 2.0726259 5 2009-01-05 X -0.9536876 6 2009-01-06 X 2.3763177 Note that the latter is an example of tidy data while the former is not. Why do we generally prefer such data? Precisely because the most common data operations, grouping, filtering, etc., would work notably more efficiently with such data. This is especially the case for visualization. The following demonstrates the separate function utilized for a very common data processing task- dealing with names. Here’ we’ll separate player into first and last names based on the space. bball %&gt;% separate(Player, into=c(&#39;firstName&#39;, &#39;lastName&#39;), sep=&#39; &#39;) %&gt;% select(1:5) %&gt;% head Rk firstName lastName Pos Age 1 1 Alex Abrines SG 23 2 2 Quincy Acy PF 26 3 2 Quincy Acy PF 26 4 2 Quincy Acy PF 26 5 3 Steven Adams C 23 6 4 Arron Afflalo SG 31 Note that this won’t necessarily apply to every name. So further processing may be required. purrr Purr allows you to take the apply family approach to the tidyverse. Consider Exercise 2 from the Vectorization/Apply section. We’ll use the map function to map the sum function to each element in the list, the same way we would with lapply. x = list(1:3, 4:6, 7:9) x %&gt;% map(sum) [[1]] [1] 6 [[2]] [1] 15 [[3]] [1] 24 The map functions take some getting used to, and in my experience they are typically slower than the apply functions. However they allow you stay within the tidy realm, which has its own benefits, and have more control over the nature of the output7. Personal Opinion The dplyr grammar is clear for a lot of standard data processing tasks, and some not so common. Extremely useful for data exploration and visualization. No need to create/overwrite existing objects Can overwrite columns as they are created Makes it easy to look at anything, and do otherwise tedious data checks Drawbacks: Not as fast as data.table or even some base R approaches for many things8 The mindset can make for unnecessary complication e.g. There is no need to pipe to create a single new variable Some approaches are not very intuitive Notably less ability to work with some very common data structures (e.g. matrices) All in all, if you’ve only been using base R approaches, the tidyverse will change your R life! It makes all the sorts of things you do all the time easier and clearer. Highly recommended! dplyr Exercises Exercise 0 Install and load the dplyr ggplot2movies packages. Look at the help file for the movies data set, which contains data from IMDB. install.packages(&#39;ggplot2movies&#39;) library(ggplot2movies) data(&#39;movies&#39;) Exercise 1 Using the movies data set, perform each of the following actions separately. Exercise 1a Use mutate to create a centered version of the rating variable. A centered variable is one whose mean has been subtracted from it. The process will take the following form: data %&gt;% mutate(new_var_name = &#39;?&#39;) Exercise 1b Use filter to create a new data frame that has only movies from the years 2000 and beyond. Use the greater than or equal operator &gt;=. Exercise 1c Use select to create a new data frame that only has the title, year, budget, length, rating and votes variables. There are at least 3 ways to do this. Exercise 2 Use group_by to group the data by year, and summarize to create a new variable that is the average budget. The summarize function works just like mutate in this case. Use the mean function to get the average, but you’ll also need to use the argument na.rm = TRUE within it because the earliest years have no budget recorded. Exercise 3 Now put it all together in one set of piped operations. Filter movies released after 1990 select the same variables as before but also the mpaa, Action, and Drama variables group by mpaa and (your choice) Action or Drama get the average rating It should spit out something like the following: # A tibble: 10 x 3 # Groups: mpaa [?] mpaa Drama AvgRating &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 0 5.940811 2 1 6.204495 3 NC-17 0 4.275000 4 NC-17 1 4.620000 5 PG 0 5.193590 6 PG 1 6.154913 7 PG-13 0 5.435729 8 PG-13 1 6.135092 9 R 0 4.855603 10 R 1 5.942602 You may be thinking- ‘it’s 2017 and why on earth would anyone do that?!’. Peruse most sports websites and you’ll see that fundamental web design basics escape them. See also, financial sites.↩ If you’re following good programming practices, you’ll likely never use slice.↩ As Hadley Wickham is from New Zealand, and his examples use summarise, you’ll probably see it about as much as you do the other spelling.↩ Personally I find the help file description nebulous and examples unrealistic, and oftentimes can come up with a group_by-summarise approach before I can sort out why the map approach didn’t work.↩ There is multidplyr, but it doesn’t appear to have been updated since well before dplyr itself underwent major changes.↩ "],
["08_datatable.html", "data.table", " data.table Another package for data processing that has been useful to many is data.table. It works in a notably different way than dplyr. However, you’d use it for the same reasons, e.g. subset, grouping, update, ordered joins etc., but with key advantages in speed and memory efficiency. Like dplyr, the data objects are both data.frames and a package specific class. library(data.table) dt = data.table(x=sample(1:10, 6), g=1:3, y=runif(6)) class(dt) [1] &quot;data.table&quot; &quot;data.frame&quot; Basics In general, data.table works with brackets as in base R data frames. However, in order to use data.table effectively you’ll need to forget the data frame similarity. The brackets actually work like a function call, with several key arguments. Consider the following notation to start. x[i, j, by, keyby, with = TRUE, ...] Importantly: you don’t use the brackets as you would with data.frames. What i and j can be are fairly complex. In general, you use i for filtering by rows. dt[2] dt[2,] x g y 1: 6 2 0.5620909 x g y 1: 6 2 0.5620909 You use j to select (by name!) or create new columns. We can define a new column with the := operator. dt[,x] dt[,z := x+y] # dt now has a new column dt[,z] dt[g&gt;1, mean(z), by=g] dt [1] 10 6 3 7 9 4 [1] 10.177423 6.562091 3.454054 7.738598 9.829444 4.816233 g V1 1: 2 8.195767 2: 3 4.135144 x g y z 1: 10 1 0.1774230 10.177423 2: 6 2 0.5620909 6.562091 3: 3 3 0.4540540 3.454054 4: 7 1 0.7385977 7.738598 5: 9 2 0.8294436 9.829444 6: 4 3 0.8162334 4.816233 Because j is an argument, dropping columns is awkward. dt[,-y] # creates negative values of y dt[,-&#39;y&#39;, with=F] # drops y, but now needs quotes ## dt[,y:=NULL] # drops y, but this is just a base R approach ## dt$y = NULL [1] -0.1774230 -0.5620909 -0.4540540 -0.7385977 -0.8294436 -0.8162334 x g z 1: 10 1 10.177423 2: 6 2 6.562091 3: 3 3 3.454054 4: 7 1 7.738598 5: 9 2 9.829444 6: 4 3 4.816233 Data table does not make unnecessary copies. For example if we do the following… DT = data.table(A=5:1,B=letters[5:1]) DT2 = DT DT2 and DT are just names for the same table. You’d actually need to use the copy function to make an explicit copy. DT2 = copy(DT) Grouped operations We can now attempt a ‘group-by’ operation, along with creation of a new variable. Note that these operations actually modify the dt object in place, a key distinction with dplyr. Fewer copies means less of a memory hit. dt1 = dt2 = dt dt[,sum(x,y), by=g] # sum of all x and y values g V1 1: 1 17.916021 2: 2 16.391535 3: 3 8.270287 dt1[,newvar := sum(x), by=g] # add new variable to the original data dt1 x g y z newvar 1: 10 1 0.1774230 10.177423 17 2: 6 2 0.5620909 6.562091 15 3: 3 3 0.4540540 3.454054 7 4: 7 1 0.7385977 7.738598 17 5: 9 2 0.8294436 9.829444 15 6: 4 3 0.8162334 4.816233 7 We can also create groupings on the fly. For a new summary data set, we’ll take the following approach- we create a grouping based on whether g is a value of one or not, then get the mean and sum of x for those two categories. The corresponding dplyr approach is also shown (but not evaluated) for comparison. dt2[, list(mean_x = mean(x), sum_x = sum(x)), by = g==1] g mean_x sum_x 1: TRUE 8.5 17 2: FALSE 5.5 22 ## dt2 %&gt;% group_by(g==1) %&gt;% summarise(mean_x=mean(x), sum_x=sum(x)) Faster! As mentioned, the reason to use data.table is speed. If you have large data or large operations it’ll be useful. Joins Joins can not only be faster but and easy to do. Note that the i argument can be a data.table object itself. I compare its speed to the comparable dplyr’s left_join function. dt1 = setkey(dt1, x) dt1[dt2] func mean (microseconds) dt_join 678.1 dplyr_join 1514 Group by We can use the setkey function to order a data set by a certain column(s). This ordering is done by reference; again, no copy is made. Doing this will allow for faster grouped operations, though you likely will only see the speed gain with very large data. test_dt0 = data.table(x=rnorm(10000000), g = sample(letters, 10000000, replace=T)) test_dt1 = copy(test_dt0) test_dt2 = setkey(test_dt1, g) identical(test_dt0, test_dt1) [1] FALSE identical(test_dt1, test_dt2) [1] TRUE func mean (milliseconds) test_dt0 427.4 test_dt1 146.7 test_dt2 146.7 String matching The chin function returns a vector of the positions of (first) matches of its first argument in its second, where both arguments are character vectors. Essentially it’s just like the %in% function for character vectors. Consider the following. We sample the first 14 letters 1000 times with replacement and see which ones match in a subset of another subset of letters. I compare the same operation to stringr and the stringi package whose functionality it’s using. lets_1 = sample(letters[1:14], 1000, replace=T) lets_1 %chin% letters[13:26] %&gt;% head(10) [1] TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE func mean (microseconds) dt 12.25 stringi 158.3 stringr 173.2 Reading files If you use data.table for nothing else, you’d still want to consider it strongly for reading in large text files. The function fread may be quite useful in being memory efficient too. I compare it to readr. fread(&#39;data/cars.csv&#39;) func mean (microseconds) dt 719.4 readr 5079 More speed The following demonstrates some timings from here. I reproduced it on my own machine based on 50 million observations. The grouped operations that are applied are just a sum and length on a vector. By the way, never, ever use aggregate. For anything. fun elapsed 1: aggregate 114.35 2: by 24.51 3: sapply 11.62 4: tapply 11.33 5: dplyr 10.97 6: lapply 10.65 7: data.table 2.71 Ever. Really. Pipe with data.table Piping can be done with data.table objects too, using the brackets, but it’s awkward at best. mydt[,newvar:=mean(x),][,newvar2:=sum(newvar), by=group][,-&#39;y&#39;, with=FALSE] mydt[,newvar:=mean(x), ][,newvar2:=sum(newvar), by=group ][,-&#39;y&#39;, with=FALSE ] Probably better to just use a standard pipe and dot approach if you really need it. mydt[,newvar:=mean(x),] %&gt;% .[,newvar2:=sum(newvar), by=group] %&gt;% .[,-&#39;y&#39;, with=FALSE] Summary Faster methods are great to have. If you have large data this is one package that can help. Especially for group-by and joins. Drawbacks: Complex The syntax can be awkward It doesn’t work like a data.frame, which can be confusing Piping with brackets isn’t really feasible Does not have its own ‘verse’ If speed and/or memory is (potentially) a concern, data.table. For interactive exploration, dplyr. Piping allows one to use both, so no need to choose. And on the horizon… dtplyr Coming soon to an R near you, or possibly abandonware, only time will tell. However, what the dtplyr package purports to do is implement the data.table back-end for dplyr so that you can seamlessly use them together. The following shows times for a grouped operation of a data frame of two variables, a random uniform draw of 5e7 values, and a grouping variable of 500k groups. package timing dplyr 10.97 data.table 2.71 dtplyr 2.7 Just for giggles I did the same in Python with a pandas DataFrame and group-by operation, and it took over seven seconds. But remember, R is slow! As of this writing, dplyr went through a notable update that changed some core functionality. Unfortunately, dtplyr has not been updated since then, so it will likely only work for the most standard of operations at present. data.table Exercises Exercise 0 Install and load the data.table package. Create the following data table. mydt = data.table(expand.grid(x=1:3, y=c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;)), z=sample(1:20, 9)) Exercise 1 Create a new object that contains only the ‘a’ group. Think back to how you use a logical to select rows. Exercise 2 Create a new object that is the sum of z grouped by x. You don’t need to name the sum variable. "],
["08b_Misc.html", "Miscellaneous", " Miscellaneous (SECTION DEVELOPMENT IN PROGRESS) Brief descriptions of other packages and tools that might be useful in data processing. MOAR TIDY dplyr functions: There are over a hundred functions that perform very common tasks. You really need to be aware of them as their use will come up often. broom: Convert statistical analysis objects from R into tidy data frames, so that they can more easily be combined, reshaped and otherwise processed with tools like dplyr, tidyr and ggplot2. tidy*: a lot of packages out there that are now ‘tidy’, though not a part of the official tidyverse. Some examples of the ones I’ve used: tidycensus tidybayes tidytext modelr Seriously, there are a lot. Documents pander DT "],
["09_thinkingvis.html", "Part II: Visualization Thinking Visually", " Part II: Visualization (SECTION DEVELOPMENT IN PROGRESS) Thinking Visually Information A starting point for data visualization regards the information you want to display, and then how you want to display it in order to tell the data’s story. As in statistical modeling, parsimony is the goal, but not at the cost of the more compelling story. We don’t want to waste the time of the audience or be redundant, but we also want to avoid unnecessary clutter, chart junk, and the like. We’ll start with a couple examples. Consider the following. So what’s wrong with this? Plenty. Aside from being boring, the entire story can be said with a couple words- males are taller than females (even in the Star Wars universe). There is no reason to have a visualization. And if a simple group difference is the most exciting thing you have to talk about, not many are going to be interested. Minor issues can also be noted, including unnecessary border around the bars, unnecessary vertical gridlines, and an unnecessary X axis label. You might think the following is an improvement, but I would say it’s even worse. Now the y axis has been changed to distort the difference, perceptually suggesting a height increase of over 54%. Furthermore, color is used but the colors are chosen poorly, and add no information, thus making the legend superfluous. And finally, the above doesn’t even convey the information people think it does, assuming they are even standard error bars9, which one typically has to guess about in many journal visualizations of this kind. Now we add more information, but more problems! The above has unnecessary border, gridlines, and emphasis. The labels, while possibly interesting, do not relate anything useful to the graph, and many are illegible. It imposes a straight (and too wide of a) straight line on a nonlinear relationship. And finally, color choice is both terrible and tends to draw one’s eye to the female data points. Here is what it looks like to someone with the most common form of colorblindness. If the points were less clumpy on gender, it would be very difficult to distinguish the groups. And here is what it might look like when printed. Now consider the following. We have six pieces of information in one graph- name (on hover), homeworld (shape), age (size), gender (color), mass (x), and height (y). The colors are evenly spaced from one another, and so do not draw one’s attention to one group over another, or even to the line over groups. Opacity allows the line to be added and the points to overlap without loss of information. We technically don’t need a caption, legend or gridlines, because hovering over the data tells us everything we’d want to know about a given data point. Whether this is something you’d prefer or not, the point is that we get quite a bit of information without being overwhelming, and the data is allowed to express itself cleanly. Here are some things to keep in mind when creating visualizations for scientific communication. Your audience isn’t dumb Assume your audience, which in academia is full of people with advanced degrees or those aspiring to obtain one, can handle more than a bar graph. If the visualization is good and well-explained10, they’ll be fine. See the data visualization and maps sections of 2016: The Year in Visual Stories and Graphics at the New York Times. Good data visualization of even complex relationships can be appreciated by more than an academic audience. Assume you can at least provide visualizations on that level of complexity and be okay. Clarity is key Sometimes the clearest message is a complicated one. That’s okay, science is an inherently fuzzy process. Make sure your visualization tells the story you think is important, and don’t dumb the story down in the visualization. People will likely remember the graphic before they’ll remember the table of numbers. By the same token, don’t needlessly complicate something that is straightforward. Perhaps a scatter plot with some groupwise coloring is enough. That’s fine. All of this is easier said than done, and there is no right way to do data visualizations. Prepare to experiment. Avoid clutter Gridlines, 3d, unnecessary patterning, and chartjunk in general will only detract from the message. As an example, gridlines might even seem necessary, but even faint ones can potentially hinder the pattern recognition you hope will take place, perceptually imposing clumps of data that do not exist. In addition, they practically insist on a level of data precision that you simply don’t have. What’s more, with interactivity they literally convey nothing additional, as a simple hover-over or click on a data point will reveal the precise values. Use sparingly, if at all. Color isn’t optional It’s odd for me to have to say this, as it’s been the case for many years, but no modern scientific outlet should be a print-first outfit, and if they are, you shouldn’t care to send your work there. The only thing you should be concerned with is how it will look online, because that’s how people will interact with your work first and foremost. That means that color is essentially a requirement for any visualization, so use it well in yours. Think interactively It might be best to tart by making the visualization you want to make, with interactivity and anything else you like. You can then reduce as necessary for publication or other outlets, and keep the fancy one as supplemental, or accessible on your own website to show off. Color There is a lot to consider regarding color. Until recently, the default color schemes of most visualization packages were poor at best. Thankfully, ggplot2, its imitators and extenders, in both the R world and beyond, have made it much easier to have a decent color scheme by default11. However, the defaults are still potentially problematic, so you should be prepared to go with something else. In other cases, you may just simply prefer something else. For example, for me, the gray background of ggplot2 defaults is something I have to remove for every plot12. Viridis A couple packages will help you get started in choosing a decent color scheme. One is viridis. As stated in the package description: These color maps are designed in such a way that they will analytically be perfectly perceptually-uniform, both in regular form and also when converted to black-and-white. They are also designed to be perceived by readers with the most common form of color blindness. So basically you have something that will take care of your audience without having to do much. There are four palettes. These color schemes might seem a bit odd from what you’re used to. But recall that the goal is good communication, and these will allow you to convey information accurately, without implicit bias, and be acceptable in different formats. In addition, there is ggplot2 functionality to boot, e.g. scale_color_viridis, and it will work for discrete or continuous valued data. For more, see the vignette. I also invite you to watch the introduction of the original module in Python, where you can learn more about the issues in color selection, and why viridis works. RColorBrewer Color Brewer offers another collection of palettes that will generally work well in a variety of situations. While there are print and color-blind friendly palettes, not all adhere to those restrictions. Specifically though, you have palettes for the following data situations: Qualitative (e.g. Dark213) Sequential (e.g. Reds) Diverging (e.g. RdBu) There is a ggplot2 function, scale_color_brewer, you can use as well. For more, see colorbrewer.org. There you can play around with the palettes to help make your decision. Contrast Thankfully, websites have mostly gotten past the phase where there text looks like this. The goal of scientific communication is to, well, communicate. Making text hard to read is pretty much antithetical to this. So contrast comes into play with text as well as color. In general, you should consider a 7 to 1 contrast ratio for text, minimally 4 to 1. -Here is text at 2 to 1 -Here is text at 4 to 1 -Here is text at 7 to 1 -Here is black I personally don’t like stark black, and find it visually irritating, but obviously that would be fine to use for most people. Scaling by Size You might not be aware, but there is more than one way to scale objects, e.g. in a scatterplot. Consider the following, where in both cases dots are scaled by the person’s body-mass index (BMI). The first plot scales the dots by their area, while the second scales the radius, but otherwise they are identical. It’s not generally recommended to scale the radius, as our perceptual system is more attuned to the area. Packages like ggplot2 and plotly will automatically do this, but some might not, so you should check. Transparency Using transparency is a great way to keep detailed information available to the audience without being overwhelming. Consider the following. Fifty individual trajectories are shown on the left, but it doesn’t cause any issue graphically. Without transparency, it just looks ugly, and notably busier if nothing else. In addition, transparency can be used to add additional information to a plot. In the following scatter plot, we can get a better sense of data density from the fact that the plot is darker where points overlap more. Here we apply it to a density plot to convey a group difference in distributions, while still being able to visualize the whole distribution of each group. Had we not done so, we might not be able to tell what’s going on with some of the groups at all. In general, a good use of transparency can potentially help any visualization, but consider it especially when trying to display many points, or otherwise have overlapping data. Accessibility Among many things (apparently) rarely considered in typical academic or other visualization is accessibility. The following definition comes from the World Wide Web Consortium. Web accessibility means that people with disabilities can use the Web. More specifically, Web accessibility means that people with disabilities can perceive, understand, navigate, and interact with the Web, and that they can contribute to the Web. Web accessibility also benefits others, including older people with changing abilities due to aging. The gist is that not everyone is able to use the web in the same manner. While you won’t be able to satisfy everyone who might come across, putting a little thought into your offering can go along way, and potentially widen your audience. We talked about this previously, but when communicating visually, one can do simple things like choosing a colorblind-friendly palette, or using a font contrast that will make it easier on the eyes of those reading your work. There are even browser plugins to test your web content for accessibility. In addition, there are little things like adding a title to inserted images, making links more noticeable etc., all of which can help consumers of your information. A casual list of things to avoid I’m just putting things that come to mind here. Mostly it is personal opinion, though often based on various sources in the data visualization realm or simply my own experience. Pie Pie charts and their cousins, e.g. bar charts (and stacked versions), wind rose plots etc., either convey too little information or make otherwise simple information more difficult to process perceptually. Histograms Anyone that’s used R’s hist function knows the frustration here. Use density plots instead. They convey the same information but better, and typical defaults are usually fine. Using 3D without adding any communicative value You will often come across use of 3D in scientific communication which is fairly poor and makes the data harder to interpret. In general, when going beyond two dimensions, your first thought should be to use color, size, etc. and finally prefer interactivity to 3D. Where it is useful is in things like showing structure (e.g. molecular, geographical), or continuous multi-way interactions. Using too many colors Some put a completely non-scientifically based number on this, but the idea holds. For example, if you’re trying to show U.S. state grouping by using a different color for all 50 states, no one’s going to be able to tell the yellow for Alabama vs. the slightly different yellow for Idaho. Alternatives would be to show the information via a map or use a hover over display. Showing maps that just display population Most of the maps I see on the web cover a wide range of data and can be very visually appealing, but pretty much just tell me where the most populated areas are, which is completely uninteresting. Make sure that your geographical depiction is more informative than this. Biplots A lot of folks doing PCA resort to biplots for interpretation, where a graphical model would be much more straightforward. See this chapter for example. Thinking Visually Exercises Exercise 1 The following uses the diamonds data set that comes with ggplot2. Use the scale_color_viridis function to add a more accessible palette. library(ggplot2); library(viridis) ggplot(aes(x=carat, y=price), data=diamonds) + geom_point(aes(color=price)) + ???? Exercise 2 Now color it by the cut. To use the scale_color_viridis approach, you’ll have to change one of the arguments to the function (set discrete=T). Reproduce this but using one of the other viridis palettes. See the helpfile via ?scale_color_viridis to see how to change the palette. Thinking exercises For your upcoming presentation, who is your audience? Error bars for group means can overlap and still be statistically different (the test regards the difference in means). Furthermore most visuals of this sort don’t bother to say whether it is standard deviation, standard error, or 2*standard error, or even something else.↩ People seem to think there are text limits for captions. There are none.↩ Even Matlab finally joined the club, except that they still screwed up with their default coloring scheme.↩ Hadley states “The grey background gives the plot a similar colour (in a typographical sense) to the remainder of the text, ensuring that the graphics fit in with the flow of a text without jumping out with a bright white background. Finally, the grey background creates a continuous field of colour which ensures that the plot is perceived as a single visual entity.”. The part about it being ugly is apparently left out. ☺ Also, my opinion is that it has the opposite effect, making the visualization jump out because nothing on the web is gray by default. If anything the page background is white, and having a white/transparent background would perhaps be better, but honestly, don’t you want a visualization to jump out?↩ Don’t even think about asking what the Dark1 palette is.↩ "],
["10_ggplot2.html", "ggplot2", " ggplot2 (SECTION DEVELOPMENT IN PROGRESS) The most popular visualization package in R is ggplot2. It’s so popular, it or its aesthetic is copied in other languages/programs as well. It entails a grammar of graphics (hence the gg), and learning that grammar is key to using it effectively. Some of the strengths of ggplot2 include: The ease of getting a good looking plot Easy customization A lot of necessary data processing is done for you Clear syntax Easy multidimensional approach Decent default color scheme as a default Lots of extensions Every graph is built from the same few parts, and it’s important to be aware of a few key ideas, which we will cover in turn. Layers (and geoms) Aesthetics Piping Facets Scales Themes Extensions Note that while you can obviously use base R for visualization, it’s never going to be easier, nor as flexible as ggplot2. If you’re used to using base R visuals, you should be prepared to leave them behind. Layers In general, we start with a base layer and add to it. In most cases you’ll start as follows. ggplot(aes(x=height, y=mass), data=starwars) This would just produce a plot background, but nothing else. However, with the foundation in place, we’re now ready to add something to it. Let’s add some points (the outlier is Jabba the Hut). ggplot(aes(x=height, y=mass), data=starwars) + geom_point() Perhaps we want to change labels or theme. These would be additional layers to the plot. ggplot(aes(x=height, y=mass), data=starwars) + geom_point() + labs(x=&#39;Height in cm&#39;, y=&#39;Weight in kg&#39;) + theme_dark() Each layer is consecutively added by means of a pipe operator, and layers may regard geoms, scales, labels, facets etc. You may have many different layers to produce one plot, and there really is no limit. However some efficiencies may be possible for a given situation. For example, it’s more straightforward to use geom_smooth than calculate fits, standard errors etc. and then add multiple geoms to produce the same thing. This is the sort of thing you’ll get used to as you use ggplot more. Piping Layers are added via piping (+). The first layers added are typically geoms, or geometric objects that represent the data, and include things like: points lines density text In case you’re wondering why ggplot doesn’t use %&gt;% as in the tidyverse, it’s because ggplot2 was using pipes before it was cool, well before those came along. Otherwise, the concept is the same as we saw in the data processing section. ggplot(aes(x=myvar, y=myvar2), data=mydata) + geom_point() Our base is provided via the ggplot function, and specifies the data at the very least, but commonly also the x and y aesthetics. The geom_point function adds a layer of points, and now we would have a scatterplot. Alternatively, you could have specified the x and y aesthetic at the geom_point layer, but if you’re going to have the same x, y, color, etc. aesthetics regardless of layer, put it in the base. Otherwise, doing it by layer gives you more flexibility if needed. Geoms even have their own data argument, allowing you to combine information from several sources for a single visualization. Aesthetics Aesthetics map data to various visual aspects of the plot, including size, color etc. The function used in ggplot to do this is aes. aes(x=myvar, y=myvar2, color=myvar3, group=g) The best way to understand what goes into the aes function is if the value is varying. For example, if I want the size of points to be a certain value, I would code the following. ... + geom_point(..., size=4) However, if I want the size to be associated with the data in some way, I use it as an aesthetic. ... + geom_point(aes(size=var_x)) The same goes for practically any aspect of a geom- size, color, fill, etc. If it is a fixed value, set it outside the aesthetic. If it varies based on the data, put it within an aesthetic. Examples library(ggplot2) data(&quot;diamonds&quot;); data(&#39;economics&#39;) ggplot(aes(x=carat, y=price), data=diamonds) + geom_point() ggplot(aes(x=date, y=unemploy), data=economics) + geom_line() In the following, one setting, alpha (transparency), is not mapped to the data. ggplot(aes(x=carat, y=price), data=diamonds) + geom_point(aes(size=carat, color=clarity), alpha=.05) Stats There are many statistical functions built in, and it is a key strength of ggplot that you don’t have to do a lot of processing for very common plots. Quantile regression lines: ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_quantile() Loess (or additive model) smooth. This shows how we can do some fine-tuning and use model-based approaches for visualization. data(mcycle, package=&#39;MASS&#39;) ggplot(aes(x=times, y=accel), data=mcycle) + geom_point() + geom_smooth(formula=y ~ s(x, bs=&#39;ad&#39;), method=&#39;gam&#39;) Bootstrapped confidence intervals: ggplot(mtcars, aes(cyl, mpg)) + geom_point() + stat_summary(fun.data = &quot;mean_cl_boot&quot;, colour = &quot;orange&quot;, alpha=.75, size = 1) The take-home message here is to always let ggplot do the work for you if at all possible. Scales Often there are many things we want to change about the plot, for example, the size and values of axis labels, the range of sizes for points to take, the specific colors we want to use, and so forth. Be aware that there are a great many options here, and you will regularly want to use them. A very common thing you’ll do is change the labels for the axes. You definitely don’t have to go and change the variable name itself to do this, just use the labs function. There are also functions for individual parts, e.g. xlab, ylab and ggtitle. ggplot(aes(x=times, y=accel), data=mcycle) + geom_smooth(se=F) + labs(x=&#39;milliseconds after impact&#39;, y=&#39;head acceleration&#39;, title=&#39;Motorcycle Accident&#39;) A frequent operation is changing the x and y look in the form of limits and tick marks. Like labs, there is a general lims function and specific functions for just the specific parts. In addition, we may want to get really detailed using scale_x_* or scale_y_*. ggplot(mpg, aes(displ, hwy, size=cyl)) + geom_point() + ylim(c(0,60)) ggplot(mpg, aes(displ, hwy, size=cyl)) + geom_point() + scale_y_continuous(limits=c(0,60), breaks=seq(0,60,by=12), minor_breaks=seq(6,60,by=6)) Another common option is to change the size of points in some way. While we assign the aesthetic as before, it comes with defaults that might not work for a given situation. Play around with the range values. ggplot(mpg, aes(displ, hwy, size=cyl)) + geom_point() + scale_size(range=c(1,3)) Now that you know more about color issues, you may want to apply something besides the default options. The following shows a built-in color scale for a color aesthetic that is treated as continuous, and one that is discrete and which we want to supply our own colors (these actually come from plotly’s default color scheme). ggplot(mpg, aes(displ, hwy, color=cyl)) + geom_point() + scale_color_gradient2() ggplot(mpg, aes(displ, hwy, color=factor(cyl))) + geom_point() + scale_color_manual(values=c(&quot;#1f77b4&quot;, &quot;#ff7f0e&quot;, &quot;#2ca02c&quot;, &quot;#d62728&quot;)) We can even change the scale of the data itself. ggplot(mpg, aes(displ, hwy)) + geom_point() + scale_x_log10() In short, scales alterations are really useful for getting just the plot you want, and there is a lot of flexibility for you to work with. Facets Facets allow for paneled display, a very common operation. In general, we often want comparison plots. The facet_grid function will produce a grid, and often this is all that’s needed. However, facet_wrap is more flexible, while possibly taking a bit extra to get things just the way you want. Both use a formula approach to specify the grouping. facet_grid Facet by cylinder. ggplot(mtcars, aes(wt, mpg)) + geom_point() + facet_grid(~ cyl, labeller = label_both) ggplot(mpg, aes(displ, cty)) + geom_point() + facet_grid(~ cyl, labeller = label_both) ggplot(midwest, aes(popdensity, percbelowpoverty)) + geom_point() + facet_grid(~ state, labeller = label_both) Facet by vs and cylinder. ggplot(mtcars, aes(wt, mpg)) + geom_point() + facet_grid(vs ~ cyl, labeller = label_both) facet_wrap Specify the number of columns or rows with facet_wrap. ggplot(mtcars, aes(wt, mpg)) + geom_point() + facet_wrap(vs ~ cyl, labeller = label_both, ncol=2) Fine control ggplot2 makes it easy to get good looking graphs quickly. However the amount of fine control is extensive. The following plot is hideous (aside from the background, which is totally rad), but illustrates the point. ggplot(aes(x=carat, y=price), data=diamonds) + annotation_custom(rasterGrob(lambosun, width=unit(1,&quot;npc&quot;), height=unit(1,&quot;npc&quot;), interpolate = FALSE), -Inf, Inf, -Inf, Inf) + geom_point(aes(color=clarity), alpha=.5) + scale_y_log10(breaks=c(1000,5000,10000)) + xlim(0, 10) + scale_color_brewer(type=&#39;div&#39;) + facet_wrap(~cut, ncol=3) + theme_minimal() + theme(axis.ticks.x=element_line(color=&#39;darkred&#39;), axis.text.x=element_text(angle=-45), axis.text.y=element_text(size=20), strip.text=element_text(color=&#39;forestgreen&#39;), strip.background=element_blank(), panel.grid.minor=element_line(color=&#39;lightblue&#39;), legend.key=element_rect(linetype=4), legend.position=&#39;bottom&#39;) Themes In the last example you saw two uses of a theme- a built-in version that comes with ggplot (theme_minimal), and specific customization (theme(…)). The built-in themes provide ready-made approaches that might be enough for a finished product. For the theme function, each argument, and there are many, takes on a specific value or an element function: element_rect element_line element_text element_blank Each of those element functions has arguments specific to it. For example, for element_text you can specify the font size, while for element line you could specify the line type. Note that the base theme of ggplot is not too good. Doesn’t work well for web presentation, and is even worse for print. You will almost invariably need to tweak it. Extensions ggplot2 now has its own extension system, and there is even a website to track the extensions. Examples include: additional themes maps interactivity animations marginal plots network graphs aligning multiple ggplot visualizations In short, ggplot2 is only the beginning. You’ll have a lot of tools at your disposal. Furthermore, many modeling and other packages will produce ggplot graphics to which you can add your own layers and tweak like you would any other ggplot. Summary ggplot2 ggplot2 is an easy to use, but powerful visualization tool. Allows one to think in many dimensions for any graph: x y color size opacity facet 2D graphs are only useful for conveying the simplest of ideas. Use ggplot2 to easily create more interesting visualizations. ggplot2 Exercises Exercise 0 Install and load the ggplot2 package. Exercise 1 Create two plots, one a scatterplot (e.g. with geom_point) and one with lines (e.g. geom_line) with a data set of your choosing (all of the following are base R or available after loading ggplot2. Some suggestions: faithful: Waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. msleep: mammals sleep dataset with sleep times and weights etc. diamonds: used in the slides economics: US economic time series. txhousing: Housing sales in TX. midwest: Midwest demographics. mpg: Fuel economy data from 1999 and 2008 for 38 popular models of car Recall the basic form for ggplot. ggplot(aes(x=*, y=*, other), data=*) + geom_*() + otherLayers, theme etc. Themes to play with: theme_bw theme_classic theme_dark theme_gray theme_light theme_linedraw theme_minimal theme_trueMinimal (requires the lazerhawk package and an appreciation of the Lamborghini background from the previous visualization) Exercise 2 Play around and change the arguments to the following. You’ll need to install the maps package. For example, do points for all county midpoints, with different colors. For that you’d need to change the x and y for the point geom to an aesthetic based on the longitude and latitude, as well as change its data argument to use the seats data frame. library(maps) mi = map_data(&quot;county&quot;, &quot;michigan&quot;) seats = mi %&gt;% group_by(subregion) %&gt;% summarise_at(vars(lat, long), function(x) median(range(x))) ggplot(mi, aes(long, lat)) + geom_polygon(aes(group = group), fill = NA, colour = &quot;grey60&quot;) + geom_text(aes(label = subregion), data = seats, size = 1, angle = 45) + geom_point(x=-83.748333, y=42.281389, color=&#39;#1e90ff&#39;, size=3) + theme_minimal() + theme(panel.grid=element_blank()) "],
["11_interactive.html", "Interactive Visualization", " Interactive Visualization (SECTION DEVELOPMENT IN PROGRESS) Packages As mentioned, ggplot2 is the most widely used package for visualization in R. However, it is not interactive by default. Many packages use htmlwidgets, d3 (JavaScript library), and other tools to provide interactive graphics. What’s great is that while you may have to learn new packages, you don’t necessarily have to change your approach or thinking about a plot, or learn some other language. Many of these packages can be lumped into more general packages that try to provide a plotting system (similar to ggplot2), versus those that just aim to do a specific type of plot well. Here are some to give a sense of this. General: plotly used also in Python, Matlab, Julia can convert ggplot2 images to interactive ones (with varying degrees of success) highcharter also very general wrapper for highcharts.js and works with some R packages out of the box rbokeh like plotly, it also has cross language support Specific functionality: DT interactive data tables leaflet maps with OpenStreetMap visNetwork Network visualization In what follows we’ll see some of these in action. Note that unlike the previous chapter, the goal here is not to dive deeply, but to get an idea of what’s available. Piping for Visualization One of the advantages to piping is that it’s not limited to dplyr style data management functions. Any R function can be potentially piped to, and several examples have already been shown. Many newer visualization packages take advantage of piping, and this facilitates data exploration. We don’t have to create objects just to do a visualization. New variables can be easily created and subsequently manipulated just for visualization. Furthermore, data manipulation not separated from visualization. htmlwidgets The htmlwidgets package is a package that makes it easy to create javascript visualizations. If you’re not familiar with javascript, you actually are, as it’s basically the language of the web, visual or otherwise. The packages using it typically are pipe-oriented and produce interactive plots. In addition, you can use the htmlwidgets package to create your own functions that use a particular javascript library (but someone probably already has). Plotly We’ll begin our foray into the interactive world with a couple demonstrations of plotly. To give some background, you can think of plotly similar to RStudio, in that it has both enterprise (i.e. pay for) aspects and open source aspects. Just like RStudio, you have full access to what it has to offer via the open source R package. You may see old help suggestions referring to needing an account, but this is no longer necessary. When using plotly, you’ll note the layering approach similar to what we did with ggplot2. Piping is used before plotting to do some data manipulation, after which we seamlessly move to the plot itself. The =~ is essentially the way we denote aesthetics14. library(plotly) midwest %&gt;% filter(inmetro==T) %&gt;% plot_ly(x=~percbelowpoverty, y=~percollege) %&gt;% add_markers() plotly has modes, which allow for points, lines, text and combinations. Traces, add_*, work similar to geoms. library(mgcv); library(modelr); library(glue) mtcars %&gt;% mutate(amFactor = factor(am, labels=c(&#39;auto&#39;, &#39;manual&#39;)), hovertext = glue(&#39;weight: {wt} &lt;br&gt; mgp: {mpg} &lt;br&gt; {amFactor}&#39;)) %&gt;% add_predictions(gam(mpg~s(wt, am, bs=&#39;fs&#39;), data=mtcars)) %&gt;% plot_ly() %&gt;% add_markers(x=~wt, y=~mpg, color=~amFactor, opacity=.5, text=~hovertext, hoverinfo=&#39;text&#39;, showlegend=F) %&gt;% add_lines(x=~wt, y=~pred, color=~amFactor, name=&#39;gam prediction&#39;) While you can use plotly as a one-liner15, this would only be good for quick peeks while doing data exploration. It would generally be far too limiting otherwise. plot_ly(midwest, x = ~percollege, color = ~state, type = &quot;box&quot;) ggplotly One of the strengths of plotly is that we can feed a ggplot object to it, and turn our formerly static plots into interactive ones. It would have been easy to use geom_smooth to get a similar result, so let’s do so. gp = mtcars %&gt;% mutate(amFactor = factor(am, labels=c(&#39;auto&#39;, &#39;manual&#39;)), hovertext = paste(wt, mpg, amFactor)) %&gt;% arrange(wt) %&gt;% ggplot(aes(x=wt, y=mpg, color=amFactor)) + geom_smooth(se=F) + geom_point(aes(color=amFactor)) ggplotly() Note that this is not a one-to-one transformation. The plotly image will have different line widths and point sizes. It will usually be easier to change it within the ggplot process than tweaking the ggplotly object. Be prepared to spend time getting used to plotly. It has terrible documentation, is not nearly as flexible as ggplot2, has hidden (and arbitrary) defaults that can creep into a plot based on aspects of the data (rather than your settings), and some modes do not play nicely with others. That said, it works great for a lot of things. Highcharter Highcharter is also fairly useful for a wide variety of plots, and is based on the highcharts.js library. If you have data suited to one of its functions, getting a great interactive plot can be ridiculously easy. In what follows we use quantmod to create an xts (time series) object of Google’s stock price, including opening and closing values. The highcharter object has a ready-made plot for such data16. library(highcharter); library(quantmod) google_price = getSymbols(&quot;GOOG&quot;, auto.assign = FALSE) hchart(google_price) visNetwork The visNetwork package is specific to network visualizations and similar, and is based on the vis.js library. Networks require nodes and edges to connect them. These take on different aspects, and so are created in separate data frames. set.seed(1352) nodes = data.frame(id = 0:5, label = c(&#39;Bobby&#39;, &#39;Janie&#39;,&#39;Timmie&#39;, &#39;Mary&#39;, &#39;Johnny&#39;, &#39;Billy&#39;), group = c(&#39;friend&#39;, &#39;frenemy&#39;,&#39;frenemy&#39;, rep(&#39;friend&#39;, 3)), value = sample(10:50, 6)) edges = data.frame(from = c(0,0,0,1,1,2,2,3,3,3,4,5,5), to = sample(0:5, 13, replace = T), value = sample(1:10, 13, replace = T)) %&gt;% filter(from!=to) library(visNetwork) visNetwork(nodes, edges, height=300, width=800) %&gt;% visNodes(shape=&#39;circle&#39;, font=list(), scaling=list(min=10, max=50, label=list(enable=T))) %&gt;% visLegend() leaflet The leaflet package from RStudio is good for quick interactive maps, and it’s quite flexible and has some nice functionality to take your maps further. Unfortunately, it actually doesn’t play well with many markdown formats. hoevertext &lt;- paste(sep = &quot;&lt;br/&gt;&quot;, &quot;&lt;b&gt;&lt;a href=&#39;http://umich.edu/&#39;&gt;University of Michigan&lt;/a&gt;&lt;/b&gt;&quot;, &quot;Ann Arbor, MI&quot; ) library(leaflet) leaflet() %&gt;% addTiles() %&gt;% addPopups(-83.738222, 42.277030, hoevertext, options = popupOptions(closeButton = FALSE) ) DT It might be a bit odd to think of data frames visually, but they can be interactive also. Use the DT package for interactive data frames. This can be very useful when working in collaborative environments where one shares reports, as you can embed the data within the document itself. library(DT) ggplot2movies::movies %&gt;% select(1:6) %&gt;% filter(rating&gt;8, !is.na(budget), votes &gt; 1000) %&gt;% datatable() The other thing to be aware of is that tables can be visual, it’s just that many academic outlets waste this opportunity. Simple bolding, italics, and even sizing, can make results pop more easily for the audience. The DT package even allows for coloring and simple things like bars that connotes values. iris %&gt;% # arrange(desc(Petal.Length)) %&gt;% datatable(rownames=F, options=list(dom=&#39;firtp&#39;), class = &#39;row-border&#39;) %&gt;% formatStyle(&#39;Sepal.Length&#39;, fontWeight = styleInterval(5, c(&#39;normal&#39;, &#39;bold&#39;))) %&gt;% formatStyle( &#39;Sepal.Width&#39;, color = styleInterval(c(3.4, 3.8), c(&#39;#7f7f7f&#39;, &#39;#00aaff&#39;, &#39;#ff5500&#39;)), backgroundColor = styleInterval(3.4, c(&#39;#ebebeb&#39;, &#39;aliceblue&#39;)) ) %&gt;% formatStyle( &#39;Petal.Length&#39;, # color = &#39;transparent&#39;, background = styleColorBar(iris$Petal.Length, &#39;#5500ff&#39;), backgroundSize = &#39;100% 90%&#39;, backgroundRepeat = &#39;no-repeat&#39;, backgroundPosition = &#39;center&#39; ) %&gt;% formatStyle( &#39;Species&#39;, color=&#39;white&#39;, transform = &#39;rotateX(45deg) rotateY(20deg) rotateZ(30deg)&#39;, backgroundColor = styleEqual( unique(iris$Species), c(&#39;#1f65b7&#39;, &#39;#66b71f&#39;, &#39;#b71f66&#39;) ) ) I would in no way recommend using the bars (unless the you want a visual instead of the value) or angled tag options, as that is more or less a prime example of chartjunk. However, subtle use of color and emphasis can make tables of results that your audience will actually spend time exploring. Shiny Shiny is a framework that can essentially allow you to build an interactive website/app. Like some of the other packages mentioned, it’s provided by RStudio developers. However, most of the more recently developed interactive visualization packages will work specifically within the shiny and rmarkdown setting. You can make shiny apps just for your own use and run them locally. But note, you are using R to build a webpage, and it’s not particularly well-suited for it. Much of how you use R will not be useful in building a shiny app, and so it will definitely take some getting used to, and you will likely need to do a lot of tedious adjustments to get things just how you want. Shiny apps have two main components, a part that specifies the user interface, and a server function that will do all the work. With those in place (either in a single ‘app.R’ file or in separate files), you can then simply run app. This example is taken from the shiny help file, and you can actually run it as is. library(shiny) # Running a Shiny app object app &lt;- shinyApp( ui = bootstrapPage( numericInput(&#39;n&#39;, &#39;Number of obs&#39;, 100), plotOutput(&#39;plot&#39;) ), server = function(input, output) { output$plot &lt;- renderPlot({ hist(runif(input$n)) }) } ) runApp(app) You can share your app code/directory with anyone and they’ll be able to run it also. However, this is great mostly just for teaching someone how to do shiny, which most people aren’t going to do. Typically you’ll want someone to use the app itself, not run code. In that case you’ll need a web server. You can get up to 5 free ‘running’ applications at shinyapps.io. However, you will notably be limited in the amount of computing resources that can be used to run the apps in a given month. Even minor usage of those could easily overtake the free settings. For personal use it’s plenty though. Interactive and Visual Data Exploration As seen above, just a couple visualization packages can go a very long way. It’s now very easy to incorporate interactivity, so you should use it even if only for your own data exploration. In general, interactivity allows for even more dimensions to be brought to a graphic, and can be more fun too! However, they must serve a purpose. Too often they are simply distraction, and can actually detract from the data story. Make sure to use them when they can enhance the narrative you wish to express. Interactive Visualization Exercises Exercise 0 Install and load the plotly package. Load the tidyverse package if necessary (so you can use dplyr and ggplot2), and install/load the ggplot2movies for the IMDB data. Exercise 1 Using dplyr group by year, and summarize to create a new variable that is the Average rating. Then create a plot with plotly for a line or scatter plot (for the latter, use the add_markers function). It will take the following form, but you’ll need to supply the plotly arguments. library(ggplot2movies) movies %&gt;% group_by(year) %&gt;% summarise(Avg_Rating=mean(rating)) plot_ly() %&gt;% add_markers() Exercise 2 This time group by year and Drama. In the summarize create average rating again, but also a variable representing the average number of votes. In your plotly line, use the size and color arguments to represent whether the average number of votes and whether it was drama or not respectively. Use add_markers. Note that Drama will be treated as numeric since it’s a 0-1 indicator. This won’t affect the plot, but if you want, you might use mutate to change it to a factor with labels ‘Drama’ and ‘Other’. Exercise 3 Create a ggplot of your design and then use ggplotly to make it interactive. Often you’ll get an error because you used = instead of =~. Also, I find trying to set single values for things like size unintuitive, and it may be implemented differently for different traces (e.g. setting size in marker traces requires size=I(value)).↩ You can with ggplot2 as well, but I intentionally made no mention of it. You should learn how to use the package generally before learning how to use its shortcuts.↩ This is the sort of thing that takes the ‘wow factor’ out of a lot of stuff you see on the web. You’ll find a lot of those cool plots are actually lazy folks using default settings of stuff that make only take a single line of code.↩ "],
["999_summary.html", "Summary", " Summary With the right tools, data exploration can be: easier faster more efficient more fun! Use them to wring your data dry of what it has to offer. Recommended next steps: R for Data Science Advanced R Embrace a richer understanding of your data! "]
]
