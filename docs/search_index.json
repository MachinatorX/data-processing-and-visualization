[
["index.html", "Data Processing &amp; Visualization", " Data Processing &amp; Visualization Michael Clark https://m-clark.github.io/ 2020-02-27 "],
["intro.html", "Intro Outline Preparation Other", " Intro This document is the basis for multiple workshops, whose common goal is to provide some tools, tips, packages etc. that make data processing, programming, modeling, visualization, and presentation in R easier. It is oriented toward those who have had some exposure to R in an applied data analysis fashion, but would also be useful to someone coming to R from another programming language. It is not an introduction to R. The goal here is primarily to instill awareness, specifically of tools that will make your data exploration, modeling, and visualization easier, and to understand some of the why behind the tools, so that one can better implement them. It is meant to fill in some of the gaps that typically befall applied users of R. Outline Part 1: Information Processing Understanding Basic R Approaches to Gathering and Processing Data Overview of Data Structures Input/Output Indexing Getting Acquainted with Other Approaches to Data Processing Pipes, and how to use them tidyverse data.table Misc. Part 2: Programming Basics Using R more fully Dealing with objects Iterative programming Writing functions Going further Code style Vectorization Regular expressions Part 3: Modeling Model Exploration Key concepts Understanding and fitting models Overview of extensions Model Criticism Model Assessment Model Comparison Machine Learning Concepts Demonstration of techniques Part 4: Visualization Thinking Visually Visualizing Information Color Contrast and more… ggplot2 Aesthetics Layers Themes and more… Adding Interactivity Package demos Shiny Part 5: Presentation Possible future addition. See this for now. Preparation To follow along with the examples, clone/download the related section repos. Downloading any one of them will have an R project and associated data, such that the code from any section should run. (in progress as document is being revamped and extended) R I: Information Processing R II: Programming R III: Modeling R IV: Visualization R V: Presentation Other Color coding in text: emphasis package function object/class link Some key packages used in the following demonstrations and exercises: tidyverse (several packages), data.table, ggplot2movies Python Python notebooks for the data processing section and visualization sections may be found here. R Many other packages are also used, so feel free to install as we come across them. Here are a few. tidymodels, nycflights13, DT, highcharter, magrittr, maps, mgcv (already comes with base R), plotly, quantmod, readr, visNetwork "],
["data_structures.html", "Data Structures Vectors Matrices Lists Data Frames Data Structure Exercises", " Data Structures In order to use R to understand the world around you, you have to know the basics of how R works. Everything in R revolves around information in the form of data, so let’s start with how data exists within R. R has several core data structures, and we’ll take a look at each. Vectors Factors Lists Matrices/arrays Data frames The more you know about R data structures, the more you’ll know how to use them, how packages use them, and you’ll also better understand why things go wrong if they do, and the further you’ll be able to go with your data. Vectors Vectors form the basis of R data structures. Two main types are atomic and lists, but we’ll talk about lists separately. Here is an R vector. The elements of the vector are numeric values. x = c(1, 3, 2, 5, 4) x [1] 1 3 2 5 4 All elements of an atomic vector are the same type. Example types include: character numeric (double) integer logical In addition, there are special kinds of values like NA (‘not available’ i.e. missing), NULL, NaN (not a number), Inf (infinite) and so forth. You can use typeof to examine an object’s type, or use an is function, e.g. is.logical, to check if an object is a specific type. Character strings When dealing with text, objects of class character are what you’d typically be dealing with. x = c(&#39;... Of Your Fake Dimension&#39;, &#39;Ephemeron&#39;, &#39;Dryswch&#39;, &#39;Isotasy&#39;, &#39;Memory&#39;) x Not much to it, but be aware there is no real limit to what is represented as a character vector. For example, in a data frame, you could have a column where each entry is one of the works of Shakespeare. Factors An important type of vector is a factor. Factors are used to represent categorical data structures. Although not exactly precise, one can think of factors as integers with labels. So the underlying representation of a variable for sex is 1:2 with labels ‘Male’ and ‘Female’. They are a special class with attributes, or metadata, that contains the information about the levels. x = factor(rep(letters[1:3], e = 10)) x [1] a a a a a a a a a a b b b b b b b b b b c c c c c c c c c c Levels: a b c attributes(x) $levels [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; $class [1] &quot;factor&quot; The underlying representation is numeric, but it is important to remember that factors are categorical. Thus, they can’t be used as numbers would be, as the following demonstrates. x_num = as.numeric(x) # convert to a numeric object sum(x_num) [1] 60 sum(x) Error in Summary.factor(structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, : &#39;sum&#39; not meaningful for factors Strings vs. Factors The main thing to note is that factors are generally a statistical phenomenon, and are required to do statistical things with data that would otherwise be a simple character string. If you know the relatively few levels the data can take, you’ll generally want to use factors, or at least know that statistical packages and methods may require them. In addition, factors allow you to easily overcome the silly default alphabetical ordering of category levels in some very popular visualization packages. For other things, such as text analysis, you’ll almost certainly want character strings instead, and in many cases it will be required. It’s also worth noting that a lot of base R and other behavior will coerce strings to factors. This made a lot more sense in the early days of R, but is not really necessary these days. Some packages to note to help you with processing strings and factors: forcats stringr Logicals Logical scalar/vectors are those that take on one of two values TRUE or FALSE. They are especially useful in flagging whether to run certain parts of code, and indexing certain parts of data structures (e.g. taking rows that correspond to TRUE). We’ll talk about the latter usage more later in the document. Here is a logical vector. my_logic = c(TRUE, FALSE, TRUE, FALSE, TRUE, TRUE) Note also that logicals are also treated as binary 0:1, and so, for example, taking the mean will provide the proportion of TRUE values. !my_logic [1] FALSE TRUE FALSE TRUE FALSE FALSE as.numeric(my_logic) [1] 1 0 1 0 1 1 mean(my_logic) [1] 0.6666667 Numeric and integer The most common type of data structure you’ll deal with are integer and numeric vectors. class(1:3) [1] &quot;integer&quot; x = rnorm(5) x [1] 0.23407910 -0.67915124 0.01324971 -0.81492346 -0.78106288 class(x) [1] &quot;numeric&quot; Matrices With multiple dimensions, we are dealing with arrays. Matrices are 2-d arrays, and extremely commonly used for scientific computing. The vectors making up a matrix must all be of the same type. For example, all values in a matrix might be numeric, or all character strings. Creating a matrix Creating a matrix can be done in a variety of ways. # create vectors x = 1:4 y = 5:8 z = 9:12 rbind(x, y, z) # row bind [,1] [,2] [,3] [,4] x 1 2 3 4 y 5 6 7 8 z 9 10 11 12 cbind(x, y, z) # column bind x y z [1,] 1 5 9 [2,] 2 6 10 [3,] 3 7 11 [4,] 4 8 12 matrix( c(x, y, z), nrow = 3, ncol = 4, byrow = TRUE ) [,1] [,2] [,3] [,4] [1,] 1 2 3 4 [2,] 5 6 7 8 [3,] 9 10 11 12 Lists Lists in R are highly flexible objects, and probably the most commonly used for applied data science. Unlike vectors, whose elements must be of the same type, lists can contain anything as their elements, even other lists. Here is a list. We use the list function to create it. x = list(1, &quot;apple&quot;, list(3, &quot;cat&quot;)) x [[1]] [1] 1 [[2]] [1] &quot;apple&quot; [[3]] [[3]][[1]] [1] 3 [[3]][[2]] [1] &quot;cat&quot; We often want to loop some function over a list. for (elem in x) print(class(elem)) [1] &quot;numeric&quot; [1] &quot;character&quot; [1] &quot;list&quot; Lists can, and often do, have named elements. x = list(&quot;a&quot; = 25, &quot;b&quot; = -1, &quot;c&quot; = 0) x[[&quot;b&quot;]] [1] -1 Almost all standard models in base R and other packages return an object that is a list. Knowing how to work with a list will allow you to easily access the contents of the model object for further processing. Python has similar structures, lists and dictionaries, where the latter works similarly to R’s named list. Data Frames Data frames are a very commonly used data structure. Elements of a data frame can be different types, and this is because the data.frame class is actually just a list. As such, everything about lists applies to them. But they can also be indexed by row or column as well, just like matrices. There are other very common types of object classes associated with packages that are both a data.frame and some other type of structure (e.g. tibbles in the tidyverse). Usually your data frame will come directly from import or manipulation of other R objects (e.g. matrices). However, you should know how to create one from scratch. Creating a data frame The following will create a data frame with two columns, a and b. mydf = data.frame(a = c(1,5,2), b = c(3,8,1)) Much to the disdain of the tidyverse, we can add row names also. rownames(mydf) = paste0(&#39;row&#39;, 1:3) mydf a b row1 1 3 row2 5 8 row3 2 1 Everything about lists applies to data.frames, so we can add, select, and remove elements of a data frame just like lists. However we’ll visit this more in depth later, and see that we’ll have much more flexibility with data frames than we would lists for common data analysis and visualization. Data Structure Exercises Excercise #1 Create an object that is a matrix and/or a data.frame, and inspect its class or structure (use the class or str functions on the object you just created). Exercise #2 Create a list of 3 elements, the first of which contains character strings, the second numbers, and the third, the data.frame or matrix you just created. Thinking Exercises How is a factor different from a character vector? How is a data.frame the same as and different from a matrix? How is a data.frame the same as and different from a list? "],
["io.html", "Input/Output Other approaches Faster approaches Other Data On the horizon Big Data I/O Exercises", " Input/Output Until you get comfortable getting data into R, you’re not going to use it as much as you would. You should at least be able to read in common data formats like comma/tab-separated, Excel, etc. Standard methods of reading in tabular data include the following functions: read.table read.csv readLines Base R also comes with the foreign package for reading in other types of files, especially other statistical packages. However, while you may see it still in use, it’s not as useful as what’s found in other packages. Reading in data is usually a one-off event, such that you’ll never need to use the package again after the data is loaded. In that case, you might use the following approach, so that you don’t need to attach the whole package. readr::read_csv(&#39;fileloc/filename.csv&#39;) You can use that for any package, which can help avoid naming conflicts by not loading a bunch of different packages. Furthermore, if you need packages that do have a naming conflict, using this approach will ensure the function from the package you want will be used. Other approaches There are some better and faster ways to read in data than the base R approach. A package for reading in foreign statistical files is haven, which has functions like read_spss and read_dta for SPSS and Stata files respectively. The package readxl is a clean way to read Excel files that doesn’t require any outside packages or languages. The package rio uses haven, readxl etc., but with just two functions for everything: import, export (also convert). Faster approaches For faster versions of base R functions, readr has read_csv, read_delim, and others. These make assumptions about what type each vector is after an initial scan of the data, then proceed accordingly. If you don’t have ‘big’ data, the subsequent speed gain won’t help much, however, such an approach actually can be used as a diagnostic to pick up potential data entry errors, as warnings are given when unexpected observations occur. The data.table package provides a faster version read.table, and is typically faster than readr approaches (fread). Other Data Be aware that R can handle practically any type of data you want to throw at it. Some examples include: JSON SQL XML YAML MongoDB NETCDF text (e.g. a novel) shapefiles (e.g. for geographic data) Google spreadsheets And many, many others. On the horizon feather is designed to make reading/writing data frames efficient, and the really nice thing about it is that it works in both Python and R. It’s still in early stages of development on the R side though. Big Data You may come across the situation where your data cannot be held in memory. One of the first things to be aware of for data processing is that you may not need to have the data all in memory at once. Before shifting to a hardware solution, consider if the following is possible. Chunking: reading and processing the data in chunks Line at a time: dealing with individual lines of data Other data formats: for example SQL databases (sqldf package, src_dbi in dplyr) However, it may be that the end result is still too large. In that case you’ll have to consider a cluster-based or distributed data situation. Of course R will have tools for that as well. DBI sparklyr And more. I/O Exercises Exercise 1 Use readr and haven to read the following files. Use the URL just like you would any file name. The latter is a Stata file. You can use the RStudio’s menu approach to import the file if you want. https://raw.githubusercontent.com/m-clark/data-processing-and-visualization/master/data/cars.csv https://raw.githubusercontent.com/m-clark/data-processing-and-visualization/master/data/presvote.dta If you downloaded the data for this workshop, the files can be accessed in that folder. Thinking Exercises Why might you use read_csv from the readr package rather than read.csv in base R? What is your definition of ‘big’ data? "],
["indexing.html", "Indexing Base R Indexing Refresher List/Data.frame extraction Indexing Exercises", " Indexing What follows is a refresher. Presumably you’ve had enough R exposure to be aware of some of this. However, much of data processing regards data frames, or other tables of mixed data types, so more time will be spent on slicing and dicing of data frames instead. Even so, it would be impossible to use R effectively without knowing how to handle basic data types. Base R Indexing Refresher Slicing vectors letters[4:6] # lower case letters a-z [1] &quot;d&quot; &quot;e&quot; &quot;f&quot; letters[c(13, 10, 3)] [1] &quot;m&quot; &quot;j&quot; &quot;c&quot; Slicing matrices/data.frames myMatrix[1, 2:3] # matrix[rows, columns] Label-based indexing mydf[&#39;row1&#39;, &#39;b&#39;] Position-based indexing mydf[1, 2] Mixed indexing mydf[&#39;row1&#39;, 2] If the row/column value is empty, all rows/columns are retained. mydf[&#39;row1&#39;, ] mydf[, &#39;b&#39;] Non-contiguous mydf[c(1, 3), ] Boolean mydf[mydf$a &gt;= 2, ] List/Data.frame extraction [ : grab a slice of elements/columns [[ : grab specific elements/columns $ : grab specific elements/columns @: extract slot for S4 objects my_list_or_df[2:4] my_list_or_df[[&#39;name&#39;]] my_list_or_df$name my_list@name In general, position-based indexing should be avoided, except in the case of iterative programming of the sort that will be covered later. The reason is that these become magic numbers when not commented, such that no one will know what they refer to. In addition, any change to the rows/columns of data will render the numbers incorrect, where labels would still be applicable. Indexing Exercises This following is a refresher of base R indexing only. Here is a matrix, a data.frame and a list. mymatrix = matrix(rnorm(100), 10, 10) mydf = cars mylist = list(mymatrix, thisdf = mydf) Exercise 1 For the matrix, in separate operations, take a slice of rows, a selection of columns, and a single element. Exercise 2 For the data.frame, grab a column in 3 different ways. Exercise 3 For the list grab an element by number and by name. "],
["pipes.html", "Pipes Using variables as they are created Pipes for Visualization (more later) The dot Flexibility Summary", " Pipes Pipes are operators that send what comes before the pipe to what comes after. There are many different pipes, and some packages that use their own. However, the vast majority of packages use the same pipe: %&gt;% Here, we’ll focus on their use with the dplyr package, and the tidyverse more generally. Pipes are also utilized heavily in visualization. Example: mydf %&gt;% select(var1, var2) %&gt;% filter(var1 == &#39;Yes&#39;) %&gt;% summary() Start with a data.frame %&gt;% select columns from it %&gt;% filter/subset it %&gt;% get a summary Using variables as they are created One nice thing about pipelines is that we can use variables as soon as they are created, without having to break out separate objects/steps. mydf %&gt;% mutate(newvar1 = var1 + var2, newvar2 = newvar1 / var3) %&gt;% summarise(newvar2avg = mean(newvar2)) Pipes for Visualization (more later) The following provides a means to think about pipes for visualization. It’s just a generic example for now, but we’ll see more later. basegraph %&gt;% points %&gt;% lines %&gt;% layout The dot Most functions are not ‘pipe-aware’ by default, or at least, do not have data as their first argument as most functions in the tidyverse do, as well as others using tidystyle. In the following we try to send our data frame to lm for a regression. mydf %&gt;% lm(y ~ x) # error Other pipes could potentially work in this situation, e.g. %$% in magrittr. But generally, when you come upon this, you can use a dot to represent the object before the pipe. mydf %&gt;% lm(y ~ x, data=.) # . == mydf Flexibility Piping is not just for data.frames. For example, the more general list objects can be used as well, and would be the primary object for the purrr family of functions we’ll discuss later. As a final example, we’ll create a function in an abstract way with pipes. The following starts with a character vector. Sends it to a recursive function (named ..). .. is created on-the-fly, and has a single argument (.). After the function is created, it’s used on ., which represents the string before the pipe. Result: pipes between the words1. c(&#39;Ceci&#39;, &quot;n&#39;est&quot;, &#39;pas&#39;, &#39;une&#39;, &#39;pipe!&#39;) %&gt;% { .. &lt;- . %&gt;% if (length(.) == 1) . else paste(.[1], &#39;%&gt;%&#39;, ..(.[-1])) ..(.) } [1] &quot;Ceci %&gt;% n&#39;est %&gt;% pas %&gt;% une %&gt;% pipe!&quot; Put that in your pipe and smoke it René Magritte! Summary Pipes are best used interactively, though you can use them within functions as well, and they are extremely useful for data exploration. Nowadays, more and more packages are being made that are ‘pipe-aware’, especially many visualization packages. See the magrittr package for more types of pipes, and more detail on pipes is provided in these slides. That was a very complicated way to do this paste(c('Ceci', \"n'est\", 'pas', 'une', 'pipe!'), collapse=' %&gt;% ').↩︎ "],
["tidyverse.html", "Tidyverse What is the tidyverse? What is tidy? dplyr Running example Selecting Columns Filtering Rows Generating New Data Grouping and Summarizing Data Renaming columns Merging Data tidyr More Tidyverse Personal Opinion tidyverse Exercises", " Tidyverse What is the tidyverse? The tidyverse consists of a few key packages: ggplot2: data visualization tibble: tibbles, a modern re-imagining of data frames tidyr: data tidying readr: data import purrr: functional programming, e.g. alternate approaches to apply dplyr: data manipulation And of course the tidyverse package itself, which will load all of the above in a way that will avoid naming conflicts. library(tidyverse) Loading tidyverse: ggplot2 Loading tidyverse: tibble Loading tidyverse: tidyr Loading tidyverse: readr Loading tidyverse: purrr Loading tidyverse: dplyr Conflicts with tidy packages ------------------------- filter(): dplyr, stats lag(): dplyr, stats In addition, there are other packages like lubridate, rvest, stringr and others in the hadleyverse that are also greatly useful. What is tidy? Tidy data refers to data arranged in a way that makes data processing, analysis, and visualization simpler. In a tidy data set: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Think long before wide. dplyr dplyr provides a grammar of data manipulation (like ggplot2 does for visualization). It is the next iteration of plyr, but plyr is deprecated and no longer used. It’s focused on tools for working with data frames, with over 100 functions that might be of specific use to you. It has three main goals: Make the most important data manipulation tasks easier. Do them faster. Use the same interface to work with data frames, data tables or a database. Some key operations include: select: grab columns select helpers: one_of, starts_with, num_range etc. filter/slice: grab rows group_by: grouped operations mutate/transmute: create new variables summarize: summarize/aggregate There are various (SQL-like) join/merge functions: inner_join, left_join etc. And there are a lot of little things like: n, n_distinct, nth, n_groups, count, recode, between In addition, there is no need to quote variable names. An example Let’s say we want to select from our data the following variables: Start with the ID variable The variables X1 through X10, which are not all grouped together, and there are many more X* columns The variables var1 and var2, which are the only variables with var in their name Any variable with a name that starts with XYZ How might we go about this in a dataset of possibly hundreds or even thousands of columns? There are several base R approaches that we could go with, but often they will be tedious, or require multiple objects to be created just to get the columns you want. Let’s start with the worst choice. newData = oldData[,c(1,2,3,4, etc.)] Using numeric indexes, or rather magic numbers, is not conducive to readability or reproducibility. If anything changes about the data columns, the numbers may no longer be applicable, and you’d have to redo the line again. We could name the variables explicitly. newData = oldData[,c(&#39;ID&#39;,&#39;X1&#39;, &#39;X2&#39;, etc.)] This would be fine if there are only a handful. But if you’re trying to reduce a 1000 column data set to several dozen it’s tedious, and generally not pretty regardless. A more advanced alternative regards a two-step approach with regular expressions. This requires that you know something about regex (and you should), but it is difficult to read/understand by those who don’t, and often by even yourself if it’s more complicated. In any case, you first will need to create an object that represents the column names first, otherwise it looks unwieldy if used within brackets or a function like subset. cols = c(&#39;ID&#39;, paste0(&#39;X&#39;, 1:10), &#39;var1&#39;, &#39;var2&#39;, grep(colnames(oldData), &#39;^XYZ&#39;, value=T)) newData = oldData[,cols] # or via subset newData = subset(oldData, select = cols) Now consider there is even more to do. What if you also want observations where Z is Yes, Q is No, and only the observations with the top 50 values of var2, ordered by var1 (descending)? Probably the more straightforward way in R to do so would be something like the following, where each part is broken out and we continuously write over the object as we modify it. # three operations and overwriting or creating new objects if we want clarity newData = newData[oldData$Z == &#39;Yes&#39; &amp; oldData$Q == &#39;No&#39;,] newData = newData[order(newData$var2, decreasing=T)[1:50],] newData = newData[order(newData$var1, decreasing=T),] And this is for fairly straightforward operations. Now consider doing all of the previous in one piped operation. The dplyr package will allow us to do something like the following. newData = oldData %&gt;% select(num_range(&#39;X&#39;, 1:10), contains(&#39;var&#39;), starts_with(&#39;XYZ&#39;)) %&gt;% filter(Z == &#39;Yes&#39;, Q == &#39;No&#39;) %&gt;% top_n(n=50, var2) %&gt;% arrange(desc(var1)) Even if it hadn’t been explained before, you might have been able to guess a little as to what was going on. The code is fairly succinct, we don’t have to keep referencing objects repeatedly, and no explicit intermediary objects are created. dplyr and piping is an alternative. You can do all this sort of stuff with base R, for example, with functions like with, within, subset, transform, etc. Though the initial base R approach depicted is fairly concise, in general, it can potentially be: more verbose less legible less amenable to additional data changes requires esoteric knowledge (e.g. regular expressions) often requires creation of new objects (even if we just want to explore) often slower, possibly greatly Running example The following data was scraped initially scraped from the web as follows. It is data from the NBA basketball league for the 2017-2018 season with things like player names, position, team name, points per game, field goal percentage, and various other statistics. We’ll use it as an example to demonstrate various functionality found within dplyr. library(rvest) current_year = lubridate::year(Sys.Date()) url = glue::glue(&quot;http://www.basketball-reference.com/leagues/NBA_{current_year-1}_totals.html&quot;) bball = read_html(url) %&gt;% html_nodes(&quot;#totals_stats&quot;) %&gt;% html_table() %&gt;% data.frame() save(bball, file=&#39;data/bball.RData&#39;) However you can just load it into your workspace as below. Note that when initially gathered from the website, the data is all character strings. We’ll fix this later. The following shows the data as it will eventually be. load(&#39;data/bball.RData&#39;) glimpse(bball[,1:5]) Observations: 734 Variables: 5 $ Rk &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;, &quot;16&quot;, &quot;16&quot;, &quot;16&quot;, &quot;17&quot;, &quot;18&quot;, &quot;19&quot;, &quot;20&quot;, &quot;Rk&quot;, &quot;21&quot;, &quot;22&quot;, &quot;23&quot;, &quot;23&quot;, &quot;23&quot;, &quot;24&quot;, &quot;25&quot;, &quot;26&quot;, &quot;27&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;… $ Player &lt;chr&gt; &quot;Álex Abrines&quot;, &quot;Quincy Acy&quot;, &quot;Jaylen Adams&quot;, &quot;Steven Adams&quot;, &quot;Bam Adebayo&quot;, &quot;Deng Adel&quot;, &quot;DeVaughn Akoon-Purcell&quot;, &quot;LaMarcus Aldridge&quot;, &quot;Rawle Alkins&quot;, &quot;Grayson Allen&quot;, &quot;Jarrett Allen&quot;, &quot;Kadeem Allen&quot;,… $ Pos &lt;chr&gt; &quot;SG&quot;, &quot;PF&quot;, &quot;PG&quot;, &quot;C&quot;, &quot;C&quot;, &quot;SF&quot;, &quot;SG&quot;, &quot;C&quot;, &quot;SG&quot;, &quot;SG&quot;, &quot;C&quot;, &quot;SG&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;C&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;Pos&quot;, &quot;SF&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;PG&quot;, &quot;C&quot;, &quot;SG&quot;, &quot;PF&quot;, &quot;SG&quot;, &quot;SG&quot;, &quot;SG… $ Age &lt;chr&gt; &quot;25&quot;, &quot;28&quot;, &quot;22&quot;, &quot;25&quot;, &quot;21&quot;, &quot;21&quot;, &quot;25&quot;, &quot;33&quot;, &quot;21&quot;, &quot;23&quot;, &quot;20&quot;, &quot;26&quot;, &quot;28&quot;, &quot;25&quot;, &quot;25&quot;, &quot;30&quot;, &quot;30&quot;, &quot;30&quot;, &quot;20&quot;, &quot;24&quot;, &quot;21&quot;, &quot;34&quot;, &quot;Age&quot;, &quot;21&quot;, &quot;24&quot;, &quot;33&quot;, &quot;33&quot;, &quot;33&quot;, &quot;31&quot;, &quot;20&quot;, &quot;23&quot;, &quot;19&quot;, &quot;25&quot;, &quot;25… $ Tm &lt;chr&gt; &quot;OKC&quot;, &quot;PHO&quot;, &quot;ATL&quot;, &quot;OKC&quot;, &quot;MIA&quot;, &quot;CLE&quot;, &quot;DEN&quot;, &quot;SAS&quot;, &quot;CHI&quot;, &quot;UTA&quot;, &quot;BRK&quot;, &quot;NYK&quot;, &quot;POR&quot;, &quot;ATL&quot;, &quot;MEM&quot;, &quot;TOT&quot;, &quot;PHO&quot;, &quot;MIA&quot;, &quot;IND&quot;, &quot;MIL&quot;, &quot;DAL&quot;, &quot;HOU&quot;, &quot;Tm&quot;, &quot;TOR&quot;, &quot;CHI&quot;, &quot;TOT&quot;, &quot;PHO&quot;, &quot;WAS&quot;, &quot;ORL&quot;, … Selecting Columns Often you do not need the entire data set. While this is easily handled in base R (as shown earlier), it can be more clear to use select in dplyr. Now we won’t have to create separate objects, use quotes or $, etc. bball %&gt;% select(Player, Tm, Pos) %&gt;% head() Player Tm Pos 1 Álex Abrines OKC SG 2 Quincy Acy PHO PF 3 Jaylen Adams ATL PG 4 Steven Adams OKC C 5 Bam Adebayo MIA C 6 Deng Adel CLE SF What if we want to drop some variables? bball %&gt;% select(-Player, -Tm, -Pos) %&gt;% head() Rk Age G GS MP FG FGA FG. X3P X3PA X3P. X2P X2PA X2P. eFG. FT FTA FT. ORB DRB TRB AST STL BLK TOV PF PTS 1 1 25 31 2 588 56 157 .357 41 127 .323 15 30 .500 .487 12 13 .923 5 43 48 20 17 6 14 53 165 2 2 28 10 0 123 4 18 .222 2 15 .133 2 3 .667 .278 7 10 .700 3 22 25 8 1 4 4 24 17 3 3 22 34 1 428 38 110 .345 25 74 .338 13 36 .361 .459 7 9 .778 11 49 60 65 14 5 28 45 108 4 4 25 80 80 2669 481 809 .595 0 2 .000 481 807 .596 .595 146 292 .500 391 369 760 124 117 76 135 204 1108 5 5 21 82 28 1913 280 486 .576 3 15 .200 277 471 .588 .579 166 226 .735 165 432 597 184 71 65 121 203 729 6 6 21 19 3 194 11 36 .306 6 23 .261 5 13 .385 .389 4 4 1.000 3 16 19 5 1 4 6 13 32 Helper functions Sometimes, we have a lot of variables to select, and if they have a common naming scheme, this can be very easy. bball %&gt;% select(Player, contains(&quot;3P&quot;), ends_with(&quot;RB&quot;)) %&gt;% arrange(desc(TRB)) %&gt;% head() Player X3P X3PA X3P. ORB DRB TRB 1 Player 3P 3PA 3P% ORB DRB TRB 2 Player 3P 3PA 3P% ORB DRB TRB 3 Player 3P 3PA 3P% ORB DRB TRB 4 Player 3P 3PA 3P% ORB DRB TRB 5 Player 3P 3PA 3P% ORB DRB TRB 6 Player 3P 3PA 3P% ORB DRB TRB The select also has helper functions to make selecting columns even easier. I probably don’t even need to explain what’s being done above, and this is the power of the tidyverse way. Here is the list of helper functions to be aware of: starts_with: starts with a prefix ends_with: ends with a suffix contains: contains a literal string matches: matches a regular expression num_range: a numerical range like x01, x02, x03. one_of: variables in character vector. everything: all variables. Filtering Rows There are repeated header rows in this data2, so we need to drop them. This is also why everything was character string when we first scraped it, because having any character strings in a column coerces the entire column to be character, since all elements of a vector need to be of the same type. Character string is chosen over others because anything can be converted to a string, but not everything can be a number. Filtering by rows requires the basic indexing knowledge we talked about before, especially Boolean indexing. In the following, Rk, or rank, is for all intents and purposes just a row id, but if it equals the actual text ‘Rk’ instead of something else, we know we’re dealing with a header row, so we’ll drop it. bball = bball %&gt;% filter(Rk != &quot;Rk&quot;) filter returns rows with matching conditions. slice allows for a numeric indexing approach3. Say we want to look at forwards (SF or PF) over the age of 35. The following will do this, and since some players play on multiple teams, we’ll want only the unique information on the variables of interest. The function distinct allows us to do this. bball %&gt;% filter(Age &gt; 35, Pos == &quot;SF&quot; | Pos == &quot;PF&quot;) %&gt;% distinct(Player, Pos, Age) Player Pos Age 1 Vince Carter PF 42 2 Kyle Korver PF 37 3 Dirk Nowitzki PF 40 Maybe we want just the first 10 rows. This is often the case when we perform some operation and need to quickly verify that what we’re doing is working in principle. bball %&gt;% slice(1:10) Rk Player Pos Age Tm G GS MP FG FGA FG. X3P X3PA X3P. X2P X2PA X2P. eFG. FT FTA FT. ORB DRB TRB AST STL BLK TOV PF PTS 1 1 Álex Abrines SG 25 OKC 31 2 588 56 157 .357 41 127 .323 15 30 .500 .487 12 13 .923 5 43 48 20 17 6 14 53 165 2 2 Quincy Acy PF 28 PHO 10 0 123 4 18 .222 2 15 .133 2 3 .667 .278 7 10 .700 3 22 25 8 1 4 4 24 17 3 3 Jaylen Adams PG 22 ATL 34 1 428 38 110 .345 25 74 .338 13 36 .361 .459 7 9 .778 11 49 60 65 14 5 28 45 108 4 4 Steven Adams C 25 OKC 80 80 2669 481 809 .595 0 2 .000 481 807 .596 .595 146 292 .500 391 369 760 124 117 76 135 204 1108 5 5 Bam Adebayo C 21 MIA 82 28 1913 280 486 .576 3 15 .200 277 471 .588 .579 166 226 .735 165 432 597 184 71 65 121 203 729 6 6 Deng Adel SF 21 CLE 19 3 194 11 36 .306 6 23 .261 5 13 .385 .389 4 4 1.000 3 16 19 5 1 4 6 13 32 7 7 DeVaughn Akoon-Purcell SG 25 DEN 7 0 22 3 10 .300 0 4 .000 3 6 .500 .300 1 2 .500 1 3 4 6 2 0 2 4 7 8 8 LaMarcus Aldridge C 33 SAS 81 81 2687 684 1319 .519 10 42 .238 674 1277 .528 .522 349 412 .847 251 493 744 194 43 107 144 179 1727 9 9 Rawle Alkins SG 21 CHI 10 1 120 13 39 .333 3 12 .250 10 27 .370 .372 8 12 .667 11 15 26 13 1 0 8 7 37 10 10 Grayson Allen SG 23 UTA 38 2 416 67 178 .376 32 99 .323 35 79 .443 .466 45 60 .750 3 20 23 25 6 6 33 47 211 We can use filtering even with variables just created. bball %&gt;% unite(&quot;posTeam&quot;, Pos, Tm) %&gt;% # create a new variable filter(posTeam == &quot;SG_GSW&quot;) %&gt;% # use it for filtering select(Player, posTeam, Age) %&gt;% # use it for selection arrange(desc(Age)) # descending order Player posTeam Age 1 Klay Thompson SG_GSW 28 2 Damion Lee SG_GSW 26 3 Jacob Evans SG_GSW 21 Being able to use a newly created variable on the fly, possibly only to filter or create some other variable, goes a long way toward easy visualization and generation of desired summary statistics. Generating New Data One of the most common data processing tasks is generating new variables. The function mutate takes a vector and returns one of the same dimension. In addition, there is mutate_at, mutate_if, and mutate_all to help with specific scenarios. To demonstrate, we’ll use mutate_at to make appropriate columns numeric, i.e. everything except Player, Pos, and Tm. It takes two inputs, variables and functions to apply. As there are multiple variables and (potentially) multiple functions, we use the vars and funs functions to denote them4. bball = bball %&gt;% mutate_at(vars(-Player, -Pos, -Tm), as.numeric) glimpse(bball[,1:7]) Observations: 708 Variables: 7 $ Rk &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 16, 16, 17, 18, 19, 20, 21, 22, 23, 23, 23, 24, 25, 26, 27, 28, 28, 28, 29, 30, 31, 32, 33, 33, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,… $ Player &lt;chr&gt; &quot;Álex Abrines&quot;, &quot;Quincy Acy&quot;, &quot;Jaylen Adams&quot;, &quot;Steven Adams&quot;, &quot;Bam Adebayo&quot;, &quot;Deng Adel&quot;, &quot;DeVaughn Akoon-Purcell&quot;, &quot;LaMarcus Aldridge&quot;, &quot;Rawle Alkins&quot;, &quot;Grayson Allen&quot;, &quot;Jarrett Allen&quot;, &quot;Kadeem Allen&quot;,… $ Pos &lt;chr&gt; &quot;SG&quot;, &quot;PF&quot;, &quot;PG&quot;, &quot;C&quot;, &quot;C&quot;, &quot;SF&quot;, &quot;SG&quot;, &quot;C&quot;, &quot;SG&quot;, &quot;SG&quot;, &quot;C&quot;, &quot;SG&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;C&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;PG&quot;, &quot;C&quot;, &quot;SG&quot;, &quot;PF&quot;, &quot;SG&quot;, &quot;SG&quot;, &quot;SG&quot;, &quot;PG&quot;… $ Age &lt;dbl&gt; 25, 28, 22, 25, 21, 21, 25, 33, 21, 23, 20, 26, 28, 25, 25, 30, 30, 30, 20, 24, 21, 34, 21, 24, 33, 33, 33, 31, 20, 23, 19, 25, 25, 25, 22, 21, 20, 34, 26, 26, 26, 28, 23, 30, 30, 32, 29, 25, 22, 30, 32… $ Tm &lt;chr&gt; &quot;OKC&quot;, &quot;PHO&quot;, &quot;ATL&quot;, &quot;OKC&quot;, &quot;MIA&quot;, &quot;CLE&quot;, &quot;DEN&quot;, &quot;SAS&quot;, &quot;CHI&quot;, &quot;UTA&quot;, &quot;BRK&quot;, &quot;NYK&quot;, &quot;POR&quot;, &quot;ATL&quot;, &quot;MEM&quot;, &quot;TOT&quot;, &quot;PHO&quot;, &quot;MIA&quot;, &quot;IND&quot;, &quot;MIL&quot;, &quot;DAL&quot;, &quot;HOU&quot;, &quot;TOR&quot;, &quot;CHI&quot;, &quot;TOT&quot;, &quot;PHO&quot;, &quot;WAS&quot;, &quot;ORL&quot;, &quot;PHO&quot;,… $ G &lt;dbl&gt; 31, 10, 34, 80, 82, 19, 7, 81, 10, 38, 80, 19, 81, 48, 43, 25, 15, 10, 3, 72, 2, 10, 67, 81, 69, 26, 43, 81, 71, 43, 62, 15, 11, 4, 16, 47, 47, 38, 77, 49, 28, 43, 30, 75, 34, 51, 67, 82, 81, 26, 79, 68… $ GS &lt;dbl&gt; 2, 0, 1, 80, 28, 3, 0, 81, 1, 2, 80, 1, 81, 4, 40, 8, 8, 0, 0, 72, 0, 2, 6, 32, 69, 26, 43, 81, 70, 13, 4, 0, 0, 0, 0, 45, 1, 0, 77, 49, 28, 38, 3, 72, 6, 18, 35, 82, 18, 2, 1, 3, 15, 27, 0, 12, 49, 1, … Now that the data columns are of the correct type, the following demonstrates how we can use the standard mutate function to create composites of existing variables. bball = bball %&gt;% mutate( trueShooting = PTS / (2 * (FGA + (.44 * FTA))), effectiveFG = (FG + (.5 * X3P)) / FGA, shootingDif = trueShooting - FG. ) summary(select(bball, shootingDif)) # select and others don&#39;t have to be piped to use shootingDif Min. :-0.08561 1st Qu.: 0.06722 Median : 0.09829 Mean : 0.09420 3rd Qu.: 0.12379 Max. : 0.53192 NA&#39;s :6 Grouping and Summarizing Data Another very common task is to look at group-based statistics, and we can use group_by and summarize to help us in this regard5. Base R has things like aggregate, by, and tapply for this, but they should not be used, as this approach is much more straightforward, flexible, and faster. For this demonstration, I’m going to start putting together several things we’ve demonstrated thus far. Ultimately we’ll create a variable called trueShooting, which represents ‘true shooting percentage’, and get an average for each position, and compare it to the average field goal percentage. bball %&gt;% select(Pos, FG, FGA, FG., FTA, X3P, PTS) %&gt;% mutate( trueShooting = PTS / (2 * (FGA + (.44 * FTA))), effectiveFG = (FG + (.5 * X3P)) / FGA, shootingDif = trueShooting - FG. ) %&gt;% group_by(Pos) %&gt;% summarize( `Mean FG%` = mean(FG., na.rm = TRUE), `Mean True Shooting` = mean(trueShooting, na.rm = TRUE) ) # A tibble: 11 x 3 Pos `Mean FG%` `Mean True Shooting` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 C 0.522 0.572 2 C-PF 0.407 0.530 3 PF 0.442 0.536 4 PF-C 0.356 0.492 5 PF-SF 0.419 0.544 6 PG 0.409 0.512 7 SF 0.425 0.529 8 SF-SG 0.431 0.558 9 SG 0.407 0.517 10 SG-PF 0.416 0.582 11 SG-SF 0.38 0.466 We can use do on grouped data to go further. This function can be used to create a new list-column in the data, the elements of which can be anything, even the results of an analysis for each group. As such, we can use tidyr’s unnest to get back to a standard data frame. To demonstrate, the following will group data by position, then get the correlation between field-goal percentage and free-throw shooting percentage. Some players are listed with multiple positions, so we will reduce those to whatever their first position is. On your own, I recommend just looking at it to the do line first, and compare to this result. bball %&gt;% mutate( Pos = case_when( Pos == &#39;PG-SG&#39; ~ &#39;PG&#39;, Pos == &#39;C-PF&#39; ~ &#39;C&#39;, Pos == &#39;SF-SG&#39; ~ &#39;SF&#39;, Pos == &#39;PF-C&#39; | Pos == &#39;PF-SF&#39; ~ &#39;PF&#39;, Pos == &#39;SG-PF&#39; | Pos == &#39;SG-SF&#39; ~ &#39;SG&#39;, TRUE ~ Pos )) %&gt;% group_by(Pos) %&gt;% do(FgFt_Corr = cor(.$FG., .$FT., use = &#39;complete&#39;)) %&gt;% unnest(FgFt_Corr) # A tibble: 5 x 2 Pos FgFt_Corr &lt;chr&gt; &lt;dbl&gt; 1 C -0.122 2 PF -0.0186 3 PG 0.0857 4 SF 0.00422 5 SG -0.0585 As a reminder, data frames are lists. As such, anything can go into the ‘columns’. library(nycflights13) carriers = group_by(flights, carrier) group_size(carriers) [1] 18460 32729 714 54635 48110 54173 685 3260 342 26397 32 58665 20536 5162 12275 601 mods = carriers %&gt;% do(model = lm(arr_delay ~ dep_time, data = .)) mods Source: local data frame [16 x 2] Groups: &lt;by row&gt; # A tibble: 16 x 2 carrier model * &lt;chr&gt; &lt;list&gt; 1 9E &lt;lm&gt; 2 AA &lt;lm&gt; 3 AS &lt;lm&gt; 4 B6 &lt;lm&gt; 5 DL &lt;lm&gt; 6 EV &lt;lm&gt; 7 F9 &lt;lm&gt; 8 FL &lt;lm&gt; 9 HA &lt;lm&gt; 10 MQ &lt;lm&gt; 11 OO &lt;lm&gt; 12 UA &lt;lm&gt; 13 US &lt;lm&gt; 14 VX &lt;lm&gt; 15 WN &lt;lm&gt; 16 YV &lt;lm&gt; mods %&gt;% summarize(carrier = carrier, `Adjusted Rsq` = summary(model)$adj.r.squared) %&gt;% head() # A tibble: 6 x 2 carrier `Adjusted Rsq` &lt;chr&gt; &lt;dbl&gt; 1 9E 0.0513 2 AA 0.0504 3 AS 0.0815 4 B6 0.0241 5 DL 0.0347 6 EV 0.0836 You can use group_by on more than one variable, e.g. group_by(var1, var2) Renaming columns Tibbles in the tidyverse don’t really have a problem with variable names starting with numbers or incorporating symbols and spaces. I would still suggest it is poor practice, because even if your data set looks fine, you’ll encounter problems with modeling and visualization package using that data. However, as a demonstration, we can ‘fix’ some of the variable names. One issue is that when we scraped the data and converted it to a data.frame, the names that started with a number, like 3P for ‘three point baskets made’, were made into X3P, because that’s the way R works by default. In addition, 3P%, i.e. three point percentage made, was made into 3P. Same goes for the 2P (two-pointers) and FT (free-throw) variables. We can use rename to change column names. A basic example is as follows. data %&gt;% rename(new_name = old_name, new_name2 = old_name2) Very straightforward. However, oftentimes we’ll need to change patterns, as with our current problem. The following uses str_replace and str_remove from stringr to look for a pattern in a name, and replace that pattern with some other pattern. It uses regular expressions for the patterns. bball %&gt;% rename_at(vars(contains(&#39;.&#39;)), str_replace, pattern=&#39;\\\\.&#39;, replacement=&#39;%&#39;) %&gt;% rename_at(vars(starts_with(&#39;X&#39;)), str_remove, pattern=&#39;X&#39;) %&gt;% colnames() [1] &quot;Rk&quot; &quot;Player&quot; &quot;Pos&quot; &quot;Age&quot; &quot;Tm&quot; &quot;G&quot; &quot;GS&quot; &quot;MP&quot; &quot;FG&quot; &quot;FGA&quot; &quot;FG%&quot; &quot;3P&quot; &quot;3PA&quot; &quot;3P%&quot; [15] &quot;2P&quot; &quot;2PA&quot; &quot;2P%&quot; &quot;eFG%&quot; &quot;FT&quot; &quot;FTA&quot; &quot;FT%&quot; &quot;ORB&quot; &quot;DRB&quot; &quot;TRB&quot; &quot;AST&quot; &quot;STL&quot; &quot;BLK&quot; &quot;TOV&quot; [29] &quot;PF&quot; &quot;PTS&quot; &quot;trueShooting&quot; &quot;effectiveFG&quot; &quot;shootingDif&quot; Merging Data Merging data is yet another very common data task, as data often comes from multiple sources. In order to do this, we need some common identifier among the sources by which to join them. The following is a list of dplyr join functions. inner_join: return all rows from x where there are matching values in y, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned. left_join: return all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned. right_join: return all rows from y, and all columns from x and y. Rows in y with no match in x will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned. semi_join: return all rows from x where there are matching values in y, keeping just columns from x. It differs from an inner join because an inner join will return one row of x for each matching row of y, where a semi join will never duplicate rows of x. anti_join: return all rows from x where there are not matching values in y, keeping just columns from x. full_join: return all rows and all columns from both x and y. Where there are not matching values, returns NA for the one missing. Probably the most common is a left join, where we have one primary data set, and are adding data from another source to it while retaining it as a base. The following is a simple demonstration. band_members # A tibble: 3 x 2 Name Band &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise 2 Francis Pixies 3 Bubba The New Year band_instruments # A tibble: 3 x 2 Name Instrument &lt;chr&gt; &lt;chr&gt; 1 Seth Synthesizer 2 Francis Guitar 3 Bubba Guitar left_join(band_members, band_instruments) Joining, by = &quot;Name&quot; # A tibble: 3 x 3 Name Band Instrument &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise Synthesizer 2 Francis Pixies Guitar 3 Bubba The New Year Guitar When we don’t have a one to one match, the result of the different types of join will become more apparent. band_members # A tibble: 4 x 2 Name Band &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise 2 Francis Pixies 3 Bubba The New Year 4 Stephen Pavement band_instruments # A tibble: 4 x 2 Name Instrument &lt;chr&gt; &lt;chr&gt; 1 Seth Synthesizer 2 Francis Guitar 3 Bubba Guitar 4 Steve Rage left_join(band_members, band_instruments) Joining, by = &quot;Name&quot; # A tibble: 4 x 3 Name Band Instrument &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise Synthesizer 2 Francis Pixies Guitar 3 Bubba The New Year Guitar 4 Stephen Pavement &lt;NA&gt; right_join(band_members, band_instruments) Joining, by = &quot;Name&quot; # A tibble: 4 x 3 Name Band Instrument &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise Synthesizer 2 Francis Pixies Guitar 3 Bubba The New Year Guitar 4 Steve &lt;NA&gt; Rage inner_join(band_members, band_instruments) Joining, by = &quot;Name&quot; # A tibble: 3 x 3 Name Band Instrument &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise Synthesizer 2 Francis Pixies Guitar 3 Bubba The New Year Guitar full_join(band_members, band_instruments) Joining, by = &quot;Name&quot; # A tibble: 5 x 3 Name Band Instrument &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise Synthesizer 2 Francis Pixies Guitar 3 Bubba The New Year Guitar 4 Stephen Pavement &lt;NA&gt; 5 Steve &lt;NA&gt; Rage anti_join(band_members, band_instruments) Joining, by = &quot;Name&quot; # A tibble: 1 x 2 Name Band &lt;chr&gt; &lt;chr&gt; 1 Stephen Pavement anti_join(band_instruments, band_members) Joining, by = &quot;Name&quot; # A tibble: 1 x 2 Name Instrument &lt;chr&gt; &lt;chr&gt; 1 Steve Rage Merges can get quite complex, and involve multiple data sources. In many cases you may have to do a lot of processing before getting to the merge, but dplyr’s joins will help quite a bit. tidyr The tidyr package can be thought of as a specialized subset of dplyr’s functionality, as well as an update to the previous reshape and reshape2 packages6. Some of its functions for manipulating data you’ll want to be familiar with are: pivot_longer: convert data from a wider format to longer one pivot_wider: convert data from a longer format to wider one unite: paste together multiple columns into one separate: complement of unite unnest: expand ‘list columns’ The following example shows how we take a ‘wide-form’ data set, where multiple columns represent different stock prices, and turn it into two columns, one representing stock name, and one for the price. We need to know which columns to work on, which is the first entry. It works very much like the select function, where you can use helpers etc. Then we need to give a name to the column(s) representing the indicators of what were multiple columns in the wide format. And finally we need to specify the column(s) of the values. library(tidyr) stocks &lt;- data.frame( time = as.Date(&#39;2009-01-01&#39;) + 0:9, X = rnorm(10, 0, 1), Y = rnorm(10, 0, 2), Z = rnorm(10, 0, 4) ) stocks %&gt;% head time X Y Z 1 2009-01-01 0.87248035 -1.990710 -4.910259 2 2009-01-02 -0.32845867 1.095290 4.001244 3 2009-01-03 0.04838128 2.454024 12.092072 4 2009-01-04 -0.86193397 -1.192655 -9.831471 5 2009-01-05 0.15616001 -1.402142 4.274242 6 2009-01-06 -2.07946115 2.471130 -6.205991 stocks %&gt;% pivot_longer( cols = -time, # works similar to using select() names_to = &#39;stock&#39;, # the name of the column that will have column names as labels values_to = &#39;price&#39; # the name of the column for the values ) %&gt;% head() # A tibble: 6 x 3 time stock price &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; 1 2009-01-01 X 0.872 2 2009-01-01 Y -1.99 3 2009-01-01 Z -4.91 4 2009-01-02 X -0.328 5 2009-01-02 Y 1.10 6 2009-01-02 Z 4.00 Here is a more complex example where we can handle multiple repeated entries. We additionally add another column for labeling, and posit the separator for the column names. library(tidyr) stocks &lt;- data.frame( time = as.Date(&#39;2009-01-01&#39;) + 0:9, X_1 = rnorm(10, 0, 1), X_2 = rnorm(10, 0, 1), Y_1 = rnorm(10, 0, 2), Y_2 = rnorm(10, 0, 2), Z_1 = rnorm(10, 0, 4), Z_2 = rnorm(10, 0, 4) ) head(stocks) time X_1 X_2 Y_1 Y_2 Z_1 Z_2 1 2009-01-01 -1.94806979 0.63707793 0.49301471 -2.6313047 -1.4641147 1.712415 2 2009-01-02 -1.44642323 -0.04766978 -0.05408821 0.3940757 1.1337330 3.386289 3 2009-01-03 0.01332318 1.86542434 -1.43236332 1.3381826 6.2941687 4.197391 4 2009-01-04 -0.31460453 0.39655805 0.64988914 -0.2861203 8.4378433 3.810982 5 2009-01-05 -0.29680101 -0.90635234 -1.55646194 3.4132894 -6.4112981 -3.208256 6 2009-01-06 -0.46222882 -2.29252838 -3.21387570 2.4857163 -0.9565964 -1.574207 stocks %&gt;% pivot_longer( cols = -time, names_to = c(&#39;stock&#39;, &#39;entry&#39;), names_sep = &#39;_&#39;, values_to = &#39;price&#39; ) %&gt;% head() # A tibble: 6 x 4 time stock entry price &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 2009-01-01 X 1 -1.95 2 2009-01-01 X 2 0.637 3 2009-01-01 Y 1 0.493 4 2009-01-01 Y 2 -2.63 5 2009-01-01 Z 1 -1.46 6 2009-01-01 Z 2 1.71 Note that the latter is an example of tidy data while the former is not. Why do we generally prefer such data? Precisely because the most common data operations, grouping, filtering, etc., would work notably more efficiently with such data. This is especially the case for visualization. The following demonstrates the separate function utilized for a very common data processing task- dealing with names. Here’ we’ll separate player into first and last names based on the space. bball %&gt;% separate(Player, into=c(&#39;first_name&#39;, &#39;last_name&#39;), sep=&#39; &#39;) %&gt;% select(1:5) %&gt;% head() Rk first_name last_name Pos Age 1 1 Álex Abrines SG 25 2 2 Quincy Acy PF 28 3 3 Jaylen Adams PG 22 4 4 Steven Adams C 25 5 5 Bam Adebayo C 21 6 6 Deng Adel SF 21 Note that this won’t necessarily apply to every name, so further processing may be required. More Tidyverse dplyr functions: There are over a hundred utility functions that perform very common tasks. You really need to be aware of them, as their use will come up often. broom: Convert statistical analysis objects from R into tidy data frames, so that they can more easily be combined, reshaped and otherwise processed with tools like dplyr, tidyr and ggplot2. tidy*: a lot of packages out there are now ‘tidy’, though not a part of the official tidyverse. Some examples of the ones I’ve used: tidycensus tidybayes tidytext modelr Seriously, there are a lot. Personal Opinion The dplyr grammar is clear for a lot of standard data processing tasks, and some not so common. Extremely useful for data exploration and visualization. No need to create/overwrite existing objects Can overwrite columns and use as they are created Makes it easy to look at anything, and do otherwise tedious data checks Drawbacks: Not as fast as data.table or even some base R approaches for many things7 The mindset can make for unnecessary complication e.g. There is no need to pipe to create a single new variable Some approaches, are not very intuitive Notably less ability to work with some very common data structures (e.g. matrices) All in all, if you’ve only been using base R approaches, the tidyverse will change your R life! It makes all the sorts of things you do all the time easier and clearer. Highly recommended! tidyverse Exercises Exercise 0 Install and load the dplyr ggplot2movies packages. Look at the help file for the movies data set, which contains data from IMDB. install.packages(&#39;ggplot2movies&#39;) library(ggplot2movies) data(&#39;movies&#39;) Exercise 1 Using the movies data set, perform each of the following actions separately. Exercise 1a Use mutate to create a centered version of the rating variable. A centered variable is one whose mean has been subtracted from it. The process will take the following form: data %&gt;% mutate(new_var_name = &#39;?&#39;) Exercise 1b Use filter to create a new data frame that has only movies from the years 2000 and beyond. Use the greater than or equal operator &gt;=. Exercise 1c Use select to create a new data frame that only has the title, year, budget, length, rating and votes variables. There are at least 3 ways to do this. Exercise 1d Rename the length column to length_in_min (i.e. length in minutes). Exercise 2 Use group_by to group the data by year, and summarize to create a new variable that is the average budget. The summarize function works just like mutate in this case. Use the mean function to get the average, but you’ll also need to use the argument na.rm = TRUE within it because the earliest years have no budget recorded. Exercise 3 Use pivot_longer to create a ‘tidy’ data set from the following. dat = tibble(id = 1:10, x = rnorm(10), y = rnorm(10)) Exercise 4 Now put several actions together in one set of piped operations. Filter movies released after 1990 select the same variables as before but also the mpaa, Action, and Drama variables group by mpaa and (your choice) Action or Drama get the average rating It should spit out something like the following: # A tibble: 10 x 3 # Groups: mpaa [5] mpaa Drama AvgRating &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 &quot;&quot; 0 5.94 2 &quot;&quot; 1 6.20 3 &quot;NC-17&quot; 0 4.28 4 &quot;NC-17&quot; 1 4.62 5 &quot;PG&quot; 0 5.19 6 &quot;PG&quot; 1 6.15 7 &quot;PG-13&quot; 0 5.44 8 &quot;PG-13&quot; 1 6.14 9 &quot;R&quot; 0 4.86 10 &quot;R&quot; 1 5.94 You may be thinking- ‘it’s 2020 and why on earth would anyone do that?!’. Peruse most sports websites and you’ll see that fundamental web design basics escape them. See also, financial sites.↩︎ If you’re following good programming practices, you’ll likely never use slice.↩︎ For more than one function, you could have supplied `funs(function1, function2, …).↩︎ As Hadley Wickham is from New Zealand, and his examples use summarise, you’ll probably see it about as much as you do the other spelling, especially since it will come up first in autocomplete.↩︎ Some still use reshape2 but there is no reason to and it is no longer developed.↩︎ There is multidplyr, dtplyr, and now tidyfast to help get as fast as possible for large data processing.↩︎ "],
["data_table.html", "data.table Basics Grouped operations Faster! Pipe with data.table Summary Faster dplyr alternatives data.table Exercises", " data.table Another package for data processing that has been useful to many is data.table. It works in a notably different way than dplyr. However, you’d use it for the same reasons, e.g. subset, grouping, update, ordered joins etc., but with key advantages in speed and memory efficiency. Like dplyr, the data objects are both data.frames and a package specific class. [1] &quot;data.table&quot; &quot;data.frame&quot; Basics In general, data.table works with brackets as in base R data frames. However, in order to use data.table effectively you’ll need to forget the data frame similarity. The brackets actually work like a function call, with several key arguments. Consider the following notation to start. Importantly: you don’t use the brackets as you would with data.frames. What i and j can be are fairly complex. In general, you use i for filtering by rows. x g y 1: 4 2 0.5316671 x g y 1: 4 2 0.5316671 You use j to select (by name!) or create new columns. We can define a new column with the := operator. [1] 3 4 9 2 1 6 [1] 3.314605 4.531667 9.986560 2.366484 1.724130 6.939924 g V1 1: 2 3.127899 2: 3 8.463242 x g y z 1: 3 1 0.3146050 3.314605 2: 4 2 0.5316671 4.531667 3: 9 3 0.9865603 9.986560 4: 2 1 0.3664836 2.366484 5: 1 2 0.7241302 1.724130 6: 6 3 0.9399237 6.939924 Because j is an argument, dropping columns is awkward. [1] -0.3146050 -0.5316671 -0.9865603 -0.3664836 -0.7241302 -0.9399237 x g z 1: 3 1 3.314605 2: 4 2 4.531667 3: 9 3 9.986560 4: 2 1 2.366484 5: 1 2 1.724130 6: 6 3 6.939924 Data table does not make unnecessary copies. For example if we do the following… DT2 and DT are just names for the same table. You’d actually need to use the copy function to make an explicit copy, otherwise whatever you do to DT2 will be done to DT. A B q 1: 5 e 1 2: 4 d 1 3: 3 c 1 4: 2 b 1 5: 1 a 1 A B 1: 5 e 2: 4 d 3: 3 c 4: 2 b 5: 1 a Grouped operations We can now attempt a ‘group-by’ operation, along with creation of a new variable. Note that these operations actually modify the dt object in place, a key distinction with dplyr. Fewer copies means less of a memory hit. g V1 1: 1 5.681089 2: 2 6.255797 3: 3 16.926484 x g y z mysum 1: 3 1 0.3146050 3.314605 5 2: 4 2 0.5316671 4.531667 5 3: 9 3 0.9865603 9.986560 15 4: 2 1 0.3664836 2.366484 5 5: 1 2 0.7241302 1.724130 5 6: 6 3 0.9399237 6.939924 15 We can also create groupings on the fly. For a new summary data set, we’ll take the following approach- we create a grouping based on whether g is a value of one or not, then get the mean and sum of x for those two categories. The corresponding dplyr approach is also shown (but not evaluated) for comparison. g mean_x sum_x 1: TRUE 2.5 5 2: FALSE 5.0 20 Faster! As mentioned, the reason to use data.table is speed. If you have large data or large operations it’ll be useful. Joins Joins can not only be faster but also easy to do. Note that the i argument can be a data.table object itself. I compare its speed to the comparable dplyr’s left_join function. func mean (microseconds) dt_join 528.26 dplyr_join 725.87 Group by We can use the setkey function to order a data set by a certain column(s). This ordering is done by reference; again, no copy is made. Doing this will allow for faster grouped operations, though you likely will only see the speed gain with very large data. The timing regards creating a new variable test_dt0 = data.table(x = rnorm(10000000), g = sample(letters, 10000000, replace = T)) test_dt1 = copy(test_dt0) test_dt2 = setkey(test_dt1, g) identical(test_dt0, test_dt1) [1] FALSE identical(test_dt1, test_dt2) [1] TRUE test_dt0 = test_dt0[, mean := mean(x), by = g] test_dt1 = test_dt1[, mean := mean(x), by = g] test_dt2 = test_dt2[, mean := mean(x), by = g] func mean (milliseconds) test_dt0 353.08 test_dt1 133.53 test_dt2 133.32 String matching The chin function returns a vector of the positions of (first) matches of its first argument in its second, where both arguments are character vectors. Essentially it’s just like the %in% function for character vectors. Consider the following. We sample the first 14 letters 1000 times with replacement and see which ones match in a subset of another subset of letters. I compare the same operation to stringr and the stringi package whose functionality stringr using. They are both far slower than chin. lets_1 = sample(letters[1:14], 1000, replace=T) lets_1 %chin% letters[13:26] %&gt;% head(10) [1] FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE # stri_detect_regex(lets_1, paste(letters[13:26], collapse=&#39;|&#39;)) Reading files If you use data.table for nothing else, you’d still want to consider it strongly for reading in large text files. The function fread may be quite useful in being memory efficient too. I compare it to readr. func mean (microseconds) dt 365.91 readr 2650.18 More speed The following demonstrates some timings from here. I reproduced it on my own machine based on 50 million observations. The grouped operations that are applied are just a sum and length on a vector. By the way, never, ever use aggregate. For anything. fun elapsed aggregate 56.857 by 18.118 dplyr 14.447 sapply 12.200 lapply 11.309 tapply 10.570 data.table 0.866 Ever. Really. Another thing to note is that the tidy approach is more about clarity and code efficiency relative to base R, as well as doing important background data checks and returning more usable results. In practice, it likely won’t be notably faster except in some cases, like with aggregate. Pipe with data.table Piping can be done with data.table objects too, using the brackets, but it’s awkward at best. Probably better to just use a standard pipe and dot approach if you really need it. Summary Faster and more memory-efficient methods are great to have. If you have large data this is one package that can help. For reading data Especially for group-by and joins. Drawbacks: Complex The syntax can be awkward It doesn’t work like a data.frame, which can be confusing Piping with brackets isn’t really feasible, and the dot approach is awkward Does not have its own ‘verse’, though many packages use it If speed and/or memory is (potentially) a concern, data.table. For interactive exploration, dplyr. Piping allows one to use both, so no need to choose. And on the horizon… Faster dplyr alternatives So we have data.table as a starting point for faster data processing operations, but there are others. The dtplyr package implements the data.table back-end for dplyr, so that you can seamlessly use them together. The newer package tidyfast works directly with a data.table object, but uses dplyr-esque functions. The following shows times for a counting unique arrival times in the nycflights13 flights data (336776 rows). package timing dplyr 10.580 dtplyr 4.575 data.table 3.519 tidyfast 3.507 a Median time in milliseconds to do a count of arr_time on nycflights::flights Just for giggles I did the same in Python with a pandas DataFrame, and it was notably slower than all of these options (almost 10x slower than standard dplyr). A lot of folks that use Python primarily think R is slow, but that is mostly because they don’t know how to effectively program with R for data science. data.table Exercises Exercise 0 Install and load the data.table package. Create the following data table. Exercise 1 Create a new object that contains only the ‘a’ group. Think back to how you use a logical to select rows. Exercise 2 Create a new object that is the sum of z grouped by x. You don’t need to name the sum variable. "],
["programming.html", "Basics R Objects Help Files Objects Exercises", " Basics Becoming a better programmer is in many ways like learning any language. While it may be literal, there is much nuance, and many ways are available to express yourself in order to solve some problem. However, it doesn’t take much in the way of practice to develop a few skills that will not only last, but go a long way toward saving you time and allowing you to explore your data, models, and visualizations more extensively. So let’s get to it! R Objects Object Inspection &amp; Exploration Let’s say you’ve imported your data into R. If you are going to be able to do anything with it, you’ll have had to create an R object that represents that data. What is that object? By now you know it’s a data frame, specifically, an object of class data.frame or possibly a tibble if you’re working within the tidyverse. If you want to look at it, you might be tempted to look at it this way with View, or clicking on it in your Environment viewer. View(diamonds) While this is certainly one way to inspect it, it’s not very useful. There’s far too much information to get much out of it, and information you may need to know is absent. Consider the following: str(diamonds) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 53940 obs. of 10 variables: $ carat : num 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... $ cut : Ord.factor w/ 5 levels &quot;Fair&quot;&lt;&quot;Good&quot;&lt;..: 5 4 2 4 2 3 3 3 1 3 ... $ color : Ord.factor w/ 7 levels &quot;D&quot;&lt;&quot;E&quot;&lt;&quot;F&quot;&lt;&quot;G&quot;&lt;..: 2 2 2 6 7 7 6 5 2 5 ... $ clarity: Ord.factor w/ 8 levels &quot;I1&quot;&lt;&quot;SI2&quot;&lt;&quot;SI1&quot;&lt;..: 2 3 5 4 2 6 7 3 4 5 ... $ depth : num 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... $ table : num 55 61 65 58 58 57 57 55 61 61 ... $ price : int 326 326 327 334 335 336 336 337 337 338 ... $ x : num 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... $ y : num 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... $ z : num 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... glimpse(diamonds) Observations: 53,940 Variables: 10 $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.30, 0.23, 0.22, 0.31, 0.20, 0.32, 0.30, 0.30, 0.30, 0.30, 0.30, 0.23, 0.23, 0.31, 0.31, 0.23, 0.24, 0.30, 0.23, 0.23, 0.23, 0.23, 0.23, 0.2… $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Very Good, Fair, Very Good, Good, Ideal, Premium, Ideal, Premium, Premium, Ideal, Good, Good, Very Good, Good, Very Good, Very Good, Very Good… $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I, E, H, J, J, G, I, J, D, F, F, F, E, E, D, F, E, H, D, I, I, J, D, D, H, F, H, H, E, H, F, G, I, E, D, I, J, I, I, I, I, D, D, D, I, G, I, … $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, SI1, SI2, SI2, I1, SI2, SI1, SI1, SI1, SI2, VS2, VS1, SI1, SI1, VVS2, VS1, VS2, VS2, VS1, VS1, VS1, VS1, VS1, VS1, VS1, VS1, SI1, VS2, SI2,… $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64.0, 62.8, 60.4, 62.2, 60.2, 60.9, 62.0, 63.4, 63.8, 62.7, 63.3, 63.8, 61.0, 59.4, 58.1, 60.4, 62.5, 62.2, 60.5, 60.9, 60.0, 59.8, 60.7, 59.… $ table &lt;dbl&gt; 55.0, 61.0, 65.0, 58.0, 58.0, 57.0, 57.0, 55.0, 61.0, 61.0, 55.0, 56.0, 61.0, 54.0, 62.0, 58.0, 54.0, 54.0, 56.0, 59.0, 56.0, 55.0, 57.0, 62.0, 62.0, 58.0, 57.0, 57.0, 61.0, 57.0, 57.0, 57.0, 59.0, 58.… $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 342, 344, 345, 345, 348, 351, 351, 351, 351, 352, 353, 353, 353, 354, 355, 357, 357, 357, 402, 402, 402, 402, 402, 402, 402, 402, 403, 403, 4… $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.25, 3.93, 3.88, 4.35, 3.79, 4.38, 4.31, 4.23, 4.23, 4.21, 4.26, 3.85, 3.94, 4.39, 4.44, 3.97, 3.97, 4.28, 3.96, 3.96, 4.00, 4.04, 3.97, 4.0… $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.28, 3.90, 3.84, 4.37, 3.75, 4.42, 4.34, 4.29, 4.26, 4.27, 4.30, 3.92, 3.96, 4.43, 4.47, 4.01, 3.94, 4.30, 3.97, 3.99, 4.03, 4.06, 4.01, 4.0… $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.73, 2.46, 2.33, 2.71, 2.27, 2.68, 2.68, 2.70, 2.71, 2.66, 2.71, 2.48, 2.41, 2.62, 2.59, 2.41, 2.47, 2.67, 2.40, 2.42, 2.41, 2.42, 2.42, 2.4… The str function looks at the structure of the object, while glimpse perhaps provides a possibly more readable version, and is just str specifically suited toward data frames. In both cases, we get info about the object and the various things within it. While you might be doing this with data frames, you should be doing it with any of the objects you’re interested in. Consider a regression model object. lm_mod = lm(mpg ~ ., data=mtcars) str(lm_mod, 0) List of 12 - attr(*, &quot;class&quot;)= chr &quot;lm&quot; str(lm_mod, 1) List of 12 $ coefficients : Named num [1:11] 12.3034 -0.1114 0.0133 -0.0215 0.7871 ... ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;(Intercept)&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... $ residuals : Named num [1:32] -1.6 -1.112 -3.451 0.163 1.007 ... ..- attr(*, &quot;names&quot;)= chr [1:32] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... $ effects : Named num [1:32] -113.65 -28.6 6.13 -3.06 -4.06 ... ..- attr(*, &quot;names&quot;)= chr [1:32] &quot;(Intercept)&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... $ rank : int 11 $ fitted.values: Named num [1:32] 22.6 22.1 26.3 21.2 17.7 ... ..- attr(*, &quot;names&quot;)= chr [1:32] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... $ assign : int [1:11] 0 1 2 3 4 5 6 7 8 9 ... $ qr :List of 5 ..- attr(*, &quot;class&quot;)= chr &quot;qr&quot; $ df.residual : int 21 $ xlevels : Named list() $ call : language lm(formula = mpg ~ ., data = mtcars) $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb .. ..- attr(*, &quot;variables&quot;)= language list(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) .. ..- attr(*, &quot;factors&quot;)= int [1:11, 1:10] 0 1 0 0 0 0 0 0 0 0 ... .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..- attr(*, &quot;term.labels&quot;)= chr [1:10] &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; ... .. ..- attr(*, &quot;order&quot;)= int [1:10] 1 1 1 1 1 1 1 1 1 1 .. ..- attr(*, &quot;intercept&quot;)= int 1 .. ..- attr(*, &quot;response&quot;)= int 1 .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; .. ..- attr(*, &quot;predvars&quot;)= language list(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:11] &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ... .. .. ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... $ model :&#39;data.frame&#39;: 32 obs. of 11 variables: ..- attr(*, &quot;terms&quot;)=Classes &#39;terms&#39;, &#39;formula&#39; language mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb .. .. ..- attr(*, &quot;variables&quot;)= language list(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) .. .. ..- attr(*, &quot;factors&quot;)= int [1:11, 1:10] 0 1 0 0 0 0 0 0 0 0 ... .. .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. .. ..- attr(*, &quot;term.labels&quot;)= chr [1:10] &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; ... .. .. ..- attr(*, &quot;order&quot;)= int [1:10] 1 1 1 1 1 1 1 1 1 1 .. .. ..- attr(*, &quot;intercept&quot;)= int 1 .. .. ..- attr(*, &quot;response&quot;)= int 1 .. .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; .. .. ..- attr(*, &quot;predvars&quot;)= language list(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) .. .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:11] &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ... .. .. .. ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... - attr(*, &quot;class&quot;)= chr &quot;lm&quot; Here we look at the object at the lowest level of detail (0), which basically just tells us that it’s a list of stuff. But if we go into more depth, we can see that there is quite a bit going on in here! Coefficients, the data frame used in the model (i.e. only the variables used and no NA), and much more are available to us, and we can pluck out any piece of it. lm_mod$coefficients (Intercept) cyl disp hp drat wt qsec vs am gear carb 12.30337416 -0.11144048 0.01333524 -0.02148212 0.78711097 -3.71530393 0.82104075 0.31776281 2.52022689 0.65541302 -0.19941925 lm_mod$model %&gt;% head() mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Let’s do a summary of it, something you’ve probably done many times. summary(lm_mod) Call: lm(formula = mpg ~ ., data = mtcars) Residuals: Min 1Q Median 3Q Max -3.4506 -1.6044 -0.1196 1.2193 4.6271 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 12.30337 18.71788 0.657 0.5181 cyl -0.11144 1.04502 -0.107 0.9161 disp 0.01334 0.01786 0.747 0.4635 hp -0.02148 0.02177 -0.987 0.3350 drat 0.78711 1.63537 0.481 0.6353 wt -3.71530 1.89441 -1.961 0.0633 . qsec 0.82104 0.73084 1.123 0.2739 vs 0.31776 2.10451 0.151 0.8814 am 2.52023 2.05665 1.225 0.2340 gear 0.65541 1.49326 0.439 0.6652 carb -0.19942 0.82875 -0.241 0.8122 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.65 on 21 degrees of freedom Multiple R-squared: 0.869, Adjusted R-squared: 0.8066 F-statistic: 13.93 on 10 and 21 DF, p-value: 3.793e-07 But you can assign that to an object and inspect it too! lm_mod_summary = summary(lm_mod) str(lm_mod_summary) List of 11 $ call : language lm(formula = mpg ~ ., data = mtcars) $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb .. ..- attr(*, &quot;variables&quot;)= language list(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) .. ..- attr(*, &quot;factors&quot;)= int [1:11, 1:10] 0 1 0 0 0 0 0 0 0 0 ... .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. .. .. ..$ : chr [1:11] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... .. .. .. ..$ : chr [1:10] &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; ... .. ..- attr(*, &quot;term.labels&quot;)= chr [1:10] &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; ... .. ..- attr(*, &quot;order&quot;)= int [1:10] 1 1 1 1 1 1 1 1 1 1 .. ..- attr(*, &quot;intercept&quot;)= int 1 .. ..- attr(*, &quot;response&quot;)= int 1 .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; .. ..- attr(*, &quot;predvars&quot;)= language list(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:11] &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ... .. .. ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... $ residuals : Named num [1:32] -1.6 -1.112 -3.451 0.163 1.007 ... ..- attr(*, &quot;names&quot;)= chr [1:32] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... $ coefficients : num [1:11, 1:4] 12.3034 -0.1114 0.0133 -0.0215 0.7871 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:11] &quot;(Intercept)&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Std. Error&quot; &quot;t value&quot; &quot;Pr(&gt;|t|)&quot; $ aliased : Named logi [1:11] FALSE FALSE FALSE FALSE FALSE FALSE ... ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;(Intercept)&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... $ sigma : num 2.65 $ df : int [1:3] 11 21 11 $ r.squared : num 0.869 $ adj.r.squared: num 0.807 $ fstatistic : Named num [1:3] 13.9 10 21 ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;value&quot; &quot;numdf&quot; &quot;dendf&quot; $ cov.unscaled : num [1:11, 1:11] 49.883532 -1.874242 -0.000841 -0.003789 -1.842635 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:11] &quot;(Intercept)&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... .. ..$ : chr [1:11] &quot;(Intercept)&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... - attr(*, &quot;class&quot;)= chr &quot;summary.lm&quot; If we pull the coefficients from this object, we are not just getting the values, but the table that’s printed in the summary. And we can now get that ready for publishing for example8. lm_mod_summary$coefficients %&gt;% kableExtra::kable(digits = 2) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 12.30 18.72 0.66 0.52 cyl -0.11 1.05 -0.11 0.92 disp 0.01 0.02 0.75 0.46 hp -0.02 0.02 -0.99 0.33 drat 0.79 1.64 0.48 0.64 wt -3.72 1.89 -1.96 0.06 qsec 0.82 0.73 1.12 0.27 vs 0.32 2.10 0.15 0.88 am 2.52 2.06 1.23 0.23 gear 0.66 1.49 0.44 0.67 carb -0.20 0.83 -0.24 0.81 After a while, you’ll know what’s in the objects you use most often, and that will allow you more easily work with the content they contain, allowing you to work with them more efficiently. Methods Consider the following: summary(diamonds) # data frame carat cut color clarity depth table price x y z Min. :0.2000 Fair : 1610 D: 6775 SI1 :13065 Min. :43.00 Min. :43.00 Min. : 326 Min. : 0.000 Min. : 0.000 Min. : 0.000 1st Qu.:0.4000 Good : 4906 E: 9797 VS2 :12258 1st Qu.:61.00 1st Qu.:56.00 1st Qu.: 950 1st Qu.: 4.710 1st Qu.: 4.720 1st Qu.: 2.910 Median :0.7000 Very Good:12082 F: 9542 SI2 : 9194 Median :61.80 Median :57.00 Median : 2401 Median : 5.700 Median : 5.710 Median : 3.530 Mean :0.7979 Premium :13791 G:11292 VS1 : 8171 Mean :61.75 Mean :57.46 Mean : 3933 Mean : 5.731 Mean : 5.735 Mean : 3.539 3rd Qu.:1.0400 Ideal :21551 H: 8304 VVS2 : 5066 3rd Qu.:62.50 3rd Qu.:59.00 3rd Qu.: 5324 3rd Qu.: 6.540 3rd Qu.: 6.540 3rd Qu.: 4.040 Max. :5.0100 I: 5422 VVS1 : 3655 Max. :79.00 Max. :95.00 Max. :18823 Max. :10.740 Max. :58.900 Max. :31.800 J: 2808 (Other): 2531 summary(diamonds$clarity) # vector I1 SI2 SI1 VS2 VS1 VVS2 VVS1 IF 741 9194 13065 12258 8171 5066 3655 1790 summary(lm_mod) # lm object Call: lm(formula = mpg ~ ., data = mtcars) Residuals: Min 1Q Median 3Q Max -3.4506 -1.6044 -0.1196 1.2193 4.6271 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 12.30337 18.71788 0.657 0.5181 cyl -0.11144 1.04502 -0.107 0.9161 disp 0.01334 0.01786 0.747 0.4635 hp -0.02148 0.02177 -0.987 0.3350 drat 0.78711 1.63537 0.481 0.6353 wt -3.71530 1.89441 -1.961 0.0633 . qsec 0.82104 0.73084 1.123 0.2739 vs 0.31776 2.10451 0.151 0.8814 am 2.52023 2.05665 1.225 0.2340 gear 0.65541 1.49326 0.439 0.6652 carb -0.19942 0.82875 -0.241 0.8122 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.65 on 21 degrees of freedom Multiple R-squared: 0.869, Adjusted R-squared: 0.8066 F-statistic: 13.93 on 10 and 21 DF, p-value: 3.793e-07 summary(lm_mod_summary) # lm summary object Length Class Mode call 3 -none- call terms 3 terms call residuals 32 -none- numeric coefficients 44 -none- numeric aliased 11 -none- logical sigma 1 -none- numeric df 3 -none- numeric r.squared 1 -none- numeric adj.r.squared 1 -none- numeric fstatistic 3 -none- numeric cov.unscaled 121 -none- numeric How is it that one function works on all these different types of objects? That’s not all. In RStudio, type summary. and hit the tab key. When you load additional packages, you’ll see even more methods for the summary function. When you call summary on an object, the appropriate type of summary method will be used depending on the class of the object. If there is no specific type, e.g. when we called summary on something that already had summary called on it, it will just use a default version listing the contents. To see all the methods for summary, type the following, and you’ll see all that is currently available for your R session. methods(&#39;summary&#39;) [1] summary,ANY-method summary,DBIObject-method summary,diagonalMatrix-method summary,sparseMatrix-method summary.aov [6] summary.aovlist* summary.aspell* summary.check_packages_in_dir* summary.connection summary.corAR1* [11] summary.corARMA* summary.corCAR1* summary.corCompSymm* summary.corExp* summary.corGaus* [16] summary.corIdent* summary.corLin* summary.corNatural* summary.corRatio* summary.corSpher* [21] summary.corStruct* summary.corSymm* summary.data.frame summary.Date summary.default [26] summary.Duration* summary.ecdf* summary.factor summary.gam summary.ggplot* [31] summary.glm summary.gls* summary.hcl_palettes* summary.infl* summary.Interval* [36] summary.lm summary.lme* summary.lmList* summary.loess* summary.manova [41] summary.matrix summary.microbenchmark* summary.mlm* summary.modelStruct* summary.nls* [46] summary.nlsList* summary.packageStatus* summary.pandas.core.frame.DataFrame* summary.pandas.core.series.Series* summary.pdBlocked* [51] summary.pdCompSymm* summary.pdDiag* summary.pdIdent* summary.pdIdnot* summary.pdLogChol* [56] summary.pdMat* summary.pdNatural* summary.pdSymm* summary.pdTens* summary.Period* [61] summary.POSIXct summary.POSIXlt summary.ppr* summary.prcomp* summary.princomp* [66] summary.proc_time summary.python.builtin.object* summary.reStruct* summary.rlang_error* summary.rlang_trace* [71] summary.shingle* summary.srcfile summary.srcref summary.stepfun summary.stl* [76] summary.table summary.trellis* summary.tukeysmooth* summary.varComb* summary.varConstPower* [81] summary.varExp* summary.varFixed* summary.varFunc* summary.varIdent* summary.varPower* [86] summary.vctrs_sclr* summary.vctrs_vctr* summary.warnings see &#39;?methods&#39; for accessing help and source code Say you are new to a modeling package, and as such, you might want to see what all you can do with the resulting object. Once you’ve discerned the class of the model object, you can then list all the functions that can be used on that object. library(brms) methods(class = &#39;brmsfit&#39;) [1] add_criterion add_ic as.array as.data.frame as.matrix as.mcmc autocor bayes_factor bayes_R2 [10] bridge_sampler coef conditional_effects conditional_smooths control_params expose_functions extract_draws family fitted [19] fixef formula getCall hypothesis kfold launch_shinystan log_lik log_posterior logLik [28] loo_compare loo_linpred loo_model_weights loo_predict loo_predictive_interval loo_R2 loo_subsample loo LOO [37] marginal_effects marginal_smooths mcmc_plot model_weights model.frame neff_ratio ngrps nobs nsamples [46] nuts_params pairs parnames plot_coefficients plot post_prob posterior_average posterior_interval posterior_linpred [55] posterior_predict posterior_samples posterior_summary pp_average pp_check pp_expect pp_mixture predict predictive_error [64] predictive_interval print prior_samples prior_summary ranef reloo residuals rhat stancode [73] standata stanplot summary update VarCorr vcov waic WAIC see &#39;?methods&#39; for accessing help and source code This allows you to more quickly get familiar with a package and the objects it produces, and provides utility you might not have even known to look for in the first place! S4 classes Everything we’ve been dealing with at this point are S3 objects, classes, and methods. R is a dialect of the S language, and the S3 name reflects the version of S at the time of R’s creation. S4 was the next iteration of S, but I’m not going to say much about the S4 system of objects other than they are a separate type of object with their own methods. For practical use you might not see much difference, but if you see an S4 object, it will have slots accessible via @. car_matrix = mtcars %&gt;% as.matrix() %&gt;% # convert from df to matrix Matrix::Matrix() # convert to Matrix class (S4) typeof(car_matrix) [1] &quot;S4&quot; str(car_matrix) Formal class &#39;dgeMatrix&#39; [package &quot;Matrix&quot;] with 4 slots ..@ x : num [1:352] 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ..@ Dim : int [1:2] 32 11 ..@ Dimnames:List of 2 .. ..$ : chr [1:32] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... .. ..$ : chr [1:11] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... ..@ factors : list() Usually you will access the contents via methods rather than using the @, and that assumes you know what those methods are. Mostly, I just find S4 objects slightly more annoying to work with for applied work, but you should be at least somewhat familiar with them so that you won’t be thrown off course when they appear. Others Indeed there are more types of R objects, but they will probably not be of much notice to the applied user. As an example, the text2vec package uses R6. I can only say that you’ll just have to cross that bridge should you get to it. Inspecting Functions You might not think of them as such, but in R, everything’s an object, including functions. You can inspect them like anything else. str(lm) function (formula, data, subset, weights, na.action, method = &quot;qr&quot;, model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, offset, ...) ## lm 1 function (formula, data, subset, weights, na.action, method = &quot;qr&quot;, 2 model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, 3 contrasts = NULL, offset, ...) 4 { 5 ret.x &lt;- x 6 ret.y &lt;- y 7 cl &lt;- match.call() 8 mf &lt;- match.call(expand.dots = FALSE) 9 m &lt;- match(c(&quot;formula&quot;, &quot;data&quot;, &quot;subset&quot;, &quot;weights&quot;, &quot;na.action&quot;, 10 &quot;offset&quot;), names(mf), 0L) 11 mf &lt;- mf[c(1L, m)] 12 mf$drop.unused.levels &lt;- TRUE 13 mf[[1L]] &lt;- quote(stats::model.frame) 14 mf &lt;- eval(mf, parent.frame()) 15 if (method == &quot;model.frame&quot;) 16 return(mf) 17 else if (method != &quot;qr&quot;) 18 warning(gettextf(&quot;method = &#39;%s&#39; is not supported. Using &#39;qr&#39;&quot;, 19 method), domain = NA) 20 mt &lt;- attr(mf, &quot;terms&quot;) One of the primary reasons for R’s popularity is the accessibility of the underlying code. People can very easily access the code for some function, modify it, extend it, etc. From an applied perspective, if you want to get better at writing code, or modify existing code, all you have to do is dive in! We’ll talk more about writing functions later. Help Files Many applied users of R are quick to search the web for available help when they come to a problem. This is great, you’ll find a lot of information out there. However, it will likely take you a bit to sort through things and find exactly what you need. Strangely, I see many users of R don’t use the help files first, and yet this is typically the quickest way to answer many of the questions they’ll have. Let’s start with an example. We’ll use the sample function to get a random sample of 10 values from the range of numbers 1 through 5. So, go ahead and do so! sample(?) Don’t know what to put? Consult the help file! We get a brief description of a function at the top, then we see how to actually use it, i.e. the form the syntax should take. We find out there is even an additional function, sample.int, that we could use. Next we see what arguments are possible. First we need an x, so what is the thing we’re trying to sample from? The numbers 1 through 5. Next is the size, which is how many values we want, in this case 10. So let’s try it. nums = 1:5 sample(nums, 10) Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when &#39;replace = FALSE&#39; Uh oh- we have a problem with the replace argument! We can see in the help file that, by default, it is FALSE9, but if we want to sample 10 times from only 5 numbers, we’ll need to change it to TRUE. Now we are on our way! The help file gives detailed information about the sampling that is possible, which actually is not as simple as one would think! The Value is important, as it tells us what we can expect the function to return, whether a data frame, list, or whatever. We even get references, other functions that might be of interest (See Also), and examples. There is a lot to digest for this function! Not all functions have all this information, but most do, and if they are adhering to standards they will10. However, all functions have this same documentation form, which puts R above and beyond most programming languages in this regard. Once you look at a couple of help files, you’ll always be able to quickly find the information you need from any other. Objects Exercises With one function, find out what the class, number of rows, number of columns are of the following object, including what kind of object the last three columns are. Inspect the help file also. library(dplyr) ?starwars An alternative would be to use the coef method, broom::tidy on the lm object itself, or use pander instead of kableExtra to work on the lm object.↩︎ For the things I do, I need sample with replacement far more than I don’t, but that may not be the same for everyone.↩︎ Ahem, tidyverse.↩︎ "],
["iterative.html", "Iterative Programming For loops Implicit Loops Looping with lists Iterative Programming Exercises", " Iterative Programming Almost everything you do when dealing with data will need to be done again, and again, and again. If you are copy-pasting your way to repetitively do the same thing, you’re not only doing things inefficiently, you’re almost certainly setting yourself up for trouble if anything changes about the data or underlying process. In order to avoid this, you need to be familiar with basic programming, and a starting point is to use an iterative approach to repetitive problems. Let’s look at the following. Let’s say we want to get the means of some columns in our data set. Do you do something like this? means1 = mean(df$x) means2 = mean(df$y) means3 = mean(df$z) means4 = mean(df$q) Now consider what you have to change if you change a variable name, decide to do a median, or the data object name changes. Any minor change with the data will cause you to have to redo that code, and possibly every line of it. For loops A for loop will help us get around the problem. The idea is that we want to perform a particular action for every iteration of some sequence. That sequence may be over columns, rows, lines in a text, whatever. Here is a loop. for (column in c(&#39;x&#39;,&#39;y&#39;,&#39;z&#39;,&#39;q&#39;)) { mean(df[[column]]) } What’s going on here? We’ve created an iterative process in which, for every element in c('x','y','z','q'), we are going to do something. We use the completely arbitrary word column as a placeholder to index which of the four columns we’re dealing with at a given point in the process. On the first iteration, column will equal x, on the second y, and so on. We then take the mean of df[[column]], which will be df[['x']], then df[['y']], etc. Here is an example with the nycflights data, which regards flights that departed New York City in 2013. The weather data set has columns for things like temperature, humidity, and so forth. weather = nycflights13::weather for (column in c(&#39;temp&#39;, &#39;humid&#39;, &#39;wind_speed&#39;, &#39;precip&#39;)) { print(mean(weather[[column]], na.rm = TRUE)) } [1] 55.26039 [1] 62.53006 [1] 10.51749 [1] 0.004469079 You can check this for yourself by testing a column or two directly with just mean(df$x). Now if the data name changes, the columns we want change, or we want to calculate something else, we usually end up only changing one thing, rather than at least changing one at a minimum, and probably many more things. In addition, the amount of code is the same whether the loop goes over 100 columns or 4. Let’s do things a little differently. columns = c(&#39;temp&#39;, &#39;humid&#39;, &#39;wind_speed&#39;, &#39;precip&#39;) nyc_means = rep(NA, length(columns)) for (i in seq_along(columns)) { column = columns[i] nyc_means[i] = mean(weather[[column]], na.rm = TRUE) # alternative without the initial first step # nyc_means[i] = mean(weather[[columns[i]]], na.rm = TRUE) } nyc_means [1] 55.260392127 62.530058972 10.517488384 0.004469079 By creating a columns object, if anything changes about the columns we want, that’s the only line in the code that would need to be changed. The i is now a place holder for a number that goes from 1 to the length of columns (i.e. 4). We make an empty nyc_means object that’s the length of the columns, so that each element will eventually be the mean of the corresponding column. In the following I remove precipitation and add visibility and air pressure. columns = c(&#39;temp&#39;, &#39;humid&#39;, &#39;wind_speed&#39;, &#39;visib&#39;, &#39;pressure&#39;) nyc_means = rep(NA, length(columns)) for (i in seq_along(columns)) { nyc_means[i] = mean(weather[[columns[i]]], na.rm = TRUE) } nyc_means %&gt;% round(2) [1] 55.26 62.53 10.52 9.26 1017.90 Had we been copy-pasting, this would require deleting or commenting out a line in our code, pasting two more, and changing each one after pasting to represent the new columns. That’s tedious, and not a fun way to code. A slight speed gain Note that you do not have to create an empty object like we did. The following works also. columns = c(&#39;temp&#39;, &#39;humid&#39;, &#39;wind_speed&#39;, &#39;visib&#39;, &#39;pressure&#39;) nyc_means = numeric() for (i in seq_along(columns)) { nyc_means[i] = mean(weather[[columns[i]]], na.rm = TRUE) } nyc_means %&gt;% round(2) [1] 55.26 62.53 10.52 9.26 1017.90 However, the other approach is slightly faster, because memory is already allocated for all elements of nyc_means, rather than updating it every iteration of the loop. This speed gain can become noticeable when dealing with thousands of columns and complex operations. While alternative When you look at some people’s R code, you may see a loop of a different sort. columns = c(&#39;temp&#39;,&#39;humid&#39;,&#39;wind_speed&#39;, &#39;visib&#39;, &#39;pressure&#39;) nyc_means = c() i = 1 while (i &lt;= length(columns)) { nyc_means[i] = mean(weather[[columns[i]]], na.rm = TRUE) i = i + 1 } nyc_means %&gt;% round(2) [1] 55.26 62.53 10.52 9.26 1017.90 This involves a while statement. It states, while i is less than or equal to the length (number) of columns, compute the value of the ith element of nyc_means as the mean of ith column of weather. After that, increase the value of i. So, we start with i = 1, compute that subsequent mean, i now equals 2, do the process again, and so on. The process will stop as soon as the value of i is greater than the length of columns. There is zero difference to using the while approach vs. the for loop. While is often used when there is a check to be made, e.g. in modeling functions that have to stop the estimation process at some point, or else they’d go on indefinitely. In that case the while syntax is probably more natural. Either is fine. Loops summary Understanding loops is fundamental toward spending less time processing data and more time toward exploring it. Your code will be more succinct and more able to handle the usual changes that come with dealing with data. Now that you have a sense of it, know that once you are armed with the sorts of things we’ll be talking about next- apply functions, writing functions, and vectorization - you’ll likely have little need to write explicit loops. While there is always a need for iterative processing of data, R provides even more efficient means to do so. Implicit Loops Writing loops is straightforward once you get the initial hang of it. However, R offers alternative ways to do loops that can simplify code without losing readability. As such, even when you loop in R, you don’t have to do so explicitly. apply family A family of functions comes with R that allows for a succinct way of looping when it is appropriate. Common functions in this family include: apply arrays, matrices, data.frames lapply, sapply, vapply lists, data.frames, vectors tapply grouped operations (table apply) mapply multivariate version of sapply replicate performs an operation N times As an example we’ll consider standardizing variables, i.e. taking a set of numbers, subtracting the mean, and dividing by the standard deviation. This results in a variable with mean of 0 and standard deviation of 1. Let’s start with a loop approach. for (i in 1:ncol(mydf)) { x = mydf[, i] for (j in 1:length(x)) { x[j] = (x[j] - mean(x)) / sd(x) } } The above would be a really bad way to use R. It goes over each column individually, then over each value of the column. Conversely, apply will take a matrix or data frame, and apply a function over the margin, row or column, you want to loop over. The first argument is the data you’re considering, the margin is the second argument (1 for rows, 2 for columns11), and the function you want to apply to those rows is the third argument. The following example is much cleaner compared to the loop, and now you’d have a function you can use elsewhere if needed. stdize &lt;- function(x) { (x - mean(x)) / sd(x) } apply(mydf, 2, stdize) # 1 for rows, 2 for columnwise application Many of the other apply functions work similarly, taking an object and a function to do the work on the object (possibly implicit), possibly with other arguments specified if necessary. lapply Let’s say we have a list object, or even just a vector of values. There are no rows or columns to iterate over, so what do we do here? x = list(&#39;aba&#39;, &#39;abb&#39;, &#39;abc&#39;, &#39;abd&#39;, &#39;abe&#39;) lapply(x, str_remove, pattern = &#39;ab&#39;) [[1]] [1] &quot;a&quot; [[2]] [1] &quot;b&quot; [[3]] [1] &quot;c&quot; [[4]] [1] &quot;d&quot; [[5]] [1] &quot;e&quot; The lapply operation iterates over each element of the list and applies a function to them. In this case, the function is str_remove. It has an argument for the string pattern we want to take out of the character string that is fed to it (‘ab’). For example, for ‘aba’ we will be left with just the ‘a’. As can be seen, lapply starts with a list and returns a list. The only difference with sapply is that sapply will return a simplified form if possible12. sapply(x, str_remove, pattern = &#39;ab&#39;) [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; In this case we just get a vector back. Apply functions It is important to be familiar with the apply family for efficient data processing, if only because you’ll regularly come code employing these functions. A summary of benefits includes: Cleaner/simpler code Environment kept clear of unnecessary objects Potentially more reproducible more likely to use generalizable functions Parallelizable Note that apply functions are NOT necessarily faster than explicit loops, and if you create an empty object for the loop as discussed previously, the explicit loop will likely be faster. On top of that, functions like replicate and mapply are especially slow. However, the apply family can ALWAYS potentially be faster than standard R loops do to parallelization. With base R’s parallel package, there are parallel versions of the apply family, e.g.parApply, parLapply etc. As every modern computer has at least four cores to play with, you’ll always potentially have nearly a 4x speedup by using the parallel apply functions. Apply functions and similar approaches should be a part of your regular R experience. We’ll talk about other options that may have even more benefits, but you need to know the basics of how apply functions work in order to use those. I use R every day, and very rarely use explicit loops. Note that there is no speed difference for a for loop vs. using while. And if you must use an explicit loop, create an empty object of the dimension/form you need, and then fill it in via the loop. This will be notably faster. I pretty much never use an explicit double loop, as a little more thinking about the problem will usually provide a more efficient path to solving the problem. purrr The purrr package allows you to take the apply family approach to the tidyverse. And with packages future + furrr, they too are parallelizable. Consider the following. We’ll use the map function to map the sum function to each element in the list, the same way we would with lapply. x = list(1:3, 4:6, 7:9) map(x, sum) [[1]] [1] 6 [[2]] [1] 15 [[3]] [1] 24 The map functions take some getting used to, and in my experience they are typically slower than the apply functions, sometimes notably so. However they allow you stay within the tidy realm, which has its own benefits, and have more control over the nature of the output13, which is especially important in reproducibility, package development, producing production-level code, etc. The key idea is that the map functions will always return something the same length as the input given to it. The purrr functions want a list or vector, i.e. they don’t work with data.frame objects in the same way we’ve done with mutate and summarize except in the sense that data.frames are lists. ## mtcars %&gt;% ## map(scale) # returns a list, not shown mtcars %&gt;% map_df(scale) # returns a df # A tibble: 32 x 11 mpg cyl disp hp drat wt qsec vs am gear carb &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.151 -0.105 -0.571 -0.535 0.568 -0.610 -0.777 -0.868 1.19 0.424 0.735 2 0.151 -0.105 -0.571 -0.535 0.568 -0.350 -0.464 -0.868 1.19 0.424 0.735 3 0.450 -1.22 -0.990 -0.783 0.474 -0.917 0.426 1.12 1.19 0.424 -1.12 4 0.217 -0.105 0.220 -0.535 -0.966 -0.00230 0.890 1.12 -0.814 -0.932 -1.12 5 -0.231 1.01 1.04 0.413 -0.835 0.228 -0.464 -0.868 -0.814 -0.932 -0.503 6 -0.330 -0.105 -0.0462 -0.608 -1.56 0.248 1.33 1.12 -0.814 -0.932 -1.12 7 -0.961 1.01 1.04 1.43 -0.723 0.361 -1.12 -0.868 -0.814 -0.932 0.735 8 0.715 -1.22 -0.678 -1.24 0.175 -0.0278 1.20 1.12 -0.814 0.424 -0.503 9 0.450 -1.22 -0.726 -0.754 0.605 -0.0687 2.83 1.12 -0.814 0.424 -0.503 10 -0.148 -0.105 -0.509 -0.345 0.605 0.228 0.253 1.12 -0.814 0.424 0.735 # … with 22 more rows mtcars %&gt;% map_dbl(sum) # returns a numeric (double) vector of column sums mpg cyl disp hp drat wt qsec vs am gear carb 642.900 198.000 7383.100 4694.000 115.090 102.952 571.160 14.000 13.000 118.000 90.000 diamonds %&gt;% map_at( vars(carat, depth, price), function(x) as.integer(x &gt; median(x)) ) %&gt;% as_tibble() # A tibble: 53,940 x 10 carat cut color clarity depth table price x y z &lt;int&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 Ideal E SI2 0 55 0 3.95 3.98 2.43 2 0 Premium E SI1 0 61 0 3.89 3.84 2.31 3 0 Good E VS1 0 65 0 4.05 4.07 2.31 4 0 Premium I VS2 1 58 0 4.2 4.23 2.63 5 0 Good J SI2 1 58 0 4.34 4.35 2.75 6 0 Very Good J VVS2 1 57 0 3.94 3.96 2.48 7 0 Very Good I VVS1 1 57 0 3.95 3.98 2.47 8 0 Very Good H SI1 1 55 0 4.07 4.11 2.53 9 0 Fair E VS2 1 61 0 3.87 3.78 2.49 10 0 Very Good H VS1 0 61 0 4 4.05 2.39 # … with 53,930 more rows However, working with lists is very useful, so let’s turn to that. Looping with lists Aside from data frames, you may think you don’t have much need for list objects. However, list objects make it very easy to iterate some form of data processing. Let’s say you have models of increasing complexity, and you want to easily summarise and/or compare them. We create a list for which each element is a model object. We then apply a function, e.g. to get the AIC value for each, or adjusted R square (this requires a custom function). library(mgcv) # for gam mtcars$cyl = factor(mtcars$cyl) mod_lm = lm(mpg ~ wt, data = mtcars) mod_poly = lm(mpg ~ poly(wt, 2), data = mtcars) mod_inter = lm(mpg ~ wt * cyl, data = mtcars) mod_gam = gam(mpg ~ s(wt), data = mtcars) mod_gam_inter = gam(mpg ~ cyl + s(wt, by = cyl), data = mtcars) model_list = list( mod_lm = mod_lm, mod_poly = mod_poly, mod_inter = mod_inter, mod_gam = mod_gam, mod_gam_inter = mod_gam_inter ) # lowest wins model_list %&gt;% map_dbl(AIC) %&gt;% sort() mod_gam_inter mod_inter mod_poly mod_gam mod_lm 150.6324 155.4811 158.0484 158.5717 166.0294 # highest wins model_list %&gt;% map_dbl( function(x) if_else(inherits(x, &#39;gam&#39;), summary(x)$r.sq, summary(x)$adj) ) %&gt;% sort(decreasing = TRUE) mod_gam_inter mod_inter mod_poly mod_gam mod_lm 0.8643020 0.8349382 0.8065828 0.8041651 0.7445939 Let’s go further and create a plot of these results. We’ll map to a data frame, use pivot_longer to melt it to two columns of model and value, then use ggplot2 to plot the results14. model_list %&gt;% map_df( function(x) if_else(inherits(x, &#39;gam&#39;), summary(x)$r.sq, summary(x)$adj) ) %&gt;% pivot_longer(cols = starts_with(&#39;mod&#39;), names_to = &#39;model&#39;, values_to = &quot;Adj. Rsq&quot;) %&gt;% arrange(desc(`Adj. Rsq`)) %&gt;% mutate(model = factor(model, levels = model)) %&gt;% # sigh ggplot(aes(x = model, y = `Adj. Rsq`)) + geom_point(aes(color = model), size = 10, show.legend = F) Why not throw in AIC also? mod_rsq = model_list %&gt;% map_df( function(x) if_else( inherits(x, &#39;gam&#39;), summary(x)$r.sq, summary(x)$adj ) ) %&gt;% pivot_longer(cols = starts_with(&#39;mod&#39;), names_to = &#39;model&#39;, values_to = &#39;Rsq&#39;) mod_aic = model_list %&gt;% map_df(AIC) %&gt;% pivot_longer(cols = starts_with(&#39;mod&#39;), names_to = &#39;model&#39;, values_to = &#39;AIC&#39;) left_join(mod_rsq, mod_aic) %&gt;% arrange(AIC) %&gt;% mutate(model = factor(model, levels = model)) %&gt;% pivot_longer(cols = -model, names_to = &#39;measure&#39;, values_to = &#39;value&#39;) %&gt;% ggplot(aes(x = model, y = value)) + geom_point(aes(color = model), size = 10, show.legend = F) + facet_wrap(~ measure, scales = &#39;free&#39;) List columns As data.frames are lists, anything can be put into a column just as you would a list element. We’ll use pmap here, as it can take more than one argument, and we’re feeding all columns of the data.frame. You don’t need to worry about the details here, we just want to create a column that is actually a list. In this case the column will contain a data frame in each entry. mtcars2 = as.matrix(mtcars) mtcars2[sample(1:length(mtcars2), 50)] = NA # add some missing data mtcars2 = data.frame(mtcars2) %&gt;% rownames_to_column(var = &#39;observation&#39;) %&gt;% as_tibble() head(mtcars2) # A tibble: 6 x 12 observation mpg cyl disp hp drat wt qsec vs am gear carb &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 Mazda RX4 21.0 &lt;NA&gt; 160.0 110 &lt;NA&gt; 2.620 16.46 0 1 4 4 2 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 &lt;NA&gt; 3 Datsun 710 22.8 4 108.0 &lt;NA&gt; 3.85 2.320 18.61 1 1 4 1 4 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 5 Hornet Sportabout &lt;NA&gt; 8 360.0 175 3.15 3.440 17.02 0 0 3 2 6 Valiant 18.1 6 225.0 &lt;NA&gt; 2.76 &lt;NA&gt; 20.22 1 0 &lt;NA&gt; 1 mtcars2 = mtcars2 %&gt;% mutate( newvar = pmap(., ~ data.frame( N = sum(!is.na(c(...))), Missing = sum(is.na(c(...))) ) ) ) Now check out the list column. mtcars2 # A tibble: 32 x 13 observation mpg cyl disp hp drat wt qsec vs am gear carb newvar &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; 1 Mazda RX4 21.0 &lt;NA&gt; 160.0 &quot;110&quot; &lt;NA&gt; 2.620 16.46 0 1 4 4 &lt;df[,2] [1 × 2]&gt; 2 Mazda RX4 Wag 21.0 6 160.0 &quot;110&quot; 3.90 2.875 17.02 0 1 4 &lt;NA&gt; &lt;df[,2] [1 × 2]&gt; 3 Datsun 710 22.8 4 108.0 &lt;NA&gt; 3.85 2.320 18.61 1 1 4 1 &lt;df[,2] [1 × 2]&gt; 4 Hornet 4 Drive 21.4 6 258.0 &quot;110&quot; 3.08 3.215 19.44 1 0 3 1 &lt;df[,2] [1 × 2]&gt; 5 Hornet Sportabout &lt;NA&gt; 8 360.0 &quot;175&quot; 3.15 3.440 17.02 0 0 3 2 &lt;df[,2] [1 × 2]&gt; 6 Valiant 18.1 6 225.0 &lt;NA&gt; 2.76 &lt;NA&gt; 20.22 1 0 &lt;NA&gt; 1 &lt;df[,2] [1 × 2]&gt; 7 Duster 360 14.3 8 360.0 &quot;245&quot; 3.21 3.570 15.84 0 0 3 &lt;NA&gt; &lt;df[,2] [1 × 2]&gt; 8 Merc 240D 24.4 4 &lt;NA&gt; &quot; 62&quot; 3.69 3.190 20.00 &lt;NA&gt; &lt;NA&gt; 4 2 &lt;df[,2] [1 × 2]&gt; 9 Merc 230 &lt;NA&gt; 4 140.8 &quot; 95&quot; 3.92 &lt;NA&gt; 22.90 &lt;NA&gt; 0 4 2 &lt;df[,2] [1 × 2]&gt; 10 Merc 280 19.2 6 167.6 &quot;123&quot; 3.92 3.440 18.30 &lt;NA&gt; 0 4 4 &lt;df[,2] [1 × 2]&gt; # … with 22 more rows mtcars2$newvar %&gt;% head(3) [[1]] N Missing 1 10 2 [[2]] N Missing 1 11 1 [[3]] N Missing 1 11 1 Unnest it with the tidyr function. mtcars2 %&gt;% unnest(newvar) # A tibble: 32 x 14 observation mpg cyl disp hp drat wt qsec vs am gear carb N Missing &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; 1 Mazda RX4 21.0 &lt;NA&gt; 160.0 &quot;110&quot; &lt;NA&gt; 2.620 16.46 0 1 4 4 10 2 2 Mazda RX4 Wag 21.0 6 160.0 &quot;110&quot; 3.90 2.875 17.02 0 1 4 &lt;NA&gt; 11 1 3 Datsun 710 22.8 4 108.0 &lt;NA&gt; 3.85 2.320 18.61 1 1 4 1 11 1 4 Hornet 4 Drive 21.4 6 258.0 &quot;110&quot; 3.08 3.215 19.44 1 0 3 1 12 0 5 Hornet Sportabout &lt;NA&gt; 8 360.0 &quot;175&quot; 3.15 3.440 17.02 0 0 3 2 11 1 6 Valiant 18.1 6 225.0 &lt;NA&gt; 2.76 &lt;NA&gt; 20.22 1 0 &lt;NA&gt; 1 9 3 7 Duster 360 14.3 8 360.0 &quot;245&quot; 3.21 3.570 15.84 0 0 3 &lt;NA&gt; 11 1 8 Merc 240D 24.4 4 &lt;NA&gt; &quot; 62&quot; 3.69 3.190 20.00 &lt;NA&gt; &lt;NA&gt; 4 2 9 3 9 Merc 230 &lt;NA&gt; 4 140.8 &quot; 95&quot; 3.92 &lt;NA&gt; 22.90 &lt;NA&gt; 0 4 2 9 3 10 Merc 280 19.2 6 167.6 &quot;123&quot; 3.92 3.440 18.30 &lt;NA&gt; 0 4 4 11 1 # … with 22 more rows This is a pretty esoteric demonstration, and not something you’d normally want to do, as mutate or other approaches would be far more efficient and sensical. However, the idea is that you might want to retain the information you might otherwise store in a list with the data that was used to create it. As an example, you could potentially attach models as a list column to a dataframe that contains meta-information about each model. Once you have a list column, you can use that column as you would any list for iterative programming. Iterative Programming Exercises Exercise 1 With the following matrix, use apply and the sum function to get row or column sums of the matrix x. x = matrix(1:9, 3, 3) Exercise 2 With the following list object x, use lapply and sapply and the sum function to get sums for the elements. There is no margin to specify for a list, so just supply the list and the sum function. x = list(1:3, 4:10, 11:100) Exercise 3 As in the previous example, use a map function to create a data frame of the column means. See ?map to see all your options. d = tibble( x = rnorm(100), y = rnorm(100, 10, 2), z = rnorm(100, 50, 10), ) You can have 3 or more dimensional arrays, and apply will work over those dimensions (or any combination of them), but this doesn’t come up too often.↩︎ sapply is actually just a wrapper for lapply. If you supply the argument simplified=F, it is identical. Otherwise, it attempts to return a vector or matrix.↩︎ Personally I find the help file description nebulous or completely lacking, and the examples unrealistic, and oftentimes I can come up with a group_by-summarise approach before I can sort out why the map approach didn’t work. The main thing to note is that both the benefit and difficulty of using the map_* functions is they are more strict with inputs, which typically should be lists or list-like objects like data frames, and outputs.↩︎ Note that ggplot2 will change the ordering of the variable unless you coerce it by creating a factor. This pretty much defeats the entire purpose of retaining characters over factors everywhere else in the tidyverse.↩︎ "],
["functions.html", "Writing Functions A starting point DRY Conditionals Anonymous functions Writing Functions Exercises", " Writing Functions You can’t do anything in R without using functions, but have you ever written your own? Why would you? Efficiency Customized functionality Reproducibility Extend the work that’s already been done There are many benefits to writing your own functions, and it’s actually easy to do. Once you get the basic concept down, you’ll likely find yourself using your own functions more and more. A starting point Let’s assume you want to calculate the mean, standard deviation, and number of missing values for a variable, called myvar. We could do something like the following mean(myvar) sd(myvar) sum(is.na(myvar)) Now let’s say you need to do it for several variables. Here’s what your custom function could look like. It takes a single input, the variable you want information about, and returns a data frame with that info. my_summary &lt;- function(x) { data.frame( mean = mean(x), sd = sd(x), N_missing = sum(is.na(x)) ) } In the above, x is an arbitrary name for an input. You can name it whatever you want, but the more meaningful the better. In R (and other languages) these are called arguments, but these inputs will determine in part what is eventually produced as output by the function. my_summary(mtcars$mpg) mean sd N_missing 1 20.09062 6.026948 0 Works fine. However, data typically isn’t that pretty. It often has missing values. load(&#39;data/gapminder.RData&#39;) my_summary(gapminder_2019$lifeExp) mean sd N_missing 1 NA NA 516 If there are actually missing values, we need to set na.rm = TRUE or the mean and sd will return NA. Let’s try it. We can either hard bake it in, as in the initial example, or add an argument to let us control how to handle NAs with our custom function. my_summary &lt;- function(x) { data.frame( mean = mean(x, na.rm = TRUE), sd = sd(x, na.rm = TRUE), N_missing = sum(is.na(x)) ) } my_summary_na &lt;- function(x, remove_na = TRUE) { data.frame( mean = mean(x, na.rm = remove_na), sd = sd(x, na.rm = remove_na), N_missing = sum(is.na(x)) ) } my_summary(gapminder_2019$lifeExp) mean sd N_missing 1 43.13218 16.31355 516 my_summary_na(gapminder_2019$lifeExp, remove_na = FALSE) mean sd N_missing 1 NA NA 516 Seems to work fine. Let’s add how many total observations there are. my_summary &lt;- function(x) { # create an arbitrarily named object with the summary information summary_data = data.frame( mean = mean(x, na.rm = TRUE), sd = sd(x, na.rm = TRUE), N_total = length(x), N_missing = sum(is.na(x)) ) # return the result! summary_data } That was easy! Let’s try it. my_summary(gapminder_2019$lifeExp) mean sd N_total N_missing 1 43.13218 16.31355 40953 516 Now let’s do it for every column! We’ve used the map function before, now let’s use a variant that will return a data frame. gapminder_2019 %&gt;% select_if(is.numeric) %&gt;% map_dfr(my_summary, .id = &#39;variable&#39;) variable mean sd N_total N_missing 1 year 1.909000e+03 6.321997e+01 40953 0 2 lifeExp 4.313218e+01 1.631355e+01 40953 516 3 pop 1.353928e+07 6.565653e+07 40953 0 4 gdpPercap 4.591026e+03 1.016210e+04 40953 0 5 giniPercap 4.005331e+01 9.102757e+00 40953 0 The map_dfr function is just like our previous usage in the iterative programming section, just that it will create mini-data.frames then row-bind them together. This shows that writing the first part of any function can be straightforward. Then, once in place, you can usually add functionality without too much trouble. Eventually you could have something very complicated, but which will make sense to you because you built it from the ground up. Keep in mind as you start out that your initial decisions to make are: What are the inputs (arguments) to the function? What is the value to be returned? When you think about writing a function, just write the code that can do it first. The goal is then to generalize beyond that single use case. RStudio even has a shortcut to help you get started. Consider our starting point. Highlight the code, hit Ctrl/Cmd + Shft + X, then give it a name. mean(myvar) sd(myvar) sum(is.na(myvar)) It should look something like this. test_fun &lt;- function(myvar) { mean(myvar) sd(myvar) sum(is.na(myvar)) } RStudio could tell that you would need at least one input myvar, but beyond that, you’re now on your way to tweaking the function as you see fit. Note that what goes in and what comes out could be anything, even nothing! two &lt;- function() { 2 } two() [1] 2 Or even another function! center &lt;- function(type) { if (type == &#39;mean&#39;) { mean } else { median } } center(type = &#39;mean&#39;) function (x, ...) UseMethod(&quot;mean&quot;) &lt;bytecode: 0x7ff1f655ac48&gt; &lt;environment: namespace:base&gt; myfun = center(type = &#39;mean&#39;) myfun(1:5) [1] 3 myfun = center(type = &#39;median&#39;) myfun(1:4) [1] 2.5 We can also set default values for the inputs. hi &lt;- function(name = &#39;Beyoncé&#39;) { paste0(&#39;Hi &#39;, name, &#39;!&#39;) } hi() [1] &quot;Hi Beyoncé!&quot; hi(name = &#39;Jay-Z&#39;) [1] &quot;Hi Jay-Z!&quot; If you are working within an RStudio project, it would be a good idea to create a folder for your functions and save each as their own script. When you need the function just use the following: source(&#39;my_functions/awesome_func.R&#39;) This would make it easy to even create your own personal package with the functions you create. However you go about creating a function and for whatever purpose, try to make a clear decision at the beginning What is the (specific) goal of your function? What is the minimum needed to obtain that goal? There is even a keyboard shortcut to create R style documentation automatically! Cmd/Ctrl + Option/Alt + Shift + R DRY An oft-quoted mantra in programming is Don’t Repeat Yourself. One context regards iterative programming, where we would rather write one line of code than one-hundred. More generally though, we would like to gain efficiency where possible. A good rule of thumb is, if you are writing the same set of code more than twice, you should write a function to do it instead. Consider the following example, where we want to subset the data given a set of conditions. Given the cylinder, engine displacement, and mileage, we’ll get different parts of the data. good_mileage_displ_low_cyl_4 = if_else(cyl == 4 &amp; displ &lt; mean(displ) &amp; hwy &gt; 30, &#39;yes&#39;, &#39;no&#39;) good_mileage_displ_low_cyl_6 = if_else(cyl == 6 &amp; displ &lt; mean(displ) &amp; hwy &gt; 30, &#39;yes&#39;, &#39;no&#39;) good_mileage_displ_low_cyl_8 = if_else(cyl == 8 &amp; displ &lt; mean(displ) &amp; hwy &gt; 30, &#39;yes&#39;, &#39;no&#39;) good_mileage_displ_high_cyl_4 = if_else(cyl == 4 &amp; displ &gt; mean(displ) &amp; hwy &gt; 30, &#39;yes&#39;, &#39;no&#39;) good_mileage_displ_high_cyl_6 = if_else(cyl == 6 &amp; displ &gt; mean(displ) &amp; hwy &gt; 30, &#39;yes&#39;, &#39;no&#39;) good_mileage_displ_high_cyl_8 = if_else(cyl == 8 &amp; displ &gt; mean(displ) &amp; hwy &gt; 30, &#39;yes&#39;, &#39;no&#39;) It was tedious, but that’s not much code. But now consider- what if you want to change the mpg cutoff? The mean to median? Something else? You have to change all of it. Screw that- let’s write a function instead! What kinds of inputs will we need? cyl: Which cylinder type we want mpg_cutoff: The cutoff for ‘good’ mileage displ_fun: Whether the displacement to be based on the mean or something else displ_low: Whether we are interested in low or high displacement vehicles cls: the class of the vehicle (e.g. compact or suv) good_mileage &lt;- function( cylinder = 4, mpg_cutoff = 30, displ_fun = mean, displ_low = TRUE, cls = &#39;compact&#39; ) { if (displ_low == TRUE) { # condition to check, if it holds, result &lt;- mpg %&gt;% # filter data given the arguments filter(cyl == cylinder, displ &lt;= displ_fun(displ), hwy &gt;= mpg_cutoff, class == cls) } else { # if the condition doesn&#39;t hold, filter result &lt;- mpg %&gt;% # the data this way instead filter( cyl == cylinder, displ &gt;= displ_fun(displ), # the only change is here hwy &gt;= mpg_cutoff, class == cls ) } result # return the object } So what’s going on here? Not a whole lot really. The function just filters the data to observations that match the input criteria, and returns that result at the end. We also put default values to the arguments, which can be done to your discretion. Conditionals The core of the above function uses a conditional statement using standard if…else structure. The if part determines whether some condition holds. If it does, then proceed to the next step in the brackets. If not, skip to the else part. You may have used the ifelse function in base R, or dplyr’s if_else as above, which are a short cuts for this approach. We can also add conditional else statements (else if), drop the else part entirely, nest conditionals within other conditionals, etc. Like loops, conditional statements look very similar across all programming languages. JavaScript: if (Math.random() &lt; 0.5) { console.log(&quot;You got Heads!&quot;) } else { console.log(&quot;You got Tails!&quot;) } Python: if x == 2: print(x) else: print(x*x) In any case, with our function at the ready, we can now do the things we want to as needed: good_mileage(mpg_cutoff = 40) # A tibble: 1 x 11 manufacturer model displ year cyl trans drv cty hwy fl class &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 volkswagen jetta 1.9 1999 4 manual(m5) f 33 44 d compact good_mileage( cylinder = 8, mpg_cutoff = 15, displ_low = FALSE, cls = &#39;suv&#39; ) # A tibble: 34 x 11 manufacturer model displ year cyl trans drv cty hwy fl class &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 chevrolet c1500 suburban 2wd 5.3 2008 8 auto(l4) r 14 20 r suv 2 chevrolet c1500 suburban 2wd 5.3 2008 8 auto(l4) r 11 15 e suv 3 chevrolet c1500 suburban 2wd 5.3 2008 8 auto(l4) r 14 20 r suv 4 chevrolet c1500 suburban 2wd 5.7 1999 8 auto(l4) r 13 17 r suv 5 chevrolet c1500 suburban 2wd 6 2008 8 auto(l4) r 12 17 r suv 6 chevrolet k1500 tahoe 4wd 5.3 2008 8 auto(l4) 4 14 19 r suv 7 chevrolet k1500 tahoe 4wd 5.7 1999 8 auto(l4) 4 11 15 r suv 8 chevrolet k1500 tahoe 4wd 6.5 1999 8 auto(l4) 4 14 17 d suv 9 dodge durango 4wd 4.7 2008 8 auto(l5) 4 13 17 r suv 10 dodge durango 4wd 4.7 2008 8 auto(l5) 4 13 17 r suv # … with 24 more rows Let’s extend the functionality by adding a year argument (the only values available are 2008 and 1999). good_mileage &lt;- function( cylinder = 4, mpg_cutoff = 30, displ_fun = mean, displ_low = TRUE, cls = &#39;compact&#39;, yr = 2008 ) { if (displ_low) { result = mpg %&gt;% filter(cyl == cylinder, displ &lt;= displ_fun(displ), hwy &gt;= mpg_cutoff, class == cls, year == yr) } else { result = mpg %&gt;% filter(cyl == cylinder, displ &gt;= displ_fun(displ), hwy &gt;= mpg_cutoff, class == cls, year == yr) } result } good_mileage( cylinder = 8, mpg_cutoff = 19, displ_low = FALSE, cls = &#39;suv&#39;, yr = 2008 ) # A tibble: 6 x 11 manufacturer model displ year cyl trans drv cty hwy fl class &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 chevrolet c1500 suburban 2wd 5.3 2008 8 auto(l4) r 14 20 r suv 2 chevrolet c1500 suburban 2wd 5.3 2008 8 auto(l4) r 14 20 r suv 3 chevrolet k1500 tahoe 4wd 5.3 2008 8 auto(l4) 4 14 19 r suv 4 ford explorer 4wd 4.6 2008 8 auto(l6) 4 13 19 r suv 5 jeep grand cherokee 4wd 4.7 2008 8 auto(l5) 4 14 19 r suv 6 mercury mountaineer 4wd 4.6 2008 8 auto(l6) 4 13 19 r suv So we now have something that is flexible, reusable, and extensible, and it took less code than writing out the individual lines of code. Anonymous functions Oftentimes we just need a quick and easy function for a one-off application, especially when using apply/map functions. Consider the following two lines of code. apply(mtcars, 2, sd) apply(mtcars, 2, function(x) x / 2 ) The difference between the two is that for the latter, our function didn’t have to be a named object already available. We created a function on the fly just to serve a specific purpose. A function doesn’t exist in base R that just does nothing but divide by two, but since it is simple, we just created it as needed. To further illustrate this, we’ll create a robust standardization function that uses the median and median absolute deviation rather than the mean and standard deviation. # some variables have a mad = 0, and so return Inf (x/0) or NaN (0/0) # apply(mtcars, 2, function(x) (x - median(x))/mad(x)) %&gt;% # head() mtcars %&gt;% map_df(function(x) (x - median(x))/mad(x)) # A tibble: 32 x 11 mpg cyl disp hp drat wt qsec vs am gear carb &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.333 0 -0.258 -0.169 0.291 -0.919 -0.883 NaN Inf 0 1.35 2 0.333 0 -0.258 -0.169 0.291 -0.587 -0.487 NaN Inf 0 1.35 3 0.665 -0.674 -0.629 -0.389 0.220 -1.31 0.636 Inf Inf 0 -0.674 4 0.407 0 0.439 -0.169 -0.873 -0.143 1.22 Inf NaN -0.674 -0.674 5 -0.0924 0.674 1.17 0.674 -0.774 0.150 -0.487 NaN NaN -0.674 0 6 -0.203 0 0.204 -0.233 -1.33 0.176 1.77 Inf NaN -0.674 -0.674 7 -0.905 0.674 1.17 1.58 -0.689 0.319 -1.32 NaN NaN -0.674 1.35 8 0.961 -0.674 -0.353 -0.791 -0.00710 -0.176 1.62 Inf NaN 0 0 9 0.665 -0.674 -0.395 -0.363 0.319 -0.228 3.67 Inf NaN 0 0 10 0 0 -0.204 0 0.319 0.150 0.417 Inf NaN 0 1.35 # … with 22 more rows Even if you don’t use anonymous functions (sometimes called lambda functions), it’s important to understand them, because you’ll often see other people’s code using them. While it goes beyond the scope of this document at present, I should note that RStudio has a very nice and easy to use debugger. Once you get comfortable writing functions, you can use the debugger to troubleshoot problems that arise, and test new functionality (see the ‘Debug’ menu). In addition, one can profile functions to see what parts are, for example, more memory intensive, or otherwise serve as a bottleneck (see the ‘Profile’ menu). You can use the profiler on any code, not just functions. Writing Functions Exercises Excercise 1 Write a function that takes the log of the sum of two values (i.e. just two single numbers) using the log function. Just remember that within a function, you can write R code just like you normally would. log_sum &lt;- function(a, b) { ? } Excercise 1b What happens if the sum of the two numbers is negative? You can’t take a log of a negative value, so it’s an error. How might we deal with this? Try using a conditional to provide an error message using the stop function. The first part is basically identical to the function you just did. But given that result, you will need to check for whether it is negative or not. The message can be whatever you want. log_sum &lt;- function(a, b) { ? if (? &lt; 0) { stop(&#39;Your message here.&#39;) } else { ? return(your_log_sum_results) # this is an arbitrary name, change accordingly } } Exercise 2 Let’s write a function that will take a numeric variable and convert it to a character string of ‘positive’ vs. ‘negative’. We can use if {}... else {} structure, ifelse, or dplyr::if_else- they all would accomplish this. In this case, the input is a single vector of numbers, and the output will recode any negative value to ‘negative’ and positive values to ‘positive’ (or whatever you want). Here is an example of how we would just do it as a one-off. set.seed(123) # so you get the exact same &#39;random&#39; result x &lt;- rnorm(10) if_else(x &lt; 0, &quot;negative&quot;, &quot;positive&quot;) Now try your hand at writing a function for that. pos_neg &lt;- function(?) { ? } "],
["more.html", "More Code Style Vectorization Regular Expressions Code Style Exercises Vectorization Exercises Regex Exercises", " More This section is kind of a grab bag of miscellaneous things related to programming. If you’ve made it this far, feel free to keep going! Code Style A lot has been written about coding style over the decades. If there was a definitive answer, you would have heard of it by now. However, there are a couple things you can do at the beginning of your programming approach to go a long way making your code notably better. Why does your code exist? Either use text in an R Markdown file or comment your R script. Explain why, not what, the code is doing. Think of it as leaving your future self a note (they will thank you!). Be clear, and don’t assume you’ll remember why you were doing what you did. Assignment You see some using &lt;- or = for assignment. While there is a slight difference, if you’re writing decent code it shouldn’t matter. Far more programming languages use =, so that’s reason to prefer it. However, if you like being snobby about things, go with &lt;-. Whichever you use, do so consistently15. Code length If your script is becoming hundreds of lines long, you probably need to compartmentalize your operations into separate scripts. For example, separate your data processing from your model scripts. Spacing Don’t be stingy with spaces. As you start out, err on the side of using them. Just note there are exceptions (e.g. no space between function name and parenthesis, unless that function is something like if or else), but you’ll get used to the exceptions over time. x=rnorm(10, mean=0,sd=1) # harder to read # space between lines too! x = rnorm(10, mean = 0, sd = 1) # easier to read Naming things You might not think of it as such initially, but one of the more difficult challenges in programming is naming things. Even if we can come up with a name for an object or file, there are different styles we can use for the name. Here is a brief list of things to keep in mind. The name should make sense to you, your future self, and others that will use the code Try to be concise, but see the previous Make liberal use of suffixes/prefixes for naming the same types of things e.g. model_x, model_z For function names, try for verbs that describe what they do (e.g. add_two vs. two_more or plus2) Don’t name anything with ‘final’ Don’t name something that is already an R function/object (e.g. T, c, data, etc.) Avoid distinguishing names only by number, e.g. data1 data2 Common naming styles include: snake_case CamelCase or camelCase spinal-case (e.g. for file names) dot.case For objects and functions, I find snake case easier to read and less prone to issues16. For example, camel case can fail miserably when acronyms are involved. Dots already have specific uses (file name extensions, function methods, etc.), so probably should be avoided unless you’re using them for that specific purpose (they can also make selecting the whole name difficult depending on the context). Other Use tools like the built-in RStudio code cleanup shortcut like Ctrl/Cmd + Shft + A. It’s not perfect, in the sense I disagree with some of its style choice, but it will definitely be better than you will do on your own starting out. Vectorization Boolean Indexing Assume x is a vector of numbers. How would we create an index representing any value greater than 2? x = c(-1, 2, 10, -5) idx = x &gt; 2 idx [1] FALSE FALSE TRUE FALSE x[idx] [1] 10 As mentioned previously, logicals are objects with values of TRUE or FALSE, like the idx variable above. While sometimes we want to deal with the logical object as an end, it is extremely commonly used as an index in data processing. Note how we don’t have to create an explicit index object first (though often you should), as R indexing is ridiculously flexible. Here are more examples, not necessarily recommended, but just to demonstrate the flexibility of Boolean indexing. x[x &gt; 2] x[x != &#39;cat&#39;] x[ifelse(x &gt; 2 &amp; x !=10, TRUE, FALSE)] x[{y = idx; y}] x[resid(lm(y ~ x)) &gt; 0] All of these will transfer to the tidyverse filter function. df %&gt;% filter(x &gt; 2, z == &#39;a&#39;) # commas are like &amp; Vectorized operations Boolean indexing allows us to take vectorized approaches to dealing with data. Consider the following unfortunately coded loop, where we create a variable y, which takes on the value of Yes if the variable x is greater than 2, and No if otherwise. for (i in 1:nrow(mydf)) { check = mydf$x[i] &gt; 2 if (check == TRUE) { mydf$y[i] = &#39;Yes&#39; } else { mydf$y[i] = &#39;No&#39; } } Compare17: mydf$y = &#39;No&#39; mydf$y[mydf$x &gt; 2] = &#39;Yes&#39; This gets us the same thing, and would be much faster than the looped approach. Boolean indexing is an example of a vectorized operation. The whole vector is considered, rather than each element individually. The result is that any preprocessing is done once rather than the n iterations of the loop. In R, this will always faster. Example: Log all values in a matrix. mymatrix_log = log(mymatrix) This is way faster than looping over elements, rows or columns. Here we’ll let the apply function stand in for our loop, logging the elements of each column. mymatrix = matrix(runif(100), 10, 10) identical(apply(mymatrix, 2, log), log(mymatrix)) [1] TRUE library(microbenchmark) microbenchmark(apply(mymatrix, 2, log), log(mymatrix)) Unit: nanoseconds expr min lq mean median uq max neval apply(mymatrix, 2, log) 23834 24453.5 25552.80 24687.0 24938.5 80821 100 log(mymatrix) 721 747.5 883.39 842.5 903.5 5402 100 Many vectorized functions already exist in R. They are often written in C, Fortran etc., and so even faster. Not all programming languages lean toward vectorized operations, and may not see much speed gain from it. In R however, you’ll want to prefer it. Even without, it’s cleaner/clearer code, another reason to use the approach. Timings We made our own function before, however, there is a scale function in base R that uses a more vectorized approach under the hood to standardize variables. The following demonstrates various approaches to standardizing the columns of the matrix, even using a parallelized approach. As you’ll see, the base R function requires very little code and beats the others. mymat = matrix(rnorm(100000), ncol=1000) stdize &lt;- function(x) { (x-mean(x)) / sd(x) } doubleloop = function() { for (i in 1:ncol(mymat_asdf)) { x = mymat_asdf[, i] for (j in 1:length(x)) { x[j] = (x[j] - mean(x)) / sd(x) } } } singleloop = function() { for (i in 1:ncol(mymat_asdf)) { x = mymat_asdf[, i] x = (x - mean(x)) / sd(x) } } library(parallel) cl = makeCluster(8) clusterExport(cl, c(&#39;stdize&#39;, &#39;mymat&#39;)) test = microbenchmark::microbenchmark( doubleloop = doubleloop(), singleloop = singleloop(), apply = apply(mymat, 2, stdize), parApply = parApply(cl, mymat, 2, stdize), vectorized = scale(mymat), times = 25 ) stopCluster(cl) test Regular Expressions A regular expression, regex for short, is a sequence of characters that can be used as a search pattern for a string. Common operations are to merely detect, extract, or replace the matching string. There are actually many different flavors of regex for different programming languages, which are all flavors that originate with the Perl approach, or can enable the Perl approach to be used. However, knowing one means you pretty much know the others with only minor modifications if any. To be clear, not only is regex another language, it’s nigh on indecipherable. You will not learn much regex, but what you do learn will save a potentially enormous amount of time you’d otherwise spend trying to do things in a more haphazard fashion. Furthermore, practically every situation that will come up has already been asked and answered on Stack Overflow, so you’ll almost always be able to search for what you need. Here is an example of a pattern we might be interested in: ^r.*shiny[0-9]$ What is that you may ask? Well here is an example of strings it would and wouldn’t match. We’re using grepl to return a logical (i.e. TRUE or FALSE) if any of the strings match the pattern in some way. string = c(&#39;r is the shiny&#39;, &#39;r is the shiny1&#39;, &#39;r shines brightly&#39;) grepl(string, pattern=&#39;^r.*shiny[0-9]$&#39;) [1] FALSE TRUE FALSE What the regex is esoterically attempting to match is any string that starts with ‘r’ and ends with ‘shiny_’ where _ is some single digit. Specifically it breaks down as follows: ^ : starts with, so ^r means starts with r . : any character * : match the preceding zero or more times shiny : match ‘shiny’ [0-9] : any digit 0-9 (note that we are still talking about strings, not actual numbered values) $ : ends with preceding Typical Uses None of it makes sense, so don’t attempt to do so. Just try to remember a couple key approaches, and search the web for the rest. Along with ^ . * [0-9] $, a couple more common ones are: [a-z] : letters a-z [A-Z] : capital letters + : match the preceding one or more times () : groupings | : logical or e.g. [a-z]|[0-9] (a lower case letter or a number) ? : preceding item is optional, and will be matched at most once. Typically used for ‘look ahead’ and ‘look behind’ \\ : escape a character, like if you actually wanted to search for a period instead of using it as a regex pattern, you’d use \\., though in R you need \\\\, i.e. double slashes, for escape. In addition, in R there are certain predefined characters that can be called: [:punct:] : punctuation [:blank:] : spaces and tabs [:alnum:] : alphanumeric characters Those are just a few. The key functions can be found by looking at the help file for the grep function (?grep). However, the stringr package has the same functionality with perhaps a slightly faster processing (though that’s due to the underlying stringi package). See if you can guess which of the following will turn up TRUE. grepl(c(&#39;apple&#39;, &#39;pear&#39;, &#39;banana&#39;), pattern=&#39;a&#39;) grepl(c(&#39;apple&#39;, &#39;pear&#39;, &#39;banana&#39;), pattern=&#39;^a&#39;) grepl(c(&#39;apple&#39;, &#39;pear&#39;, &#39;banana&#39;), pattern=&#39;^a|a$&#39;) Scraping the web, munging data, just finding things in your scripts … you can potentially use this all the time, and not only with text analysis, as we’ll now see. Code Style Exercises Exercise 1 For the following model related output, come up with a name for each object. lm(hwy ~ cyl, data = mpg) # hwy mileage predicted by number of cylinders summary(lm(hwy ~ cyl, data = mpg)) # the summary of that lm(hwy ~ cyl + displ + year, data = mpg) # an extension of that Exercise 2 Fix this code. x=rnorm(100, 10, 2) y=.2* x+ rnorm(100) data = data.frame(x,y) q = lm(y~x, data=data) summary(q) Vectorization Exercises Before we do this, did you remember to fix the names in the previous exercise? Exercise 1 Show a non-vectorized (e.g. a loop) and a vectorized way to add a two to the numbers 1 through 3. ? Exercise 2 Of the following, which do you think is faster? Test it with the microbenchmark package. x = matrix(rpois(100000, lambda = 5), ncol = 100) colSums(x) apply(x, 2, sum) microbenchmark::microbenchmark( cs = colSums(x), app = apply(x, 2, sum) ) Regex Exercises Exercise 1 Using stringr and str_replace, replace all the states a’s with nothing. library(stringr) str_replace(state.name, pattern = ?, replacement = ?) I usually reserve arrows for functions and package development.↩︎ Even though I think it’s obviously easier to read, based on how eyes/brains work, this is not necessarily the case for everyone. Many style guides you encounter will try to come across as definitive. They are not, nor are most of their claims tested. Same here.↩︎ For those familiar with ifelse, that would be applicable, but is not the point of the example.↩︎ "],
["models.html", "Model Exploration Model Taxonomy Linear models Estimation Fitting Models Summarizing Models Variable Transformations Variable Importance Extracting Output Visualization Extensions to the Standard Linear Model Model Exploration Summary Model Exploration Exercises", " Model Exploration The following section shows how to get started with modeling in R generally, with a focus on concepts, tools, and syntax, rather than trying to understand the specifics of a given model. We first dive into model exploration, getting a sense of the basic mechanics behind our modeling tools, and contemplating standard results. We’ll then shift our attention to understanding the strengths and limitations of our models. We’ll then change from classical methods to explore machine learning techniques. The goal of these chapters is to provide an overview of concepts and ways to think about modeling. Model Taxonomy We can begin with a taxonomy that broadly describes two classes of models: Supervised Unsupervised Some combination For supervised settings, there is a target or set of target variables which we aim to predict with a set of predictor variables or covariates. This is far and away the most common case, and the one we will focus on here. It is very common in machine learning parlance to further distinguish regression and classification among supervised models, but what they actually mean to distinguish is numeric target variables from categorical ones (it’s all regression). In the case of unsupervised models, the data itself is the target, and this setting includes techniques such as principal components analysis, factor analysis, cluster analytic approaches, topic modeling, and many others. A key goal for many such methods is dimension reduction, either of the columns or rows. For example, we may have many items of a survey we wish to group together into a few concepts, or cluster thousands of observations into a few simple categories. We can also broadly describe two primary goals of modeling: Prediction Explanation Different models will provide varying amounts of predictive and explanatory (or inferential) power. In some settings, prediction is almost entirely the goal, with little need to understand the underlying details of the relation of inputs to outputs. For example, in a model that predicts words to suggest when typing, we don’t really need to know nor much care about the details except to be able to improve those suggestions. In scientific studies however, we may be much more interested in the (potentially causal) relations among the variables under study. While these are sometimes competing goals, it is definitely not the case that they are mutually exclusive. For example, a fully interpretable model, statistically speaking, may have no predictive capability, and so is fairly useless in practical terms. Often, very predictive models offer little understanding. But sometimes we can luck out and have both a highly predictive model as well as one that is highly interpretable. Linear models Most models you see in published reports are linear models of varying kinds, and form the basis on which to build more complex forms. In such models we distinguish a target variable we want to understand from the variables which we will use to understand it. Note that these come with different names depending on the goal of the study, discipline, and other factors18. The following table denotes common nomenclature across many disciplines. Type Names Target Dependent variable Endogenous Response Outcome Output Y Regressand Left hand side (LHS) Predictor Independent variable Exogenous Explanatory Variable Covariate Input X Regressor Right hand side (RHS) A typical way to depict a linear regression model is as follows: \\[y = b_0 + b_1\\cdot x_1 + b_2\\cdot x_2 + ... + + b_p\\cdot x_p + \\epsilon\\] In the above, \\(b_0\\) is the intercept, and the other \\(b_*\\) are the regression coefficients that represent the relationship of the predictors \\(x\\) to the target variable \\(y\\). The \\(\\epsilon\\) represents the error or residual. We don’t have perfect prediction, and that represents the difference between what we can guess with our predictor relationships to the target and what we actually observe with it. In R, we specify a linear model as follows. Conveniently enough, we use a function, lm, that stands for linear model. There are various inputs, typically starting with the formula. In the formula, The target variable is first, followed by the predictor variables, separated by a tilde (~). Additional predictor variables are added with a plus sign (+). In this example, y is our target, and the predictors are x and z. lm(y ~ x + z) We can still use linear models to investigate nonlinear relationships. For example, in the following, we can add a quadratic term or an interaction, yet the model is still linear in the parameters. All of the following are standard linear regression models. lm(y ~ x + z + x:z) lm(y ~ x + x_squared) # a better way: lm(y ~ poly(x, degree = 2)) In the models above, x has a potentially nonlinear relationship with y, either by varying its (linear) relationship depending on values of z (the first case) or itself (the second). In general, the manner in which nonlinear relationships may be explored in linear models is quite flexible. An example of a nonlinear model would be population growth models, like exponential or logistic growth curves. You can use functions like nls or nlme for such models, but should have a specific theoretical reason to do so, and even then, flexible models such as GAMs might be better than assuming a functional form. Estimation One key thing to understand with predictive models of any kind is how we estimate the parameters of interest, e.g. coefficients/weights, variance, and more. To start with, we must have some sort of goal that choosing a particular set of values for the parameters achieves, and then find some way to reach that goal efficiently. Minimizing and Maximizing The goal of many estimation approaches is the reduction of loss, conceptually defined as the difference between the model predictions and the observed data, i.e. prediction error. In an introductory methods course, many are introduced to ordinary least squares as a means to estimate the coefficients for a linear regression model. In this scenario, we are seeking to come up with estimates of the coefficients that minimize the (squared) difference between the observed target value and the fitted value based on the parameter estimates. The loss in this case is defined as the sum of the squared errors. Formally we can state it as follows. \\[\\mathcal{Loss} = \\Sigma(y - \\hat{y})^2\\] We can see how this works more clearly with some simple conceptual code. In what follows, we create a [function][writing-functions], allows us to move [row by row][for-loops] through the data, calculating both our prediction based on the given model parameters- \\(\\hat{y}\\), and the difference between that and our target variable \\(y\\). We sum these squared differences to get a total. In practice such a function is called the loss function, cost function, or objective function. ls_loss &lt;- function(X, y, beta) { # initialize the objects loss = rep(0, nrow(X)) y_hat = rep(0, nrow(X)) # for each row, calculate y_hat and square the difference with y for (n in 1:nrow(X)) { y_hat[n] = sum(X[n, ] * beta) loss[n] = (y[n] - y_hat[n]) ^ 2 } sum(loss) } Now we need some data. Let’s construct some data so that we know the true underlying values for the regression coefficients. Feel free to change the sample size N or the coefficient values. set.seed(123) # for reproducibility N = 100 X = cbind(1, rnorm(N)) # a model matrix; first column represents the intercept y = 5 * X[, 1] + .5 * X[, 2] + rnorm(N) # a target with some noise; truth is y = 5 +.5*x df = data.frame(y = y, x = X[, 2]) Now let’s make some guesses for the coefficients, and see what the corresponding sum of the squared errors, i.e. the loss, would be. ls_loss(X, y, beta = c(0, 1)) # guess 1 [1] 2467.106 ls_loss(X, y, beta = c(1, 2)) # guess 2 [1] 1702.547 ls_loss(X, y, beta = c(4, .25)) # guess 3 [1] 179.2952 We see that in our third guess we reduce the loss quite a bit relative to our first guess. This makes sense because a value of 4 for the intercept and .25 for the coefficient for x are not as relatively far from the true values. However, we can also see that they are not the best we could have done. In addition, with more data, our estimated coefficients would get closer to true values. model = lm(y ~ x, df) # fit the model and obtain parameter estimates using OLS coef(model) # best guess given the data (Intercept) x 4.8971969 0.4475284 sum(residuals(model)^2) # least squares loss [1] 92.34413 In some relatively rare cases, a known approach is available and we do not have to search for the best estimates, but simply have to perform the correct steps that will result in them. For example, the following matrix operations will produce the best estimates for linear regression, which also happen to be the maximum likelihood estimates. solve(crossprod(X)) %*% crossprod(X, y) # &#39;normal equations&#39; [,1] [1,] 4.8971969 [2,] 0.4475284 coef(model) (Intercept) x 4.8971969 0.4475284 Most of the time we don’t have such luxury, or even if we did, the computations might be too great for the size of our data. Many statistical modeling techniques use maximum likelihood in some form or fashion, including Bayesian approaches, so you would do well to understand the basics. In this case, instead of minimizing the loss, we use an approach to maximize the probability of the observations of the target variable given the estimates of the parameters of the model (e.g. the coefficients in a regression)19. The following shows how this would look for estimating a single value like a mean for a set of observations from a specific distribution20. In this case, the true underlying value that maximizes the likelihood is 5, but we typically don’t know this. We see that as our guesses for the mean would get closer to 5, the likelihood of the observed values increases. Our final guess based on the observed data won’t be exactly 5, but with enough data and an appropriate model for that data, we should get close. Again, some simple conceptual code can help us. The next bit of code follows a similar approach to what we had with least squares regression, but the goal is instead to maximize the likelihood of the observed data. In this example, I fix the estimated variance, but in practice we’d need to estimate that parameter as well. As probabilities are typically very small, we work with them on the log scale. max_like &lt;- function(X, y, beta, sigma = 1) { likelihood = rep(0, nrow(X)) y_hat = rep(0, nrow(X)) for (n in 1:nrow(X)) { y_hat[n] &lt;- sum(X[n, ] * beta) likelihood[n] = dnorm(y[n], mean = y_hat[n], sd = sigma, log = TRUE) } sum(likelihood) } max_like(X, y, beta = c(0, 1)) # guess 1 [1] -1327.593 max_like(X, y, beta = c(1, 2)) # guess 2 [1] -1022.18 max_like(X, y, beta = c(4, .25)) # guess 3 [1] -300.6741 logLik(model) &#39;log Lik.&#39; -137.9115 (df=3) To better understand maximum likelihood, it might help to think of our model from a data generating perspective, rather than in terms of ‘errors’. In the standard regression setting, we think of a single observation as follows: \\[\\mu = b_0 + b_1*x_1 + ... + b_p*x_p\\] Or with matrix notation (consider it shorthand if not familiar): \\[\\mu = X\\beta\\] Now we display how \\(y\\) is generated: \\[y \\sim \\mathcal{N}(\\mathrm{mean} = \\mu, \\mathrm{sd} = \\sigma)\\] In words, this means that our target observation \\(y\\) is assumed to be normally distributed with some mean and some standard deviation/variance. The mean \\(\\mu\\) is a function, or simply weighted sum, of our covariates \\(X\\). The unknown parameters we have to estimate are the \\(\\beta\\), i.e. weights, and standard deviation \\(\\sigma\\) (or variance \\(\\sigma^2\\)). One more note regarding estimation, it is good to distinguish models from estimation procedures. The following shows the more specific to the more general for both models and estimation procedures respectively. Label Name LM Linear Model GLM Generalized Linear Model GLMM Generalized Linear Mixed Model GAMM Generalized Linear Mixed Model OLS Ordinary Least Squares WLS Weighted Least Squares GLS Generalized Least Squares GEE Generalized Estimating Equations GMM Generalized Method of Moments Optimization So we know the goal, but how do we get to it? In practice, we typically use optimization methods to iteratively search for the best estimates for the parameters of a given model. The functions we explored above provide a goal- to minimize loss (however defined- least squares for continuous, classification error for binary, etc.) or maximize the likelihood (or posterior probability in the Bayesian context). Whatever the goal, an optimizing algorithm will typically be used to find the estimates that reach that goal. Some approaches are very general, some are better for certain types of modeling problems. These algorithms continue to make guesses until some criterion has been reached (convergence)21. You generally don’t need to know the details to use these algorithms to fit models, but knowing a little bit about the optimization process and available options may prove useful to deal with more complex data scenarios, where convergence can be difficult. Some packages will even have documentation specifically dealing with convergence issues. In the more predictive models previously discussed, knowing more about the optimization algorithm may speedup the time it takes to train the model, or smooth out the variability in the process. As an aside, most Bayesian models use an estimation approach that is some form of Markov Chain Monte Carlo. It is a simulation based approach to generate subsequent estimates of parameters conditional on present estimates of them. One set of iterations is called a chain, and convergence requires multiple chains to mix well, i.e. come to similar conclusions about the parameter estimates. The goal even then is to maximize the log posterior distribution, similar to maximizing the likelihood. In the past this was an extremely computationally expensive procedure, but these days, modern laptops can handle even complex models with ease, though some data set sizes may be prohibitive still22. Fitting Models With practically every modern modeling package in R, the two components required to fit a model are the model formula, and a data frame that contains the variables specified in that formula. Consider the following models. In general the syntax is the similar regardless of package, with special considerations for the type of model. The data argument is not included in these examples, but would be needed. lm(y ~ x + z) # standard linear model/OLS glm(y ~ x + z, family = &#39;binomial&#39;) # logistic regression with binary response glm(y ~ x + z + offset(log(q)), family = &#39;poisson&#39;) # count/rate model betareg::betareg(y ~ x + z) # beta regression for targets between 0 and 1 pscl::hurdle(y ~ x + z, dist = &quot;negbin&quot;) # hurdle model with negative binomial response lme4::glmer(y ~ x + (1 | group), family = &#39;binomial&#39;) # generalized linear mixed model mgcv::gam(y ~ s(x)) # generalized additive model survival::coxph(Surv(time = t, event = q) ~ x) # Cox Proportional Hazards Regression # Bayesian mixed model brms::brm( y ~ x + (1 + x | group), family = &#39;zero_one_inflated_beta&#39;, prior = priors ) For examples of many other types of models, see this document. Let’s finally get our hands dirty and run an example. We’ll use the world happiness dataset23. This is country level data based on surveys taken at various years, and the scores are averages or proportions, along with other values like GDP. library(tidyverse) # load if you haven&#39;t already load(&#39;data/world_happiness.RData&#39;) # glimpse(happy) Variable N Mean SD Min Q1 Median Q3 Max Missing year 1704 2012.33 3.69 2005.00 2009.00 2012.00 2015.00 2018.00 0 life_ladder 1704 5.44 1.12 2.66 4.61 5.34 6.27 8.02 0 log_gdp_per_capita 1676 9.22 1.19 6.46 8.30 9.41 10.19 11.77 28 social_support 1691 0.81 0.12 0.29 0.75 0.83 0.90 0.99 13 healthy_life_expectancy_at_birth 1676 63.11 7.58 32.30 58.30 65.00 68.30 76.80 28 freedom_to_make_life_choices 1675 0.73 0.14 0.26 0.64 0.75 0.85 0.99 29 generosity 1622 0.00 0.16 -0.34 -0.12 -0.02 0.09 0.68 82 perceptions_of_corruption 1608 0.75 0.19 0.04 0.70 0.81 0.88 0.98 96 positive_affect 1685 0.71 0.11 0.36 0.62 0.72 0.80 0.94 19 negative_affect 1691 0.27 0.08 0.08 0.21 0.25 0.31 0.70 13 confidence_in_national_government 1530 0.48 0.19 0.07 0.33 0.46 0.61 0.99 174 democratic_quality 1558 -0.14 0.88 -2.45 -0.79 -0.23 0.65 1.58 146 delivery_quality 1559 0.00 0.98 -2.14 -0.71 -0.22 0.70 2.18 145 gini_index_world_bank_estimate 643 0.37 0.08 0.24 0.30 0.35 0.43 0.63 1061 happiness_score 554 5.41 1.13 2.69 4.51 5.31 6.32 7.63 1150 dystopia_residual 554 2.06 0.55 0.29 1.72 2.06 2.44 3.84 1150 The happiness score itself ranges from 2.7 to 7.6, with a mean of 5.4 and standard deviation of 1.1. Fitting a model with R is trivial, and at a minimum requires the two key ingredients mentioned before, the formula and data. Here we specify our target at happiness_score with predictors democratic quality, generosity, and GDP per capita (logged). happy_model_base = lm( happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, data = happy ) And that’s all there is to it. Using matrices Many packages still allow for matrix input instead of specifying a model formula, or even require it (but shouldn’t). This means separating data into a model (or design) matrix, and the vector or matrix of the target variable(s). For example, if we needed a speed boost and weren’t concerned about some typical output we could use lm.fit. First we need to create the required components. We can use model.matrix to get what we need. X = model.matrix( happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, data = happy ) head(X) (Intercept) democratic_quality generosity log_gdp_per_capita 8 1 -1.8443636 0.08909068 7.500539 9 1 -1.8554263 0.05136492 7.497038 10 1 -1.8865659 -0.11219829 7.497755 19 1 0.2516293 -0.08441135 9.302960 20 1 0.2572919 -0.02068741 9.337532 21 1 0.2999450 -0.03264282 9.376145 Note the column of ones in the model matrix X. This represents our intercept, but that may not mean much to you unless you understand matrix multiplication (nice demo here). The other columns are just as they are in the data. Note also that the missing values have been removed. nrow(happy) [1] 1704 nrow(X) [1] 411 The target variable must contain the same number of observations as in the model matrix, and there are various ways to create it to ensure this. Instead of model.matrix, there is also model.frame, which creates a data frame, with a method for extracting the corresponding target variable24. X_df = model.frame( happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, data = happy ) y = model.response(X_df) We can now fit the model as follows. happy_model_matrix = lm.fit(X, y) summary(happy_model_matrix) # only a standard list is returned Length Class Mode coefficients 4 -none- numeric residuals 411 -none- numeric effects 411 -none- numeric rank 1 -none- numeric fitted.values 411 -none- numeric assign 4 -none- numeric qr 5 qr list df.residual 1 -none- numeric coef(happy_model_matrix) (Intercept) democratic_quality generosity log_gdp_per_capita -1.0104775 0.1703734 1.1608465 0.6934213 In my experience, it is generally a bad sign if a package requires that you create the model matrix rather than doing so itself via the standard formula + data.frame approach. I typically find that such packages tend to skip out on many other things like using typical methods like predict, coef, etc., making them even more difficult to work with. In general, the only real time you should need to use model matrices is when you are creating your own modeling package, doing simulations, utilizing model speed-ups, or otherwise know why you need them. Summarizing Models Once we have a model, we’ll want to summarize the results of it. Most modeling packages have a summary method we can apply, which will provide parameter estimates, some notion of model fit, inferential statistics, and other output. happy_model_base_sum = summary(happy_model_base) Call: lm(formula = happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, data = happy) Residuals: Min 1Q Median 3Q Max -1.75376 -0.45585 -0.00307 0.46013 1.69925 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.01048 0.31436 -3.214 0.001412 ** democratic_quality 0.17037 0.04588 3.714 0.000233 *** generosity 1.16085 0.19548 5.938 6.18e-09 *** log_gdp_per_capita 0.69342 0.03335 20.792 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.6283 on 407 degrees of freedom (1293 observations deleted due to missingness) Multiple R-squared: 0.6953, Adjusted R-squared: 0.6931 F-statistic: 309.6 on 3 and 407 DF, p-value: &lt; 2.2e-16 There is a lot of info to parse there, so we’ll go over some of it in particular. The summary provides several pieces of information: the coefficients or weights (Estimate)25, the standard errors (Std. Error), the t-statistic (which is just the coefficient divided by the standard error), and the corresponding p-value. The main thing to look at are the actual coefficients and the direction of their relationship, positive or negative. For example, with regard to the effect of democratic quality, moving one point on democratic quality results in roughly 0.2 units of happiness. Is this a notable effect? Knowing the scale of the outcome can help us understand the magnitude of the effect in a general sense. Before we showed that the standard deviation of the happiness scale was 1.1. So, in terms of standard deviation units- moving 1 points on democratic quality would result in a 0.2 standard deviation increase in state-level happiness. We might consider this fairly small, but maybe not negligible. One thing we must also have in order to understand our results is to get a sense of the uncertainty in the effects. The following provides confidence intervals for each of the coefficients. confint(happy_model_base) 2.5 % 97.5 % (Intercept) -1.62845472 -0.3925003 democratic_quality 0.08018814 0.2605586 generosity 0.77656244 1.5451306 log_gdp_per_capita 0.62786210 0.7589806 Now we have a sense of the range of plausible values for the coefficients. The value we actually estimate is the best guess given our circumstances, but slight changes in the data, the way we collect it, the time we collect it, etc., all would result in a slightly different result. The confidence interval provides a range of what we could expect given the uncertainty, and, given its importance, you should always report it. In fact, just showing the coefficient and the interval would be better than typical reporting of the statistical test results, though you can do both. Variable Transformations Transforming variables can provide a few benefits in modeling, whether applied to the target, covariates, or both, and should regularly be used for most models. Some of these benefits include26: Interpretable intercepts More comparable covariate effects Faster estimation Easier convergence Help with heteroscedasticity For example, merely centering predictor variables, i.e. subtracting the mean, provides a more interpretable intercept that will fall within the actual range of the target variable, telling us what the value of the target variable is when the covariates are at their means (or reference value if categorical). Numeric variables The following table shows the interpretation of two extremely common transformations applied to numeric variables- logging and scaling (i.e. standardizing to mean zero, standard deviation one). target predictor interpretation y x \\(\\Delta y = \\beta\\Delta x\\) y log(x) \\(\\Delta y \\approx (\\beta/100)\\%\\Delta x\\) log(y) x \\(\\%\\Delta y \\approx 100\\cdot \\beta\\%\\Delta x\\) log(y) log(x) \\(\\%\\Delta y = \\beta\\%\\Delta x\\) y scale(x) \\(\\Delta y = \\beta\\sigma\\Delta x\\) scale(y) x \\(\\sigma\\Delta y = \\beta\\Delta x\\) scale(y) scale(x) \\(\\sigma\\Delta y = \\beta\\sigma\\Delta x\\) For example, to start with the normal linear model situation, a one-unit change in \\(x\\), i.e. \\(\\Delta x =1\\), leads to \\(\\beta\\) unit change in \\(y\\). If we log the target variable \\(y\\), the interpretation of the coefficient for \\(x\\) is that a one-unit change in \\(x\\) leads to an (approximately) 100\\(\\cdot\\)\\(\\beta\\)% change in \\(y\\). The 100 changes the result from a proportion to percentage change. More concretely, if \\(\\beta\\) was .5, a unit change in \\(x\\) leads to (roughly) a 50% change in \\(y\\). If both were logged, a percentage change in \\(x\\) leads to a \\(\\beta\\) percentage change in y27. These percentage change interpretations are called elasticities in econometrics and areas trained similarly28. It is very common to use standardized variables as well, also called normalizing, or simply scaling. If \\(y\\) and \\(x\\) are both standardized, a one unit (i.e. one standard deviation) change in \\(x\\) leads to a \\(\\beta\\) standard deviation change in \\(y\\). Again, if \\(\\beta\\) was .5, a standard deviation change in \\(x\\) leads to a half standard deviation change in \\(y\\). In general, there is nothing to lose by standardizing, so you should employ it often. Another common transformation, particularly in machine learning, is the min-max normalization, changing variables to range from some minimum to some maximum, usually zero to one. Categorical variables A raw character string is not an analyzable unit, so character strings and labeled variables like factors must be converted for analysis to be conducted on them. For categorical variables, we can employ what is called effects coding to test for specific types of group differences. Far and away the most common approach is called dummy coding or one-hot encoding29. In the next example, we will use dummy coding via the recipes package. I also to show how to standardize a numeric variable as previously discussed. library(recipes) nafta = happy %&gt;% filter(country %in% c(&#39;United States&#39;, &#39;Canada&#39;, &#39;Mexico&#39;)) dummy = nafta %&gt;% recipe(~ country + generosity) %&gt;% # formula approach for specifying variables step_dummy(country, one_hot = TRUE) %&gt;% # make variables for all factor levels step_center(generosity) %&gt;% # example of centering step_scale(generosity) # example of standardizing prep(dummy) %&gt;% # estimates the necessary data to apply to this or other data sets bake(nafta) %&gt;% # apply the computations print(n = 20) # A tibble: 39 x 4 generosity country_Canada country_Mexico country_United.States &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.835 1 0 0 2 0.819 1 0 0 3 0.891 1 0 0 4 0.801 1 0 0 5 0.707 1 0 0 6 0.841 1 0 0 7 1.06 1 0 0 8 1.21 1 0 0 9 0.940 1 0 0 10 0.838 1 0 0 11 0.590 1 0 0 12 0.305 1 0 0 13 -0.0323 1 0 0 14 NA 0 1 0 15 -1.19 0 1 0 16 -1.39 0 1 0 17 -1.08 0 1 0 18 -0.915 0 1 0 19 -1.22 0 1 0 20 -1.18 0 1 0 # … with 19 more rows We see that the first few observations are Canada, and the next few Mexico. Note that doing this is rarely required for most modeling situations, but even if not, it sometimes can be useful to do so explicitly. If your modeling package cannot handle factor variables, and thus requires explicit coding, you’ll know, and typically these are the same ones that require matrix input. Let’s run a regression as follows to show how it would happen automatically. model_dummy = lm(happiness_score ~ country, data = nafta) summary(model_dummy) Call: lm(formula = happiness_score ~ country, data = nafta) Residuals: Min 1Q Median 3Q Max -0.26960 -0.07453 -0.00615 0.06322 0.42920 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.36887 0.09633 76.493 5.64e-14 *** countryMexico -0.61107 0.13624 -4.485 0.00152 ** countryUnited States -0.34337 0.13624 -2.520 0.03275 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.1927 on 9 degrees of freedom (27 observations deleted due to missingness) Multiple R-squared: 0.692, Adjusted R-squared: 0.6236 F-statistic: 10.11 on 2 and 9 DF, p-value: 0.004994 In this case, the coefficient represents the difference in means on the target variable between the reference group and the group in question. In this case, the U.S. is -0.34 less on the happy score than the reference country (Canada). The intercept tells us the mean of the reference group. Other codings are possible, and these would allow for specific group comparisons or types of comparisons. This is sometimes called contrast coding. For example, we could compare Canada vs. both the U.S. and Mexico. By giving Canada twice the weight of the other two we can get this result. I also add a coding that will just compare Mexico vs. the U.S. The actual weights used are arbitrary, but in this case should sum to zero. group canada_vs_other mexico_vs_us Canada -0.667 0.0 Mexico 0.333 -0.5 United States 0.333 0.5 weights sum to zero, but are arbitrary Adding such coding to a factor variable allows the corresponding models to use it in constructing the model matrix, rather than dummy coding. See the group means and calculate the results by hand for yourself. nafta = nafta %&gt;% mutate(country_fac = factor(country)) contrasts(nafta$country_fac) = matrix(c(-2/3, 1/3, 1/3, 0, -.5, .5), ncol = 2) summary(lm(happiness_score ~ country_fac, data = nafta)) Call: lm(formula = happiness_score ~ country_fac, data = nafta) Residuals: Min 1Q Median 3Q Max -0.26960 -0.07453 -0.00615 0.06322 0.42920 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.05072 0.05562 126.769 6.01e-16 *** country_fac1 -0.47722 0.11799 -4.045 0.00291 ** country_fac2 0.26770 0.13624 1.965 0.08100 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.1927 on 9 degrees of freedom (27 observations deleted due to missingness) Multiple R-squared: 0.692, Adjusted R-squared: 0.6236 F-statistic: 10.11 on 2 and 9 DF, p-value: 0.004994 nafta %&gt;% group_by(country) %&gt;% summarise(happy = mean(happiness_score, na.rm = TRUE)) # A tibble: 3 x 2 country happy &lt;chr&gt; &lt;dbl&gt; 1 Canada 7.37 2 Mexico 6.76 3 United States 7.03 For example, we can see that for this balanced data set, the _fac1 coefficient is the average of the U.S. and Mexico coefficients that we got from dummy coding, which represented their respective mean differences from Canada: (-0.611 + -0.343) / 2 = -0.477. The _fac2 coefficient is just the U.S. Mexico mean difference, as expected. In other circumstances, we can use categorical embeddings to reduce a very large number of categorical levels to a smaller number of numeric variables. This is very commonly employed in deep learning. Scales, indices, and dimension reduction It is often the case that we have several correlated variables/items which do not all need to go into the model. For example, instead of using all items in a psychological scale, we can use the scale score, however defined, which is often just a sum score of the underlying items. Often people will create an index by using a principal components analysis, which can be thought of as a means to create a weighted sum score, or set of scores. Some (especially binary) items may tend toward the creation of a single variable that simply notes whether any of those collection of variables was present or not. Two-step approaches Some might do a preliminary analysis, such as a cluster analysis or factor analysis, to create new target or predictor variables. In the former we reduce several variables to a single categorical label. Factor analysis does the same but results in a more expressive continuous metric. While fine to use, the corresponding results are measured with error, so treating the categories or factor scores as you would observed variables will typically result in optimistic results when you later include them in a subsequent analysis like a linear regression. Though this difference is probably slight in most applications, keen reviewers would probably point out the model shortcoming. Don’t discretize Little pains advanced modelers more than seeing results where a nice expressive continuous metric is butchered into two categories (e.g. taking a numeric age and collapsing to ‘old’ vs. ‘young’). There is rarely a reason to do this, and it is difficult to justify. There are reasons to collapse rare labels of a categorical variable, so that the new variable has fewer but more frequent categories. For example, data may have five or six race categories, but often the values are lumped into majority group vs. minority group due to each minority category having too few observations. But even that can cause problems, and doesn’t really overcome the fact that you simply didn’t have enough data to begin with. Variable Importance In many circumstances, one of the modeling goals is to determine which predictor variable is most important out of the collection used in the model, or otherwise rank order the effectiveness of the predictors in some fashion. However, determining relative variable importance is at best an approximation with some methods, and a fairly hopeless endeavor with others. For just basic linear regression there are many methods that would not necessarily come to the same conclusions. Statistical significance, e.g. the Z/t statistic or p-value, is simply not a correct way to do so. Some believe that [standardizing numeric variables][numeric-variables] is enough, but it is not, and doesn’t help with comparison to categorical inputs. In addition, if you’re model is not strong, it doesn’t make much sense to even worry about which is the best of a bad lot. Another reason that ‘importance’ is a problematic endeavor is that a statistical result doesn’t speak to practical action, nor does it speak to the fact that small effects may be very important. Sex may be an important driver in social science model, but we may not be able to do anything about it for many outcomes that may be of interest. With health outcomes, any effects might be worthy of attention, however small, if they could practically increase the likelihood of survival. Even if you can come up with a metric you like, you would still need some measure of uncertainty around that to make a claim that one predictor is reasonably better than another, and the only real approach to do that is usually some computationally expensive procedure that you will likely have to put together by hand. As an example, for standard linear regression there are many methods that decompose \\(R^2\\) into relative contributions by the covariates. The tools to do so have to re-run the model in many ways to produce these estimates (see the relaimpo package for example), but you would then have to use bootstrapping or similar approach to get interval estimates for those measures of importance. Certain techniques like random forests have a natural way to provide variable importance metrics, but providing inference on them would similarly be very computationally expensive. In the end though, I think it is probably best to assume that any effect that seems practically distinct from zero might be worthy of attention, and can be regarded for its own sake. The more actionable, the better. Extracting Output The better you get at modeling, the more often you are going to need to get at certain parts of the model output easily. For example, we can extract the coefficients, residuals, model data and other parts from standard linear model objects from base R. Why would you want to do this? A simple example would be to compare effects across different settings. We can collect the values, put them in a data frame, and then to a table or visualization. Typical modeling methods you might want to use: summary: print results in a legible way plot: plot something about the model (e.g. diagnostic plots) predict: make predictions, possibly on new data confint: get confidence intervals for parameters coef: extract coefficients fitted: extract fitted values residuals: extract residuals AIC: extract AIC Here is an example of using the predict and coef methods. predict(happy_model_base, newdata = happy %&gt;% slice(1:5)) 1 2 3 4 5 3.838179 3.959046 3.928180 4.004129 4.171624 coef(happy_model_base) (Intercept) democratic_quality generosity log_gdp_per_capita -1.0104775 0.1703734 1.1608465 0.6934213 Also, it’s useful to assign the summary results to an object, so that you can extract things that are also useful but would not be in the model object. We did this before, so now let’s take a look. str(happy_model_base_sum, 1) List of 12 $ call : language lm(formula = happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, data = happy) $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language happiness_score ~ democratic_quality + generosity + log_gdp_per_capita .. ..- attr(*, &quot;variables&quot;)= language list(happiness_score, democratic_quality, generosity, log_gdp_per_capita) .. ..- attr(*, &quot;factors&quot;)= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..- attr(*, &quot;term.labels&quot;)= chr [1:3] &quot;democratic_quality&quot; &quot;generosity&quot; &quot;log_gdp_per_capita&quot; .. ..- attr(*, &quot;order&quot;)= int [1:3] 1 1 1 .. ..- attr(*, &quot;intercept&quot;)= int 1 .. ..- attr(*, &quot;response&quot;)= int 1 .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; .. ..- attr(*, &quot;predvars&quot;)= language list(happiness_score, democratic_quality, generosity, log_gdp_per_capita) .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:4] &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; .. .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;happiness_score&quot; &quot;democratic_quality&quot; &quot;generosity&quot; &quot;log_gdp_per_capita&quot; $ residuals : Named num [1:411] -0.405 -0.572 0.057 -0.426 -0.829 ... ..- attr(*, &quot;names&quot;)= chr [1:411] &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;19&quot; ... $ coefficients : num [1:4, 1:4] -1.01 0.17 1.161 0.693 0.314 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 $ aliased : Named logi [1:4] FALSE FALSE FALSE FALSE ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;(Intercept)&quot; &quot;democratic_quality&quot; &quot;generosity&quot; &quot;log_gdp_per_capita&quot; $ sigma : num 0.628 $ df : int [1:3] 4 407 4 $ r.squared : num 0.695 $ adj.r.squared: num 0.693 $ fstatistic : Named num [1:3] 310 3 407 ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;value&quot; &quot;numdf&quot; &quot;dendf&quot; $ cov.unscaled : num [1:4, 1:4] 0.2504 0.0229 -0.0139 -0.0264 0.0229 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 $ na.action : &#39;omit&#39; Named int [1:1293] 1 2 3 4 5 6 7 11 12 13 ... ..- attr(*, &quot;names&quot;)= chr [1:1293] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... - attr(*, &quot;class&quot;)= chr &quot;summary.lm&quot; If we want the adjusted \\(R^2\\) or root mean squared error (RMSE, i.e. average error30), they aren’t readily available in the model object, but they are in the summary object, so we can pluck them out as we would any other list object. happy_model_base_sum$adj.r.squared [1] 0.6930647 happy_model_base_sum[[&#39;sigma&#39;]] [1] 0.6282718 Package support There are many packages available to get at model results. One of the more widely used is broom, which has tidy and other functions that can apply in different ways to different models depending on their class. library(broom) tidy(happy_model_base) # A tibble: 4 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) -1.01 0.314 -3.21 1.41e- 3 2 democratic_quality 0.170 0.0459 3.71 2.33e- 4 3 generosity 1.16 0.195 5.94 6.18e- 9 4 log_gdp_per_capita 0.693 0.0333 20.8 5.93e-66 Some packages will produce tables for a model object that are more or less ready for publication. However, unless you know it’s in the exact style you need, you’re probably better off dealing with it yourself. For example, you can use tidy and do minor cleanup to get the table ready for publication. Visualization Models require visualization to be understood completely. If you aren’t using visualization as a fundamental part of your model exploration, you’re likely leaving a lot of that exploration behind, and not communicating the results as well as you could to the broadest audience possible. When adding nonlinear effects, interactions, and more, visualization is a must. Thankfully there are many packages to help you get data you need to visualize effects. We start with the emmeans package. In the following example we have a country effect, and wish to get the mean happiness scores per country. We then visualize the results. Here we can see that Mexico is lowest on average. happy_model_nafta = lm(happiness_score ~ country + year, data = nafta) library(emmeans) country_means = emmeans(happy_model_nafta, ~ country) country_means country emmean SE df lower.CL upper.CL Canada 7.37 0.064 8 7.22 7.52 Mexico 6.76 0.064 8 6.61 6.91 United States 7.03 0.064 8 6.88 7.17 Confidence level used: 0.95 plot(country_means) We can also test for pairwise differences between the countries, and there’s no reason not to visualize that also. In the following, after adjustment Mexico and U.S. might not differ on mean happiness, but the other comparisons are statistically notable31. pw_comparisons = contrast(country_means, method = &#39;pairwise&#39;, adjust = &#39;bonferroni&#39;) pw_comparisons contrast estimate SE df t.ratio p.value Canada - Mexico 0.611 0.0905 8 6.751 0.0004 Canada - United States 0.343 0.0905 8 3.793 0.0159 Mexico - United States -0.268 0.0905 8 -2.957 0.0547 P value adjustment: bonferroni method for 3 tests plot(pw_comparisons) The following example uses ggeffects. First, we run a model with an interaction of country and year (we’ll talk more about interactions later). Then we get predictions for the year by country, and subsequently visualize. We can see that the trend, while negative for all countries, is more pronounced as we move south. happy_model_nafta = lm(happiness_score ~ year*country, data = nafta) library(ggeffects) preds = ggpredict(happy_model_nafta, terms = c(&#39;year&#39;, &#39;country&#39;)) plot(preds) Whenever you move to generalized linear models or other more complicated settings, visualization is even more important, so it’s best to have some tools at your disposal. Extensions to the Standard Linear Model Different types of targets In many data situations, we do not have a continuous numeric target variable, or may want to use a different distribution to get a better fit, or adhere to some theoretical perspective. For example, count data is not continuous and often notably skewed, so assuming a normal symmetric distribution may not work as well. From a data generating perspective we can use the Poisson distribution32 for the target variable instead. \\[\\ln{\\mu} = X\\beta\\] \\[\\mu = e^{X\\beta}\\] \\[y \\sim \\mathcal{Pois}(\\mu)\\] Conceptually nothing has really changed from what we were doing with the standard linear model, except for the distribution. We still have a mean function determined by our predictors, and this is what we’re typically mainly interested in from a theoretical perspective. We do have an added step, a transformation of the mean (now usually called the linear predictor). Poisson naturally works with the log of the target, but rather than do that explicitly, we instead exponentiate the linear predictor. The link function33, which is the natural log in this setting, has a corresponding inverse link (or mean function)- exponentiation. In code we can demonstrate this as follows. set.seed(123) # for reproducibility N = 1000 # sample size beta = c(2, 1) # the true coefficient values x = rnorm(N) # a single predictor variable mu = exp(beta[1] + beta[2]*x) # the linear predictor y = rpois(N, lambda = mu) # the target variable lambda = mean glm(y ~ x, family = poisson) Call: glm(formula = y ~ x, family = poisson) Coefficients: (Intercept) x 2.009 0.994 Degrees of Freedom: 999 Total (i.e. Null); 998 Residual Null Deviance: 13240 Residual Deviance: 1056 AIC: 4831 A very common setting is the case where our target variable takes on only two values- yes vs. no, alive vs. dead, etc. The most common model used in such settings is the logistic regression model. In this case, it will have a different link to go with a different distribution. \\[\\ln{\\frac{\\mu}{1-\\mu}} = X\\beta\\] \\[\\mu = \\frac{1}{1+e^{-X\\beta}}\\] \\[y \\sim \\mathcal{Binom}(\\mathrm{prob}=\\mu, \\mathrm{size} = 1)\\] Here our link function is called the logit, and it’s inverse takes our linear predictor and puts it on the probability scale. Again, some code can help drive this home. mu = plogis(beta[1] + beta[2]*x) y = rbinom(N, size = 1, mu) glm(y ~ x, family = binomial) Call: glm(formula = y ~ x, family = binomial) Coefficients: (Intercept) x 2.141 1.227 Degrees of Freedom: 999 Total (i.e. Null); 998 Residual Null Deviance: 852.3 Residual Deviance: 708.8 AIC: 712.8 # extension to count/proportional model # mu = plogis(beta[1] + beta[2]*x) # total = rpois(N, lambda = 5) # events = rbinom(N, size = total, mu) # nonevents = total - events # # glm(cbind(events, nonevents) ~ x, family = binomial) You’ll have noticed that when we fit these models we used glm instead of lm. The normal linear model is a special case of generalized linear models, which includes a specific class of distributions - normal, poisson, binomial, gamma, beta and more - collectively referred to as the exponential family. While this family can cover a lot of ground, you do not have to restrict yourself to it, and many R modeling packages will provide easy access to more. The main point is that you have tools to deal with continuous, binary, count, ordinal, and other types of data. Furthermore, not much necessarily changes conceptually from model to model besides the link function and/or distribution. Correlated data Often in standard regression modeling situations we have data that is correlated, like when we observe multiple observations for individuals (e.g. longitudinal studies), or observations are clustered within geographic units. There are many ways to analyze all kinds of correlated data in the form of clustered data, time series, spatial data and similar. In terms of understanding the mean function and data generating distribution for our target variable, as we did in our previous models, not much changes. However, we will want to utilize estimation techniques that take this correlation into account. Examples of such models include: Mixed models (e.g. random intercepts, ‘multilevel’ models) Time series models (autoregressive) Spatial models (e.g. conditional autoregressive) As demonstration is beyond the scope of this document, the main point here is awareness. But see these on mixed models and generalized additive models. Other extensions There are many types of models that will take one well beyond the standard linear model. In some cases, the focus is multivariate, trying to model many targets at once. Other models will even be domain-specific, tailored to a very narrow type of problem. Whatever the scenario, having a good understanding of the models we’ve been discussing will likely help you navigate these new waters much more easily. Model Exploration Summary At this point you should have a good idea of how to get started exploring models with R. Generally what you will explore will be based on theory, or merely curiosity. Specific packages while make certain types of models easy to pull off, without much change to the syntax from the standard lm approach of base R. Almost invariably, you will need to process the data to make it more amenable to analysis and/or more interpretable. After model fitting, summaries and visualizations go a long way toward understanding the part of the world you are exploring. Model Exploration Exercises Exercise 1 With the Google app data, use a standard linear model (i.e. lm) to predict one of three target variables of your choosing: rating: the user ratings of the app avg_sentiment_polarity: the average sentiment score (positive vs. negative) for the app avg_sentiment_subjectivity: the average subjectivity score (subjective vs. objective) for the app For prediction use the following variables: reviews: number of reviews type: free vs. paid size_in_MB: size of the app in megabytes I would suggest preprocessing the number of reviews- dividing by 100,000, scaling (standardizing), or logging it (for the latter you can add 1 first to deal with zeros34). Interpret the results. Visualize the difference in means between free and paid apps. See the emmeans example above. load(&#39;data/google_apps.RData&#39;) model = lm(? ~ reviews + type + size_in_MB, data = google_apps) plot(emmeans::emmeans(model, ~type)) Exercise 2 Rerun the above with interactions of the number of reviews or app size (or both) with type (via a + b + a:b or just a*b for two predictors). Visualize the interaction. Does it look like the effect differs by type? model = lm(? ~ reviews + type*?, data = google_apps) plot(ggeffects::ggpredict(model, terms = c(&#39;size_in_MB&#39;, &#39;type&#39;))) Exercise 3 Use the fish data to predict the number of fish caught count by the following predictor variables: livebait: whether live bait was used or not child: how many children present persons: total persons on the trip If you wish, you can start with an lm, but as the number of fish caught is a count, it is suitable for using a poisson distribution via glm with family = poisson, so try that if you’re feeling up for it. If you exponentiate the coefficients, they can be interpreted as incidence rate ratios. load(&#39;data/fish.RData&#39;) model = glm(?, data = fish) I prefer target because it emphasizes where the focus should be- predicting/explaining that variable. It represents the bullseye we are trying to hit. Others, like ‘response’ or ‘outcome’, imply a specific type of study, and so don’t even make sense in many situations. ‘Output’ is too generic, and ‘dependent variable’, along with its roots in experimental design which is far from the normal data situation, seems to shift focus to the predictors, i.e. how the target depends on the covariates, which it often doesn’t, or doesn’t in a practical way. Furthermore, effects of one variable may depend on another, observations are dependent (correlated), etc. For the other side of the equation I’m more ambivalent, just as long as ‘independent variable’, which is only applicable in some experimental designs, is not used.↩︎ To be consistent with the many tools that minimize different types of losses, we usually minimize the negative log likelihood, rather than maximize the raw likelihood.↩︎ In this case, a Poisson distribution.↩︎ For example, if the next best guess for a regression coefficient results in only a difference to many decimal places from the previous one, you probably don’t care and can stop your search. Reaching your desired stopping point is known as convergence. In other cases, you may simply set a maximum number of iterations.↩︎ With very large data relative to the number of parameters to estimate, much of the gain for the Bayesian approach is diminished, though some interpretive advantages would remain.↩︎ The World Happiness Report is a survey of the state of global happiness that ranks countries by how happy their citizens perceive themselves to be. Almost all the information here is gleaned from the report and appendices. This regards the report data from 2008-2018 included in the 2019 report.↩︎ No, I have no idea why you can’t use model.response with model.matrix..↩︎ People tend to get hung up on different models or packages labeling these columns differently, but any GLM and many other similar models are going to provide the same basic information whichever statistical package (R or otherwise) you use, namely: estimated coefficient, the standard error, the test statistic, the p-value associated with that test statistic.↩︎ Note that none of the benefits regard normality. Transforming variables is not done to meet the normality assumption regarding residuals, and would rarely help in that regard.↩︎ The log transformations in the table are approximations that allow an ‘eyeballable’ interpretation. Typically we use exponentiated coefficients for a more exact interpretation. For example, if y is logged, a one-unit change in x leads to a \\(100*(e^B-1)\\)% change in y. See this link for more on interpreting logged variables.↩︎ In econometrics, if both target and predictor variable are logged, the coefficient is referred to as an elasticity.↩︎ Some distinguish one-hot from dummy coding in that the former creates a binary variable for all \\(C\\) categorical levels while dummy coding only creates \\(C-1\\) variables, leaving the reference group out. However, the reference group is only left out at model fitting, there’s no reason not to create all C variables, as it requires no additional effort and you might want to change the reference group.↩︎ For standard regression models, the divisor for calculating the RMSE is the residual degrees of freedom, but more generally it is just N, the sample size, as many modern models may have more parameters than observations.↩︎ Corrections to p-values are generally a poor way to deal with multiple testing. Adding regularization to shrink the coefficients from the outset is less arbitrary and more conceptually sound, as it makes no reference to an inferential framework that almost no one uses.↩︎ For historical reasons, Poisson is usually denoted with mean \\(\\lambda\\), and with the binomial the probability is often denoted with \\(\\pi\\), but consistency is desired here.↩︎ Before we distinguished models from estimation procedures, and it’s also important to distinguish models from link functions. In some disciplines (mostly one and those trained in its methods), you’ll see some refer to a ‘logit model’, for example. However that wouldn’t tell you if the model regards a target that is binary, categorical, ordered categorical, count or has values between zero and one, which cover several types of possible distributions.↩︎ Just for our purposes. If you have zeros, then that’s usually a sign that a log transform probably isn’t appropriate.↩︎ "],
["model_criticism.html", "Model Criticism Model Fit Model Assumptions Predictive Performance Model Comparison Model Averaging Model Criticism Summary Model Criticism Exercises", " Model Criticism It isn’t enough to simply fit a particular model, we must also ask how well it matches the data under study, if it can predict well on new data, where it fails, and more. In the following we will discuss how we can better understand our model and its limitations. Model Fit Standard linear model In the basic regression setting we can think of model fit in terms of a statistical result, or in terms of the match between our model predictions and the observed target values. The former provides an inferential perspective, but as we will see, is limited. The latter regards a more practical result, and may provide a more nuanced or different conclusion. Statistical Assessment In a standard linear model we can compare a model where there are no covariates vs. the model we actually care about, which may have many predictor variables. This is an almost useless test, but the results are typically reported both in standard output and academic presentation. Let’s think about it conceptually- how does the variability in our target break down? \\[\\textrm{Total Variance} = \\textrm{Model Explained Variance} + \\textrm{Residual Variance}\\] So the variability in our target (TV) can be decomposed into that which we can explain with the predictor variables (MEV), and everything else that is not in our model (RV). If we have nothing in the model, then TV = RV. Let’s revisit the summary of our model. Note the F-statistic, which represents a statistical test for the model as a whole. happy_model_base_sum Call: lm(formula = happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, data = happy) Residuals: Min 1Q Median 3Q Max -1.75376 -0.45585 -0.00307 0.46013 1.69925 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.01048 0.31436 -3.214 0.001412 ** democratic_quality 0.17037 0.04588 3.714 0.000233 *** generosity 1.16085 0.19548 5.938 6.18e-09 *** log_gdp_per_capita 0.69342 0.03335 20.792 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.6283 on 407 degrees of freedom (1293 observations deleted due to missingness) Multiple R-squared: 0.6953, Adjusted R-squared: 0.6931 F-statistic: 309.6 on 3 and 407 DF, p-value: &lt; 2.2e-16 The standard F statistic can be calculated as follows, where \\(p\\) is the number of predictors35: \\[F = \\frac{MV/p}{RV/(N-p-1)}\\] Conceptually it is a ratio of average squared variance to average unexplained variance. We can see this more explicitly as follows, where each predictor’s contribution to the total variance is provided in the Sum Sq column. anova(happy_model_base) Analysis of Variance Table Response: happiness_score Df Sum Sq Mean Sq F value Pr(&gt;F) democratic_quality 1 189.192 189.192 479.300 &lt; 2.2e-16 *** generosity 1 6.774 6.774 17.162 4.177e-05 *** log_gdp_per_capita 1 170.649 170.649 432.324 &lt; 2.2e-16 *** Residuals 407 160.653 0.395 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If we add those together and use our formula above we get: \\[F = \\frac{366.62/3}{160.653/407} = 309.6\\] Which is what is reported in the summary of the model. And the p-value is just pf(309.6, 3, 407, lower = FALSE), whose values can be extracted from the summary object. happy_model_base_sum$fstatistic value numdf dendf 309.5954 3.0000 407.0000 pf(309.6, 3, 407, lower.tail = FALSE) [1] 1.239283e-104 Because the F-value is so large and p-value so small, the printed result in the summary doesn’t give us the actual p-value. So let’s demonstrate again with a worse model, where the p-value will be higher. f_test = lm(happiness_score ~ generosity, happy) summary(f_test) Call: lm(formula = happiness_score ~ generosity, data = happy) Residuals: Min 1Q Median 3Q Max -2.81037 -0.89930 0.00716 0.84924 2.33153 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.41905 0.04852 111.692 &lt; 2e-16 *** generosity 0.89936 0.30351 2.963 0.00318 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.122 on 533 degrees of freedom (1169 observations deleted due to missingness) Multiple R-squared: 0.01621, Adjusted R-squared: 0.01436 F-statistic: 8.78 on 1 and 533 DF, p-value: 0.003181 pf(8.78, 1, 533, lower.tail = FALSE) [1] 0.003181551 We can make this F-test more explicit by actually fitting a null model and making the comparison. The following will provide the same result as before. We make sure to use the same data as in the original model, since there are missing values for some covariates. happy_model_null = lm(happiness_score ~ 1, data = model.frame(happy_model_base)) anova(happy_model_null, happy_model_base) Analysis of Variance Table Model 1: happiness_score ~ 1 Model 2: happiness_score ~ democratic_quality + generosity + log_gdp_per_capita Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 410 527.27 2 407 160.65 3 366.62 309.6 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In this case our F statistic generalizes to the following, where \\(\\textrm{Model}_1\\) is the simpler model and \\(p\\) now refers to the total number of parameters estimated (i.e. same as before + 1 for the intercept) \\[F = \\frac{(\\textrm{Model}_2\\ \\textrm{RV} - \\textrm{Model}_1\\ \\textrm{RV})/(p_2 - p_1)}{\\textrm{Model}_2\\ \\textrm{RV}/(N-p_2-1)}\\] From the previous results, we can perform the necessary arithmetic based on this formula to get the F statistic. ((527.27 - 160.65)/3) / (160.65/407) [1] 309.6054 \\(R^2\\) The statistical result just shown is mostly a straw man type of test- who actually cares if our model does statistically better than a model with nothing in it? Surely if you don’t do better than nothing, then you may need to think more intently about what you are trying to model and how. But just because you can knock the straw man down, it isn’t something to get overly excited about. Let’s turn instead to a different concept- the amount of variance of the target variable that is explained by our predictors. For the standard linear model setting, this statistic is called R-squared (\\(R^2\\)). Going back to our previous notions, \\(R^2\\) is just: \\[R^2 =\\textrm{Model Explained Variance}/\\textrm{Total Variance}\\] This also is reported by default in our summary printout. happy_model_base_sum Call: lm(formula = happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, data = happy) Residuals: Min 1Q Median 3Q Max -1.75376 -0.45585 -0.00307 0.46013 1.69925 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.01048 0.31436 -3.214 0.001412 ** democratic_quality 0.17037 0.04588 3.714 0.000233 *** generosity 1.16085 0.19548 5.938 6.18e-09 *** log_gdp_per_capita 0.69342 0.03335 20.792 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.6283 on 407 degrees of freedom (1293 observations deleted due to missingness) Multiple R-squared: 0.6953, Adjusted R-squared: 0.6931 F-statistic: 309.6 on 3 and 407 DF, p-value: &lt; 2.2e-16 With our values from before for model and total variance, we can calculate it ourselves. 366.62 / 527.27 [1] 0.6953174 Here is another way. Let’s get the model predictions, and see how well they correlate with the target. predictions = predict(happy_model_base) target = happy_model_base$model$happiness_score rho = cor(predictions, target) rho [1] 0.8338528 rho^2 [1] 0.6953106 Now you can see why it’s called \\(R^2\\). It is the squared Pearson \\(r\\) of the model expected value and the observed target variable. Adjustment One problem with \\(R^2\\) is that it always goes up, no matter what nonsense you add to a model. This is why we have an adjusted \\(R^2\\) that attempts to balance the sample size and model complexity. For very large data and/or simpler models, the difference is negligible. But you should always report the adjusted \\(R^2\\), as the default \\(R^2\\) is actually upwardly biased and doesn’t account for additional model complexity36. Beyond OLS People love \\(R^2\\), so much that they will report it wherever they can, even coming up with things like ‘Pseudo-\\(R^2\\)’ when it proves difficult. However, outside of the OLS setting where we assume a normal distribution as the underlying data-generating mechanism, \\(R^2\\) has little application, and so is not very useful. In some sense, for any numeric target variable we can ask how well our predictions correlate with the observed target values, but the notion of ‘variance explained’ doesn’t easily follow us. For example, for other distributions the estimated variance is a function of the mean (e.g. Poisson, Binomial), and so isn’t constant. In other settings we have multiple sources of (residual) variance, and some sources where it’s not clear whether the variance should be considered as part of the model explained variance or residual variance. For categorical targets the notion doesn’t really apply very well at all. At least for GLM for non-normal distributions, we can work with deviance, which is similar to the residual sum of squares in the OLS setting. We can get a ‘deviance explained’ using the following approach: Fit a null model, i.e. intercept only. This gives the total deviance (tot_dev). Fit the desired model. This provides the model unexplained deviance (model_dev) Calculate \\(\\frac{\\textrm{tot_dev} -\\textrm{model_dev}}{\\textrm{tot_dev}}\\) But this value doesn’t really behave in the same manner as \\(R^2\\). For one, it can actually go down for a more complex model, and there is no standard adjustment, neither of which is the case with \\(R^2\\) for the standard linear model. At most this can serve as an approximation. For more complicated settings you will have to rely on other means to determine model fit. Classification For categorical targets we must think about obtaining predictions that allow us to classify the observations into specific categories. Not surprisingly, this will require different metrics to assess model performance. Accuracy and other metrics A very natural starting point is accuracy, or what percentage of our predicted class labels match the observed class labels. However, our model will not spit out a character string, only a number. On the scale of the linear predictor it can be anything, but we will at some point transform it to the probability scale, obtaining a predicted probability for each category. The class associated with the highest probability is the predicted class. In the case of binary targets, this is just an if_else statement for one class if_else(probability &gt;= .5, 'class A', 'class B'). With those predicted labels and the observed labels we create what is commonly called a confusion matrix, but would more sanely be called a classification table, prediction table, or just about any other name one could come up with in the first 10 seconds of trying. Let’s look at the following hypothetical result. Observed = 1 Observed = 0 Predicted = 1 41 21 Predicted = 0 16 13 Observed = 1 Observed = 0 Predicted = 1 A B Predicted = 0 C D In some cases we predict correctly, in other cases not. In this 2 x 2 setting we label the cells A through D. With things in place, consider the following the following nomenclature. True Positive, False Positive, True Negative, False Negative: Above, these are A, B, D, and C respectively. Now let’s see what we can calculate. Accuracy: Number of correct classifications out of all predictions (A + D)/Total. In the above example this would be (41 + 13)/91, about 59%. Error Rate: 1 - Accuracy. Sensitivity: is the proportion of correctly predicted positives to all true positive events: A/(A + C). In the above example this would be 41/57, about 72%. High sensitivity would suggest a low type II error rate (see below), or high statistical power. Also known as true positive rate. Specificity: is the proportion of correctly predicted negatives to all true negative events: D/(B + D). In the above example this would be 13/34, about 38%. High specificity would suggest a low type I error rate (see below). Also known as true negative rate. Positive Predictive Value (PPV): proportion of true positives of those that are predicted positives: A/(A + B). In the above example this would be 41/62, about 66%. Negative Predictive Value (NPV): proportion of true negatives of those that are predicted negative: D/(C + D). In the above example this would be 13/29, about 45%. Precision: See PPV. Recall: See sensitivity. Lift: Ratio of positive predictions given actual positives to the proportion of positive predictions out of the total: (A/(A + C)) / ((A + B)/Total). In the above example this would be (41/(41 + 16))/((41 + 21)/(91)), or 1.06. F Score (F1 score): Harmonic mean of precision and recall: 2*(Precision*Recall)/(Precision+Recall). In the above example this would be 2*(.66*.72)/(.66+.72), about 0.69. Type I Error Rate (false positive rate): proportion of true negatives that are incorrectly predicted positive: B/(B+D). In the above example this would be 21/34, about 62%. Also known as alpha. Type II Error Rate (false negative rate): proportion of true positives that are incorrectly predicted negative: C/(C+A). In the above example this would be 16/57, about 28%. Also known as beta. False Discovery Rate: proportion of false positives among all positive predictions: B/(A+B). In the above example this would be 21/62, about 34%. Often used in multiple comparison testing in the context of ANOVA. Phi coefficient: A measure of association: (A*D - B*C) / (sqrt((A+C)*(D+B)*(A+B)*(D+C))). In the above example this would be 0.11. Several of these may also be produced on a per-class basis when there are more than two classes. In addition, for multi-class scenarios there are other metrics commonly employed. In general there are many, many other metrics for confusion matrices, any of which might be useful for your situation, but the above provides a starting point, and is enough for many situations. Model Assumptions There are quite a few assumptions for the standard linear model that we could talk about, but I’ll focus on just a handful, ordered roughly in terms of the severity of violation. Correct model Heteroscedasticity Independence of observations Normality These concern bias (the first), accurate inference (most of the rest), or other statistical concepts (efficiency, consistency). The issue with most of the assumptions you learn about in your statistics course is that they mostly just apply to the OLS setting. Moreover, you can meet all the assumptions you want and still have a crappy model. Practically speaking, the effects on inference often aren’t large enough to matter in many cases, as we shouldn’t be making any important decision based on a p-value, or slight differences in the boundaries of an interval. Even then, at least for OLS and other simpler settings, the solutions to these issues are often easy, for example, to obtain correct standard errors, or are mostly overcome by having a large amount of data. Still, the diagnostic tools can provide clues to model failure, and so have utility in that sense. As before, visualization will aid us here. library(ggfortify) autoplot(happy_model_base) The first plot shows the spread of the residuals vs. the model estimated values. By default, the three most extreme observations are noted. In this plot we are looking for a lack of any conspicuous pattern, e.g. a fanning out to one side or butterfly shape. If the variance was dependent on some of the model estimated values, we have a couple options: Use a model that does not assume constant variance Add complexity to the model to better capture more extreme observations Change the assumed distribution In this example we have it about as good as it gets. The second plot regards the normality of the residuals. If they are normally distributed, they would fall along the dotted line. Again, in practical application this is about as good as you’re going to get. In the following we can see that we have some issues, where predictions are worse at low and high ends, and we may not be capturing some of the tail of the target distribution. Another plot we can use to assess model fit is simply to note the predictions vs. the observed values, and this sort of plot would be appropriate for any model. Here I show this both as a scatterplot and a density plot. With the first, the closer the result is to a line the better, with the latter, we can more adequately see what the model is predicting in relation to the observed values. In this case, while we’re doing well, one limitation of the model is that it does not have as much spread as target, and so is not capturing the more extreme values. Beyond the OLS setting, assumptions may change, are more difficult to check, and guarantees are harder to come by. The primary one - that you have an adequate and sufficiently complex model - still remains the most vital. It is important to remember that these assumptions regard inference, not predictive capabilities. In addition, in many modeling scenarios we will actually induce bias to have more predictive capacity. In such settings statistical tests are of less importance, and there often may not even be an obvious test to use. Typically we will still have some means to get interval estimates for weights or predictions though. Predictive Performance While we can gauge predictive performance to some extent with a metric like \\(R^2\\) in the standard linear model case, even then it almost certainly an optimistic viewpoint, and adjusted \\(R^2\\) doesn’t really deal with the underlying issue. What is the problem? The concern is that we are judging model performance on the very data it was fit to. Any potential deviation to the underlying data would certainly result in a different result for \\(R^2\\), accuracy, or any metric we choose to look at. So the better estimate of how the model is doing is to observe performance on data it hasn’t seen, using a metric that better captures how close we hit the target. This data goes by different names- test set, validation set, holdout sample, etc., but the basic idea is that we use some data that wasn’t used in model fitting to assess performance. We can do this in any data situation by randomly splitting into a data set for training the model, and one used for testing the model’s performance. library(tidymodels) set.seed(12) happy_split = initial_split(happy, prop = 0.75) happy_train = training(happy_split) happy_test = testing(happy_split) %&gt;% drop_na() happy_model_train = lm( happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, data = happy_train ) predictions = predict(happy_model_train, newdata = happy_test) Comparing our loss on training and test (i.e. RMSE), we can see the loss is greater on the test set. You can use a package like yardstick to calculate this. RMSE_train RMSE_test % increase 0.622 0.758 21.9 While in many settings we could simply report performance metrics from the test set, for a more accurate assessment of test error, we’d do better by taking an average over several test sets, an approach known as cross-validation, something we’ll talk more about later. In general, we may do okay in scenarios where the model is simple and uses a lot of data, but even then we may find a notable increase in test error relative to training error. For more complex models and/or with less data, the difference in training vs. test could be quite significant. Model Comparison Up until now the focus has been entirely on one model. However, if you’re trying to learn something new, you’ll almost always want to have multiple plausible models to explore, rather than just confirming what you think you already know. This can be as simple as starting with a baseline model and adding complexity to it, but it could also be pitting fundamentally different theoretical models against one another. A notable problem is that complex models should always do better than simple ones. The question often then becomes if they are doing notably better given the additional complexity. So we’ll need some way to compare models in a way that takes the complexity of the model into account. Example: Additional covariates A starting point for adding model complexity is simply adding more covariates. Let’s add life expectancy and a yearly trend to our happiness model. To make this model comparable to our baseline model, they need to be fit to the same data, and life expectancy has a couple missing values the others do not. So we’ll start with some data processing. I will start by standardizing some of the variables, and making year start at zero, which will represent 2008, and finally dropping missing values. Refer to our previous section on transforming variables if you want to. happy_recipe = happy %&gt;% select( year, happiness_score, democratic_quality, generosity, healthy_life_expectancy_at_birth, log_gdp_per_capita ) %&gt;% recipe(happiness_score ~ . ) %&gt;% step_center(all_numeric(), -log_gdp_per_capita, -year) %&gt;% step_scale(all_numeric(), -log_gdp_per_capita, -year) %&gt;% step_knnimpute(all_numeric()) %&gt;% step_naomit(everything()) %&gt;% step_center(year, means = 2005) %&gt;% prep() happy_processed = happy_recipe %&gt;% bake(happy) Now let’s start with our baseline model again. happy_model_base = lm( happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, data = happy_processed ) summary(happy_model_base) Call: lm(formula = happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, data = happy_processed) Residuals: Min 1Q Median 3Q Max -1.53727 -0.29553 -0.01258 0.32002 1.52749 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -5.49178 0.10993 -49.958 &lt;2e-16 *** democratic_quality 0.14175 0.01441 9.838 &lt;2e-16 *** generosity 0.19826 0.01096 18.092 &lt;2e-16 *** log_gdp_per_capita 0.59284 0.01187 49.946 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.44 on 1700 degrees of freedom Multiple R-squared: 0.7805, Adjusted R-squared: 0.7801 F-statistic: 2014 on 3 and 1700 DF, p-value: &lt; 2.2e-16 We can see that moving one standard deviation on democratic quality and generosity leads to similar standard deviation increases in happiness. Moving 10 percentage points in GDP would lead to less than .1 standard deviation increase in happiness. Now we add our life expectancy and yearly trend. happy_model_more = lm( happiness_score ~ democratic_quality + generosity + log_gdp_per_capita + healthy_life_expectancy_at_birth + year, data = happy_processed ) summary(happy_model_more) Call: lm(formula = happiness_score ~ democratic_quality + generosity + log_gdp_per_capita + healthy_life_expectancy_at_birth + year, data = happy_processed) Residuals: Min 1Q Median 3Q Max -1.50879 -0.27081 -0.01524 0.29640 1.60540 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -3.691818 0.148921 -24.790 &lt; 2e-16 *** democratic_quality 0.099717 0.013618 7.322 3.75e-13 *** generosity 0.189113 0.010193 18.554 &lt; 2e-16 *** log_gdp_per_capita 0.397559 0.016121 24.661 &lt; 2e-16 *** healthy_life_expectancy_at_birth 0.311129 0.018732 16.609 &lt; 2e-16 *** year -0.007363 0.002728 -2.699 0.00702 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4083 on 1698 degrees of freedom Multiple R-squared: 0.8111, Adjusted R-squared: 0.8106 F-statistic: 1459 on 5 and 1698 DF, p-value: &lt; 2.2e-16 Here it would seem that life expectancy has a notable effect on happiness (shocker), but the yearly trend, while negative, is not statistically notable. In addition, the democratic effect is no longer significant, as it would seem that it’s contribution was more due to it’s correlation with life expectancy. But the key question is- is this model better? The adjusted \\(R^2\\) seems to indicate that we are doing slightly better with this model, but not much (0.81 vs. 0.78). We can test if the increase is a statistically notable one. Recall previously when we compared our model versus a null model to obtain a statistical test of model fit. Since these models are nested, i.e. one is a simpler form of the other, we can use the more general approach we depicted to compare these models. This ANOVA, or analysis of variance test, is essentially comparing whether the residual sum of squares (i.e. the loss) is statistically less for one model vs. the other. In many settings it is often called a likelihood ratio test. anova(happy_model_base, happy_model_more, test = &#39;Chi&#39;) Analysis of Variance Table Model 1: happiness_score ~ democratic_quality + generosity + log_gdp_per_capita Model 2: happiness_score ~ democratic_quality + generosity + log_gdp_per_capita + healthy_life_expectancy_at_birth + year Res.Df RSS Df Sum of Sq Pr(&gt;Chi) 1 1700 329.11 2 1698 283.11 2 45.997 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The Df from the test denotes that we have two additional parameters, i.e. coefficients, in the more complex model. But the main thing to note is whether the model statistically reduces the RSS. So we see that this is a statistically notable improvement as well. I actually do not like this test though. It requires nested models, which in some settings is either not the case or can be hard to determine, and ignores various aspects of uncertainty in parameter estimates. An approach that works in many settings is to compare AIC (Akaike Information Criterion). AIC is a value based on the likelihood for a given model, but which adds a penalty for complexity, since otherwise any more complex model would result in a larger likelihood (or in this case, smaller negative likelihood). In the following, \\(\\mathcal{L}\\) is the likelihood, and \\(\\mathcal{P}\\) is the number of parameters estimated for the model. \\[AIC = -2 ( \\ln (\\mathcal{L})) + 2 \\mathcal{P}\\] AIC(happy_model_base) [1] 2043.77 The value itself is meaningless until we compare models, in which case the lower value is the better model (because we are working with the negative log likelihood). With AIC, we don’t have to have nested models, so that’s a plus over the statistical test. AIC(happy_model_base, happy_model_more) df AIC happy_model_base 5 2043.770 happy_model_more 7 1791.237 Again, our new model works better. However, this still may miss out on some uncertainty in the models. To try and capture this, I will calculate interval estimates for the adjusted \\(R^2\\) via bootstrapping, and then calculate an interval for their difference. The details are beyond what I want to delve into here, but the gist is we just want a confidence interval for the difference in adjusted \\(R^2\\). model r2 2.5% 97.5% base 0.780 0.762 0.798 more 0.811 0.795 0.827 2.5% 97.5% Difference in \\(R^2\\) 0.013 0.049 It would seem the difference in adjusted \\(R^2\\) is not statistically different from zero. Likewise we could do the same for AIC. model aic 2.5% 97.5% base 2043.770 1917.958 2161.231 more 1791.237 1657.755 1911.073 2.5% 97.5% Difference in AIC -369.994 -126.722 In this case, the more complex model may not be statistically better either, as the interval for the difference in AIC also contains zero, and exhibits a notably wide range. Example: Interactions Let’s now add interactions to our model. Interactions allow the relationship of a predictor variable and target to vary depending on the values of another covariate. To keep things simple, we’ll add a single interaction to start- I will interact democratic quality with life expectancy. happy_model_interact = lm( happiness_score ~ democratic_quality + generosity + log_gdp_per_capita + healthy_life_expectancy_at_birth + democratic_quality:healthy_life_expectancy_at_birth, data = happy_processed ) summary(happy_model_interact) Call: lm(formula = happiness_score ~ democratic_quality + generosity + log_gdp_per_capita + healthy_life_expectancy_at_birth + democratic_quality:healthy_life_expectancy_at_birth, data = happy_processed) Residuals: Min 1Q Median 3Q Max -1.42801 -0.26473 -0.00607 0.26868 1.48161 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -3.63990 0.14517 -25.074 &lt; 2e-16 *** democratic_quality 0.08785 0.01335 6.580 6.24e-11 *** generosity 0.16479 0.01030 16.001 &lt; 2e-16 *** log_gdp_per_capita 0.38501 0.01578 24.404 &lt; 2e-16 *** healthy_life_expectancy_at_birth 0.33247 0.01830 18.165 &lt; 2e-16 *** democratic_quality:healthy_life_expectancy_at_birth 0.10526 0.01105 9.527 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.3987 on 1698 degrees of freedom Multiple R-squared: 0.82, Adjusted R-squared: 0.8194 F-statistic: 1547 on 5 and 1698 DF, p-value: &lt; 2.2e-16 The coefficient interpretation for variables in the interaction model changes. For those involved in an interaction, the base coefficient now only describes the effect when the variable they interact with is zero (or is at the reference group if it’s categorical). So democratic quality has a slight positive, but not statistically notable, effect at the mean of life expectancy (0.088). However, this effect increases by 0.11 when life expectancy increases by 1 (i.e. 1 standard deviation since we standardized). The same interpretation goes for life expectancy. It’s base coefficient is when democratic quality is at it’s mean (0.332), and the interaction term is interpreted identically. It seems most people (including journal reviewers) seem to have trouble understanding interactions if you just report them in a table. Furthermore, beyond the standard linear model with non-normal distributions, the coefficient for the interaction term doesn’t even have the same precise meaning. But you know what helps us in every interaction setting? Visualization! Let’s use ggeffects again. We’ll plot the effect of democratic quality at the mean of life expectancy, and at one standard deviation below and above. Since we already standardized it, this is even easier. library(ggeffects) plot( ggpredict( happy_model_interact, terms = c(&#39;democratic_quality&#39;, &#39;healthy_life_expectancy_at_birth[-1, 0, 1]&#39;) ) ) We seem to have discovered something interesting here! Democratic quality only has a positive effect for those countries with a high life expectancy, i.e. that are already in a good place in general. It may even be negative in countries in the contrary case. While this has to be taken with a lot of caution, it shows how exploring interactions can be fun and surprising! Another way to plot interactions in which the variables are continuous is with a contour plot similar to the following. Here we don’t have to pick arbitrary values to plot against, and can see the predictions at all values of the covariates in question. We see the the lowest expected happiness based on the model is with high democratic quality and low life expectancy. The best case scenario is to be high on both. Here is our model comparison for all three models with AIC. AIC(happy_model_base, happy_model_more, happy_model_interact) df AIC happy_model_base 5 2043.770 happy_model_more 7 1791.237 happy_model_interact 7 1709.801 Looks like our interaction model is winning. Example: Additive Models Generalized additive models allow our predictors to have a wiggly relationship with the target variable. For more information, see this document, but for our purposes, that’s all you really need to know- effects don’t have to be linear even with linear models! We will use the base R mgcv package because it is awesome and you don’t need to install anything. In this case, we’ll allow all the covariates to have a nonlinear relationship, and we denote this with the s() syntax. library(mgcv) happy_model_gam = gam( happiness_score ~ s(democratic_quality) + s(generosity) + s(log_gdp_per_capita) + s(healthy_life_expectancy_at_birth), data = happy_processed ) summary(happy_model_gam) Family: gaussian Link function: identity Formula: happiness_score ~ s(democratic_quality) + s(generosity) + s(log_gdp_per_capita) + s(healthy_life_expectancy_at_birth) Parametric coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.028888 0.008125 -3.555 0.000388 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Approximate significance of smooth terms: edf Ref.df F p-value s(democratic_quality) 8.685 8.972 13.26 &lt;2e-16 *** s(generosity) 6.726 7.870 27.25 &lt;2e-16 *** s(log_gdp_per_capita) 8.893 8.996 87.20 &lt;2e-16 *** s(healthy_life_expectancy_at_birth) 8.717 8.977 65.82 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 R-sq.(adj) = 0.872 Deviance explained = 87.5% GCV = 0.11479 Scale est. = 0.11249 n = 1704 The first thing you may notice is that there are no regression coefficients. This is because the effect of any of these predictors depends on their value, so trying to assess it by a single value would be problematic at best. You can guess what will help us interpret this… library(mgcViz) plot.gamViz(happy_model_gam, allTerms = T) Here is a brief summary of interpretation. We generally don’t have to worry about small wiggles. democratic_quality: Effect is most notable (positive and strong) for higher values. Negligible otherwise. generosity: Effect seems seems strongly positive, but mostly for lower values of generosity. life_expectancy: Effect is positive, but only if the country is around the mean or higher. log GDP per capita: Effect is mostly positive, but may depend on other factors not included in the model. In terms of general model fit, the Scale est. is the same as the residual standard error (squared) in the other models, and is a notably lower than even the model with the interaction (0.11 vs. 0.16). We can also see that the adjusted \\(R^2\\) is higher as well (0.87 vs. 0.82). If we wanted, we can actually do wiggly interactions also! Here is our interaction from before for the GAM case. Let’s check our AIC now to see which model wins. AIC( happy_model_null, happy_model_base, happy_model_more, happy_model_interact, happy_model_gam ) df AIC happy_model_null 2.00000 1272.755 happy_model_base 5.00000 2043.770 happy_model_more 7.00000 1791.237 happy_model_interact 7.00000 1709.801 happy_model_gam 35.02128 1148.417 It’s pretty clear our wiggly model is the winner, even with the added complexity. Note that even though we used a different function for the GAM model, the AIC is still comparable. Model Averaging Have you ever suffered from choice overload? Many folks who seek to understand some phenomenon via modeling do so. There are plenty of choices due to data processing, but then there may be many models to consider as well, and should be if you’re doing things correctly. But you know what? You don’t have to pick a best. Model averaging is a common technique in the Bayesian world and also with some applications of machine learning, but not as widely applied elsewhere, even though it could be. As an example if we (inversely) weight models by the AIC, we can get an average parameter that favors the better models, while not ignoring the lesser models if they aren’t notably poorer. People will use such an approach to get model averaged effects (i.e. coefficients) or predictions. In our setting, the GAM is doing so much better, that it’s weight would basically be 1.0 and zero for the others. So the model averaged predictions would be almost identical to the GAM predictions. model df AIC AICc deltaAICc Rel. Like. weight happy_model_base 5.000 2043.770 2043.805 893.875 0 0 happy_model_more 7.000 1791.237 1791.303 641.373 0 0 happy_model_interact 7.000 1709.801 1709.867 559.937 0 0 happy_model_gam 35.021 1148.417 1149.930 0.000 1 1 Model Criticism Summary Statistical significance with a single model does not provide enough of a story to tell with your data. A better assessment of performance can be made on data the model has not seen, and can provide a better idea of the practical capabilities of it. Furthermore, pitting various models of differing complexities will allow for better confidence in the model or set of models we ultimately deem worthy. In general, in more explanatory settings we strive to balance performance with complexity through various means. Model Criticism Exercises Exercise 0 Recall the google app exercises, we use a standard linear model (i.e. lm) to predict one of three target variables: rating: the user ratings of the app avg_sentiment_polarity: the average sentiment score (positive vs. negative) for the app avg_sentiment_subjectivity: the average subjectivity score (subjective vs. objective) for the app For prediction use the following variables: reviews: number of reviews type: free vs. paid size_in_MB: size of the app in megabytes After that we did a model with an interaction. Either using those models, or running new ones with a different target variable, conduct the following exercises. load(&#39;data/google_apps.RData&#39;) Exercise 1 Assess the model fit and performance of your first model. Perform additional diagnostics to assess how the model is doing (e.g. plot the model to look at residuals). summary(model) plot(model) Exercise 2 Compare the model with the interaction model. Based on AIC or some other metric, which one would you choose? Visualize the interaction model if it’s the better model. anova(model1, model2) AIC(model1, model2) This is often depicted in different ways depending on the text and context. In our depiction, the focus is on the F statistic as a ratio of explained variance to unexplained variance, which naturally relates to the \\(R^2\\) statistic.↩︎ The adjusted \\(R^2\\) does not correct the bias. It only accounts for complexity.↩︎ "],
["ml.html", "Machine Learning Concepts Techniques Interpreting the Black Box Machine Learning Summary Machine Learning Exercises", " Machine Learning Machine learning (ML) encompasses a wide variety of techniques, from standard regression models to almost impenetrably complex modeling tools. While it may seem like magic to the uninitiated, the main thing that distinguishes it from standard statistical methods discussed thus far is an approach that heavily favors prediction over inference and explanatory power, and which takes the necessary steps to gain any predictive advantage37. ML could potentially be applied in any setting, but typically works best with data sets much larger than classical statistical methods are usually applied to. However, nowadays even complex regression models can be applied to extremely large data sets, and properly applied ML may even work in simpler data settings, so this distinction is muddier than it used to be. The main distinguishing factor is mostly one of focus. The following only very briefly provides a demonstration of concepts and approaches. I have more in-depth document available for more details. Concepts Loss We discussed loss functions before, and there was a reason I went more in depth there, mainly because I feel, unlike with ML, loss is not explicitly focused on as much in applied research, leaving the results produced to come across as more magical than it should be. In ML however, we are explicitly concerned with loss functions and, more specifically, evaluating loss on test data. This loss is evaluated over successive iterations of a particular technique, or averaged over several test sets via cross-validation. Typical loss functions are Root Mean Squared Error for numeric targets (essentially the same as for a standard linear model), and cross-entropy for categorical outcomes. There are robust alternatives, such as mean absolute error and hinge loss functions respectively, and many other options besides. You will come across others that might be used for specific scenarios. The following image, typically called a learning curve, shows an example of loss on a test set as a function of model complexity. In this case, models with more complexity perform better, but only to a point, before test error begins to rise again. Bias-variance Tradeoff Prediction error, i.e. loss, is composed of several sources. One part is measurement error, which we can’t do anything about, and two components of specific interest: bias, the difference in the observed value and our average predicted value, and variance how much that prediction would change had we trained on different data. More generally we can think of this as a problem of underfitting vs. overfitting. With a model that is too simple, we underfit, and bias is high. If we overfit, the model is too close to the training data, and likely will do poorly with new observations. ML techniques trade some increased bias for even greater reduced variance, which often means less overfitting to the training data, leading to increased performance on new data. In the following38, the blue line represents models applied to training data, while the red line regards performance on the test set. We can see that for the data we train the model to, error will always go down with increased complexity. However, we can see that at some point, the test error will increase as we have started to overfit to the training data. Regularization As we have noted, a model fit to a single data set might do very well with the data at hand, but then suffer when predicting independent data. Also, oftentimes we are interested in a ‘best’ subset of predictors among a great many, and typically the estimated coefficients from standard approaches are overly optimistic unless dealing with sufficiently large sample sizes. This general issue can be improved by shrinking estimates toward zero, such that some of the performance in the initial fit is sacrificed for improvement with regard to prediction. The basic idea in terms of the tradeoff is that we are trading some bias for notably reduced variance. We demonstrate regularized regression below. Cross-validation Cross-validation is widely used for validation and/or testing. With validation, we are usually concerned with picking parameter settings for the model, while the testing is used for ultimate assessment of model performance. Conceptually there is nothing new beyond what was discussed previously regarding holding out data for assessing predictive performance, we just do more of it. As an example, let’s say we split our data into three parts. We use two parts (combined) as our training data, then the third part as test. At this point this is identical to our demonstration before. But then, we switch which part is test and which two are training, and do the whole thing over again. And finally once more, so that each of our three parts has taken a turn as a test set. Our estimated error is the average loss across the three times. Typically we do it more than three times, usually 10, and there are fancier methods of k-fold cross-validation, though they typically don’t serve to add much value. In any case, let’s try it with our previous example. The following uses the tidymodels approach to be consistent with early chapters use of the tidyverse39. With it we can employ k-fold cross validation to evaluate the loss. # install.packages(tidymodels) # if needed library(tidymodels) load(&#39;data/world_happiness.RData&#39;) set.seed(1212) # specify the model happy_base_spec = linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) # by default 10-folds happy_folds = vfold_cv(happy) library(tune) happy_base_results = fit_resamples( happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, happy_base_spec, happy_folds, control = control_resamples(save_pred = TRUE) ) cv_res = happy_base_results %&gt;% collect_metrics() .metric .estimator mean n std_err rmse standard 0.629 10 0.022 rsq standard 0.697 10 0.022 We now see that our average test error is 0.629. It also gives the average R2. Optimization With ML, much more attention is paid to different optimizers, but the vast majority for deep learning and other many other methods are some flavor of stochastic gradient descent. Often due to the sheer volume of data/parameters, this optimization is done on chunks of the data and in parallel. In general, some optimization approaches may work better in some situations or for some models, where ‘better’ means quicker convergence, or perhaps a smoother ride toward convergence. It is not the case that you would come to incorrect conclusions using one method vs. another per se, just that you might reach those conclusions in a more efficient fashion. The following graphic displays SGD versus several variants40. The x and y axes represent the potential values two parameters might take, with the best selection of those values based on a loss function somewhere toward the bottom right. We can see that they all would get there eventually, but some might do so more quickly. This may or may not be the case for some other data situation. Tuning parameters In any ML setting there are parameters that need to set in order to even run the model. In regularized regression this may be the penalty parameter, for random forests the tree depth, for neural nets, how many hidden units, and many other things. None of these tuning parameters is known beforehand, and so must be tuned, or learned, just like any other. This is usually done with through validation process like k-fold cross validation. The ‘best’ settings are then used to make final predictions on the test set. The usual workflow is something like the following: Tuning: With the training data, use a cross-validation approach to run models with different values for tuning parameters. Model Selection: Select the ‘best’ model as that which minimizes or maximizes the objective function estimated during cross-validation (e.g. RMSE, accuracy, etc.). The test data in this setting are typically referred to as validation sets. Prediction: Use the best model to make predictions on the test set. Techniques Regularized regression A starting point for getting into ML from the more inferential methods is to use regularized regression. These are conceptually no different than standard LM/GLM types of approaches, but they add something to the loss function. \\[\\mathcal{Loss} = \\Sigma(y - \\hat{y})^2 + \\lambda\\cdot\\Sigma\\beta^2\\] In the above, this is the same squared error loss function as before, but we add a penalty that is based on the size of the coefficients. So, while initially our loss goes down with some set of estimates, the penalty based on their size might be such that the estimated loss actually increases. This has the effect of shrinking the estimates toward zero. Well, why would we want that? This introduces bias in the coefficients, but the end result is a model that will do better on test set prediction, which is the goal of the ML approach. The way this works regards the bias-variance tradeoff we discussed previously. The following demonstrates regularized regression using the glmnet package. It actually uses elastic net, which has a mixture of two penalties, one of which is the squared sum of coefficients (typically called ridge regression) and the other is the sum of their absolute values (the so-called lasso). library(tidymodels) happy_prepped = happy %&gt;% select(-country, -gini_index_world_bank_estimate, -dystopia_residual) %&gt;% recipe(happiness_score ~ .) %&gt;% step_scale(everything()) %&gt;% step_naomit(happiness_score) %&gt;% prep() %&gt;% bake(happy) happy_folds = happy_prepped %&gt;% drop_na() %&gt;% vfold_cv() library(tune) happy_regLM_spec = linear_reg(penalty = 1e-3, mixture = .5) %&gt;% set_engine(engine = &quot;glmnet&quot;) happy_regLM_results = fit_resamples( happiness_score ~ ., happy_regLM_spec, happy_folds, control = control_resamples(save_pred = TRUE) ) cv_regLM_res = happy_regLM_results %&gt;% collect_metrics() .metric .estimator mean n std_err rmse standard 0.335 10 0.018 rsq standard 0.897 10 0.013 Tuning parameters For the previous model setting, we wouldn’t know what the penalty or the mixing parameter should be. This is where we can use cross validation to choose those. We’ll redo our model spec, create a set of values to search over, and pass that to the tuning function for cross-validation. Our ultimate model will then be applied to the test data. First we create our training-test split. # removing some variables with lots of missing values happy_split = happy %&gt;% select(-country, -gini_index_world_bank_estimate, -dystopia_residual) %&gt;% initial_split(prop = 0.75) happy_train = training(happy_split) happy_test = testing(happy_split) Next we process the data. This is all specific to the tidymodels approach. happy_prepped = happy_train %&gt;% recipe(happiness_score ~ .) %&gt;% step_knnimpute(everything()) %&gt;% # impute missing values step_center(everything()) %&gt;% # standardize step_scale(everything()) %&gt;% # standardize prep() # prepare for other uses happy_test_normalized &lt;- bake(happy_prepped, new_data = happy_test, all_predictors()) happy_folds = happy_prepped %&gt;% bake(happy) %&gt;% vfold_cv() # now we are indicating we don&#39;t know the value to place happy_regLM_spec = linear_reg(penalty = tune(), mixture = tune()) %&gt;% set_engine(engine = &quot;glmnet&quot;) Now, we need to create a set of values (grid) to try an test. In this case we set the penalty parameter from near zero to near 1, and the mixture parameter a range of values from 0 (ridge regression) to 1 (lasso). grid_search = expand_grid( penalty = exp(seq(-4, -.25, length.out = 10)), mixture = seq(0, 1, length.out = 10) ) regLM_tune = tune_grid( happy_prepped, model = happy_regLM_spec, resamples = happy_folds, grid = grid_search ) autoplot(regLM_tune, metric = &quot;rmse&quot;) + geom_smooth(se = FALSE) best = show_best(regLM_tune, metric = &quot;rmse&quot;, maximize = FALSE, n = 1) # we want to minimize rmse best # A tibble: 1 x 7 penalty mixture .metric .estimator mean n std_err &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 0.0970 1 rmse standard 0.350 10 0.00963 There isn’t a real strong relationship with these settings, except that generally a more ‘lasso-y’ mixture and smaller penalty tends to work better, and this is more or less in keeping with the ‘best’ model. Here is a plot where size indicates RMSE (smaller better) but only for RMSE &lt; .5 (slight jitter added). With the ‘best’ model selected, we can refit to the training data with the parameters in hand. We can then do our usual performance assessment with the test set. # for technical reasons, only mixture is passed to the model; see https://github.com/tidymodels/parsnip/issues/215 tuned_model = linear_reg(penalty = best$penalty, mixture = best$mixture) %&gt;% set_engine(engine = &quot;glmnet&quot;) %&gt;% fit(happiness_score ~ ., data = juice(happy_prepped)) test_predictions = predict(tuned_model, new_data = happy_test_normalized) rmse = yardstick::rmse( data = bind_cols(test_predictions, happy_test), truth = happiness_score, estimate = .pred ) rsq = yardstick::rsq( data = bind_cols(test_predictions, happy_test), truth = happiness_score, estimate = .pred ) .metric .estimator .estimate rmse standard 5.425 rsq standard 0.912 Not too bad! Random Forests A limitation of standard linear models, especially with many input variables, is that there’s not a real automatic way to incorporate interactions and nonlinearities. So we often will want to use techniques that do so. To understand random forests and similar techniques (boosted trees, etc.), we can start with a simple decision tree. To begin, for a single input variable (X1) whose range might be 1 to 10, we find that a cut at 5.75 results in the best classification, such that if all observations greater than or equal to 5.75 are classified as positive, and the rest negative. This general approach is fairly straightforward and conceptually easy to grasp, and it is because of this that tree approaches are appealing. Now let’s add a second input (X2), also on a 1 to 10 range. We now might find that even better classification results if, upon looking at the portion of data regarding those greater than or equal to 5.75, that we only classify positive if they are also less than 3 on the second variable. The following is a hypothetical tree reflecting this. This tree structure allows for both interactions between variables, and nonlinear relationships between some input and the target variable (e.g. the second branch could just be the same X1 but with some cut value greater than 5.75). Random forests randomly select a few from the available input variables, and create a tree that minimizes (maximizes) some loss (objective) function on a validation set. A given tree can potentially be very wide/deep, but instead of just one tree, we now do, say, 1000 trees. A final prediction is made based on the average across all trees. We demonstrate the random forest using the ranger package. We initially don’t do any tuning here, but do note that the number of variables to randomly select (mtry below), the number of total trees, the tree depth - all of these are potential tuning parameters to investigate in the model building process. happy_rf_spec = rand_forest(mode = &#39;regression&#39;, mtry = 6) %&gt;% set_engine(engine = &quot;ranger&quot;) happy_rf_results = fit_resamples( happiness_score ~ ., happy_rf_spec, happy_folds ) cv_rf_res = happy_rf_results %&gt;% collect_metrics() .metric .estimator mean n std_err rmse standard 0.222 10 0.004 rsq standard 0.950 10 0.003 It would appear we’re doing a bit better than the regularized regression. Tuning parameters As mentioned, we’d have a few tuning parameters to play around with. We’ll tune the number of predictors to randomly select per tree, as well as the minimum sample size for each leaf. The following takes the same appraoch as with the regularized regression model. Note that this will take a while (several minutes). grid_search = expand.grid( mtry = c(3, 5, ncol(happy_train)-1), # up to total number of predictors min_n = c(1, 5, 10) ) happy_rf_spec = rand_forest(mode = &#39;regression&#39;, mtry = tune(), min_n = tune()) %&gt;% set_engine(engine = &quot;ranger&quot;) rf_tune = tune_grid( happy_prepped, model = happy_rf_spec, resamples = happy_folds, grid = grid_search ) autoplot(rf_tune, metric = &quot;rmse&quot;) best = show_best(rf_tune, metric = &quot;rmse&quot;, maximize = FALSE, n = 1) # we want to minimize rmse best # A tibble: 1 x 7 mtry min_n .metric .estimator mean n std_err &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 13 10 rmse standard 3.58 10 0.0210 Looks like in general using all the variables for selection is the best (this is in keeping with standard approaches for random forest with regression). Now we are ready to refit the model with the selected tuning parameters and make predictions on the test data. tuned_model = rand_forest(mode = &#39;regression&#39;, mtry = best$mtry, min_n = best$min_n) %&gt;% set_engine(engine = &quot;ranger&quot;) %&gt;% fit(happiness_score ~ ., data = juice(happy_prepped)) test_predictions = predict(tuned_model, new_data = happy_test_normalized) rmse = yardstick::rmse( data = bind_cols(test_predictions, happy_test), truth = happiness_score, estimate = .pred ) rsq = yardstick::rsq( data = bind_cols(test_predictions, happy_test), truth = happiness_score, estimate = .pred ) .metric .estimator .estimate rmse standard 5.437 rsq standard 0.934 Neural Networks Neural networks have been around for a long while as a general concept in artificial intelligence and even as a machine learning algorithm, and often work quite well. In some sense, neural networks can simply be thought of as nonlinear regression. Visually however, we can see them as a graphical model with layers of inputs and outputs. Weighted combinations of the inputs are created41 and put through some function (e.g. the sigmoid function) to produce the next layer of inputs. This next layer goes through the same process to produce either another layer, or to predict the output, or even multiple outputs, which serves as the final layer. All the layers between the input and output are usually referred to as hidden layers. If there were a single hidden layer with a single unit and no transformation, then it becomes the standard regression problem. As a simple example, we can run a simple neural network with a single hidden layer of 1000 units42. Since this is a regression problem, no further transformation is required of the end result to map it onto the target variable. I set the number of epochs to 500, which you can think of as the number of iterations from our discussion of optimization. There are many tuning parameters I am not showing that could certainly be fiddled with as well. This is just an example that will run quickly with comparable performance to the previous. If you do not have keras installed, you can change the engine to nnet, which was a part of the base R set of packages well before neural nets became cool again43. This will likely take several minutes for typical machines. happy_nn_spec = mlp( mode = &#39;regression&#39;, hidden_units = 1000, epochs = 500, activation = &#39;linear&#39; ) %&gt;% set_engine(engine = &quot;keras&quot;) happy_nn_results = fit_resamples( happiness_score ~ ., happy_nn_spec, happy_folds, control = control_resamples(save_pred = TRUE, verbose = FALSE, allow_par = TRUE) ) cv_nn_res = happy_nn_results %&gt;% collect_metrics() .metric .estimator mean n std_err rmse standard 0.290 10 0.007 rsq standard 0.916 10 0.005 You will typically see neural nets applied to image and natural language processing, but as demonstrated above, they can be applied in any scenario. It will take longer to set up and train, but once that’s done, you’re good to go, and may have a much better predictive result. I leave tuning the model as an [exercise][machine-learning-exercises], but definitely switch to using nnet if you do so, otherwise you’ll have to install keras (both for R and Python) and be waiting a long time besides. As mentioned, the nnet package is in base R, so you already have it. Deep Learning Deep learning can be summarized succinctly as ‘very complicated neural nets’. Really, that’s about it. The complexity can be quite tremendous however, and there is a wide variety from which to choose. For example, we just ran a basic neural net above, but for image processing we might use a convolutional neural network, and for natural language processing some LTSM model. Here is a small(!) version of the convolutional neural network known as ‘resnet’ which has many layers in between input and output. The nice thing is that a lot of the work has already been done for you, and you can use models where most of the layers in the neural net have already been trained by people at Google, Facebook, and others who have much more resources to do do so than you. In such cases, you may only have to worry about the last couple layers for your particular problem. Applying a pre-trained model to a different data scenario is called transfer learning, and regardless of what your intuition is, it will work, and very well. Artificial intelligence (AI) used to refer to specific applications of deep/machine learning (e.g. areas in computer vision and natural language processing), but thanks to the popular press, the term has pretty much lost all meaning. AI actually has a very old history dating to the cognitive revolution in psychology and the early days of computer science in the late 50s and early 60s. Again though, you can think of it as a subset of the machine learning problems. Interpreting the Black Box One of the key issues with ML techniques is interpretability. While a decision tree is immensely interpretable, a thousand of them is not so much. What any particular node or even layer in a complex neural network represents may be difficult to fathom. However, we can still interpret prediction changes based on input changes, which is what we really care about, and really is not necessarily more difficult than our standard inferential setting. For example, a regularized regression might not have straightforward inference, but the coefficients are interpreted exactly the same as a standard GLM. Random forests can have the interactions visualized, which is what we said was required for interpretation in standard settings. Furthermore, there are many approaches such as Local Interpretable Model-Agnostic Explanations (LIME), variable importance measures, Shapley values, and more to help us in this process. It might take more work, but honestly, in my consulting experience, a great many have trouble interpreting anything beyond a standard linear model any way, and I’m not convinced that it’s fundamentally different problem to extract meaning from the machine learning context these days, though it may take a little work. Machine Learning Summary Hopefully this has demystified ML for you somewhat. Nowadays it may take little effort to get to state-of-the-art results from even just a year or two ago, and which, for all intents and purposes, would be good enough both now and for the foreseeable future. Despite what many may think, it is not magic, but for more classically statistically minded, it may require a bit of a different perspective. Machine Learning Exercises Exercise 1 Use the ranger package to predict the Google variable rating by several covariates. Feel free to just use the standard function approach rather than all the tidymodels stuff if you want, but do use a training and test approach. You can then try the model again with a different tuning. For the first go around, # run these if needed to load data and install the package # load(&#39;data/google_apps.RData&#39;) # install.packages(&#39;ranger&#39;) google_for_mod = google_apps %&gt;% select(avg_sentiment_polarity, rating, type,installs, reviews, size_in_MB, category) %&gt;% drop_na() google_split = google_for_mod %&gt;% initial_split(prop = 0.75) google_train = training(google_split) google_test = testing(google_split) ga_rf_results = rand_forest(mode = &#39;regression&#39;, mtry = 2, trees = 1000) %&gt;% set_engine(engine = &quot;ranger&quot;) %&gt;% fit( rating ~ ?, google_train ) test_predictions = predict(ga_rf_results, new_data = google_test) rmse = yardstick::rmse( data = bind_cols(test_predictions, google_test), truth = rating, estimate = .pred ) rsq = yardstick::rsq( data = bind_cols(test_predictions, google_test), truth = rating, estimate = .pred ) bind_rows( rmse, rsq ) Exercise 2 Respecify the neural net model demonstrated above as follows, and tune over the number of hidden units to have. This will probably take several minutes depending on your machine. grid_search = expand.grid( hidden_units = c(25, 50), penalty = exp(seq(-4, -.25, length.out = 5)) ) happy_nn_spec = mlp(mode = &#39;regression&#39;, penalty = tune(), hidden_units = tune()) %&gt;% set_engine(engine = &quot;nnet&quot;) nn_tune = tune_grid( happy_prepped, # from previous examples, see tuning for regularized regression model = happy_nn_spec, resamples = happy_folds, # from previous examples, see tuning for regularized regression grid = grid_search ) show_best(nn_tune, metric = &quot;rmse&quot;, maximize = FALSE, n = 1) The ‘machine’ part of the term has historical context, but otherwise has lost meaning for most modern applications, or, at least I really don’t think we need to be referring to computers doing their usual thing as something notable. ‘Learning’ is a poor term that seems to lead many to believe ML is some ‘algorithm’ automatically/magically learning from data without any human input. Nothing could be further from the truth. ML models ‘learn’ from the data as much as a linear regression does.↩︎ This image comes from Elements of Statistical Learning.↩︎ Unfortunately, unlike the tidyverse, tidymodels doesn’t really help me do things more easily. As an alternative, consider the mlr3 approach, which has its own ‘verse’ of packages. I’m not suggesting it’s easier, and it uses [R6][r6] which will take getting used to, but it’s a viable alternative if you want to try an different framework for machine learning.↩︎ This is Alec Radford’s visualization.↩︎ In at least conceptually, and most of the time, literally, in the exact same manner as happens with linear regression.↩︎ For some reason the parsnip package calls this function mlp, which means ‘multilayer’ perceptron, even though only one hidden layer is possible. Something was clearly lost in translation.↩︎ Brian Ripley, one of the early R developers, wrote a seminal text on neural networks in the mid 90s. However, google scholar seems to have forgotten it despite its thousands of citations, I guess because they think it’s written by this guy.↩︎ "],
["ggplot2.html", "ggplot2 Layers Piping Aesthetics Geoms Examples Stats Scales Facets Multiple plots Fine control Themes Extensions Python Summary ggplot2 ggplot2 Exercises", " ggplot2 The most popular visualization package in R is ggplot2. It’s so popular, it or its aesthetic is copied in other languages/programs as well. It entails a grammar of graphics (hence the gg), and learning that grammar is key to using it effectively. Some of the strengths of ggplot2 include: The ease of getting a good looking plot Easy customization A lot of necessary data processing is done for you Clear syntax Easy multidimensional approach Decent default color scheme as a default Lots of extensions Every graph is built from the same few parts, and it’s important to be aware of a few key ideas, which we will cover in turn. Layers (and geoms) Piping Aesthetics Facets Scales Themes Extensions Note that while you can obviously use base R for visualization, it’s never going to be easier, nor as flexible as ggplot2. If you’re used to using base R visuals, you should be prepared to leave them behind. Layers In general, we start with a base layer and add to it. In most cases you’ll start as follows. # recall that starwars is in the dplyr package ggplot(aes(x = height, y = mass), data = starwars) This would just produce a plot background, but nothing else. However, with the foundation in place, we’re now ready to add something to it. Let’s add some points (the outlier is Jabba the Hut). ggplot(aes(x = height, y = mass), data = starwars) + geom_point() Perhaps we want to change labels or theme. These would be additional layers to the plot. ggplot(aes(x = height, y = mass), data = starwars) + geom_point(color = &#39;white&#39;) + labs(x = &#39;Height in cm&#39;, y = &#39;Weight in kg&#39;) + theme_dark() Each layer is consecutively added by means of a pipe operator, and layers may regard geoms, scales, labels, facets etc. You may have many different layers to produce one plot, and there really is no limit. However some efficiencies may be possible for a given situation. For example, it’s more straightforward to use geom_smooth than calculate fits, standard errors etc. and then add multiple geoms to produce the same thing. This is the sort of thing you’ll get used to as you use ggplot more. Piping As we saw, layers are added via piping (+). The first layers added after the base are typically geoms, or geometric objects that represent the data, and include things like: points lines density text In case you’re wondering why ggplot doesn’t use %&gt;% as in the tidyverse and other visualization packages, it’s because ggplot2 was using pipes before it was cool, well before those came along. Otherwise, the concept is the same as we saw in the data processing section. ggplot(aes(x = myvar, y = myvar2), data = mydata) + geom_point() Our base is provided via the ggplot function, and specifies the data at the very least, but commonly also the x and y aesthetics. The geom_point function adds a layer of points, and now we would have a scatterplot. Alternatively, you could have specified the x and y aesthetic at the geom_point layer, but if you’re going to have the same x, y, color, etc. aesthetics regardless of layer, put it in the base. Otherwise, doing it by layer gives you more flexibility if needed. Geoms even have their own data argument, allowing you to combine information from several sources for a single visualization. Aesthetics Aesthetics map data to various visual aspects of the plot, including size, color etc. The function used in ggplot to do this is aes. aes( x = myvar, y = myvar2, color = myvar3, group = g ) The best way to understand what goes into the aes function is if the value is varying. For example, if I want the size of points to be a certain value, I would code the following. ... + geom_point(..., size = 4) However, if I want the size to be associated with the data in some way, I use it as an aesthetic. ... + geom_point(aes(size = myvar)) The same goes for practically any aspect of a geom- size, color, fill, etc. If it is a fixed value, set it outside the aesthetic. If it varies based on the data, put it within an aesthetic. Geoms In the ggplot2 world, geoms are the geometric objects- shapes, lines, and other parts of the visualization we want to display. Even if you use ggplot2 a lot, you probably didn’t know about many or most of these. geom_abline: Reference lines: horizontal, vertical, and diagonal geom_area: Ribbons and area plots geom_bar: Bar charts geom_bin2d: Heatmap of 2d bin counts geom_blank: Draw nothing geom_boxplot: A box and whiskers plot (in the style of Tukey) geom_col: Bar charts geom_contour: 2d contours of a 3d surface geom_count: Count overlapping points geom_crossbar: Vertical intervals: lines, crossbars &amp; errorbars geom_curve: Line segments and curves geom_density: Smoothed density estimates geom_density_2d: Contours of a 2d density estimate geom_dotplot: Dot plot geom_errorbar: Vertical intervals: lines, crossbars &amp; errorbars geom_errorbarh: Horizontal error bars geom_freqpoly: Histograms and frequency polygons geom_hex: Hexagonal heatmap of 2d bin counts geom_histogram: Histograms and frequency polygons geom_hline: Reference lines: horizontal, vertical, and diagonal geom_jitter: Jittered points geom_label: Text geom_line: Connect observations geom_linerange: Vertical intervals: lines, crossbars &amp; errorbars geom_map: Polygons from a reference map geom_path: Connect observations geom_point: Points geom_pointrange: Vertical intervals: lines, crossbars &amp; errorbars geom_polygon: Polygons geom_qq: A quantile-quantile plot geom_qq_line: A quantile-quantile plot geom_quantile: Quantile regression geom_raster: Rectangles geom_rect: Rectangles geom_ribbon: Ribbons and area plots geom_rug: Rug plots in the margins geom_segment: Line segments and curves geom_sf: Visualise sf objects geom_smooth: Smoothed conditional means geom_spoke: Line segments parameterised by location, direction and distance geom_step: Connect observations geom_text: Text geom_tile: Rectangles geom_violin: Violin plot geom_vline: Reference lines: horizontal, vertical, and diagonal Examples Let’s get more of a feel for things by seeing some examples that demonstrate some geoms and aesthetics. To begin, after setting the base aesthetic, we’ll set some explicit values for the geom. library(ggplot2) data(&quot;diamonds&quot;) data(&#39;economics&#39;) ggplot(aes(x = carat, y = price), data = diamonds) + geom_point(size = .5, color = &#39;peru&#39;) Next we use two different geoms, one using a different data source. Note that geoms often have arguments specific to them, as label is for geom_text. It would be ignored by geom_line. ggplot(aes(x = date, y = unemploy), data = economics) + geom_line() + geom_text( aes(label = unemploy), vjust = -.5, data = filter(economics, date == &#39;2009-10-01&#39;) ) In the following, one setting, alpha (transparency), is not mapped to the data, while size and color are44. ggplot(aes(x = carat, y = price), data = diamonds) + geom_point(aes(size = carat, color = clarity), alpha = .05) Stats There are many statistical functions built in, and it is a key strength of ggplot that you don’t have to do a lot of processing for very common plots. Quantile regression lines: ggplot(mpg, aes(x = displ, y = hwy)) + geom_point() + geom_quantile() Loess (or additive model) smooth. This shows how we can do some fine-tuning and use model-based approaches for visualization. data(mcycle, package = &#39;MASS&#39;) ggplot(aes(x = times, y = accel), data = mcycle) + geom_point() + geom_smooth(formula = y ~ s(x, bs = &#39;ad&#39;), method = &#39;gam&#39;) Bootstrapped confidence intervals: ggplot(mtcars, aes(cyl, mpg)) + geom_point() + stat_summary( fun.data = &quot;mean_cl_boot&quot;, colour = &quot;orange&quot;, alpha = .75, size = 1 ) The take-home message here is to always let ggplot do the work for you if at all possible. However, I will say that I find it easier to create the summary data I want to visualize with tidyverse tools, rather than use stat_summary, and you may have a similar experience. Scales Often there are many things we want to change about the plot, for example, the size and values of axis labels, the range of sizes for points to take, the specific colors we want to use, and so forth. Be aware that there are a great many options here, and you will regularly want to use them. A very common thing you’ll do is change the labels for the axes. You definitely don’t have to go and change the variable name itself to do this, just use the labs function. There are also functions for individual parts, e.g. xlab, ylab and ggtitle. ggplot(aes(x = times, y = accel), data = mcycle) + geom_smooth(se = FALSE) + labs( x = &#39;milliseconds after impact&#39;, y = &#39;head acceleration&#39;, title = &#39;Motorcycle Accident&#39; ) A frequent operation is changing the x and y look in the form of limits and tick marks. Like labs, there is a general lims function and specific functions for just the specific parts. In addition, we may want to get really detailed using scale_x_* or scale_y_*. ggplot(mpg, aes(x = displ, y = hwy, size = cyl)) + geom_point() + ylim(c(0, 60)) ggplot(mpg, aes(x = displ, y = hwy, size = cyl)) + geom_point() + scale_y_continuous( limits = c(0, 60), breaks = seq(0, 60, by = 12), minor_breaks = seq(6, 60, by = 6) ) Another common option is to change the size of points in some way. While we assign the aesthetic as before, it comes with defaults that might not work for a given situation. Play around with the range values. ggplot(mpg, aes(x = displ, y = hwy, size = cyl)) + geom_point() + scale_size(range = c(1, 3)) Now that you know more about color issues, you may want to apply something besides the default options. The following shows a built-in color scale for a color aesthetic that is treated as continuous, and one that is discrete and which we want to supply our own colors (these actually come from plotly’s default color scheme). ggplot(mpg, aes(x = displ, y = hwy, color = cyl)) + geom_point() + scale_color_gradient2() ggplot(mpg, aes(x = displ, y = hwy, color = factor(cyl))) + geom_point() + scale_color_manual(values = c(&quot;#1f77b4&quot;, &quot;#ff7f0e&quot;, &quot;#2ca02c&quot;, &quot;#d62728&quot;)) We can even change the scale of the data itself. ggplot(mpg, aes(x = displ, y = hwy)) + geom_point() + scale_x_log10() In short, scale alterations are really useful for getting just the plot you want, and there is a lot of flexibility for you to work with. There are a lot of scales too, so know what you have available. scale_alpha, scale_alpha_continuous, scale_alpha_date, scale_alpha_datetime, scale_alpha_discrete, scale_alpha_identity, scale_alpha_manual, scale_alpha_ordinal: Alpha transparency scales scale_color_brewer, scale_color_distiller: Sequential, diverging and qualitative colour scales from colorbrewer.org scale_color_continuous, scale_color_discrete, scale_color_gradient, scale_color_gradient2, scale_color_gradientn, scale_color_grey, scale_color_hue, scale_color_identity, scale_color_manual, scale_color_viridis_c, scale_color_viridis_d, scale_continuous_identity Various color scales scale_discrete_identity, scale_discrete_manual: Discrete scales scale_fill_brewer, scale_fill_continuous, scale_fill_date, scale_fill_datetime, scale_fill_discrete, scale_fill_distiller, scale_fill_gradient, scale_fill_gradient2, scale_fill_gradientn, scale_fill_grey, scale_fill_hue, scale_fill_identity, scale_fill_manual, scale_fill_ordinal, scale_fill_viridis_c, scale_fill_viridis_d: Scales for geoms that can be filled with color scale_linetype, scale_linetype_continuous, scale_linetype_discrete, scale_linetype_identity, scale_linetype_manual: Scales for line patterns scale_shape, scale_shape_continuous, scale_shape_discrete, scale_shape_identity, scale_shape_manual, scale_shape_ordinal: Scales for shapes, aka glyphs scale_size, scale_size_area, scale_size_continuous, scale_size_date, scale_size_datetime, scale_size_discrete, scale_size_identity, scale_size_manual, scale_size_ordinal: Scales for area or radius scale_x_continuous, scale_x_date, scale_x_datetime, scale_x_discrete, scale_x_log10, scale_x_reverse, scale_x_sqrt, &lt; scale_y_continuous, scale_y_date, scale_y_datetime, scale_y_discrete, scale_y_log10, scale_y_reverse, scale_y_sqrt: Position scales for continuous data (x &amp; y) scale_x_time, scale_y_time: Position scales for date/time data Facets Facets allow for paneled display, a very common operation. In general, we often want comparison plots. The facet_grid function will produce a grid, and often this is all that’s needed. However, facet_wrap is more flexible, while possibly taking a bit extra to get things just the way you want. Both use a formula approach to specify the grouping. facet_grid Facet by cylinder. ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + facet_grid(~ cyl) Facet by vs and cylinder. ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + facet_grid(vs ~ cyl, labeller = label_both) facet_wrap Specify the number of columns or rows with facet_wrap. ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + facet_wrap(vs ~ cyl, labeller = label_both, ncol=2) Multiple plots Often we want distinct visualizations to come together in one plot. There are several packages that can help you here: gridExtra, cowplot, and more recently patchwork45. The latter especially makes things easy. library(patchwork) g1 = ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() g2 = ggplot(mtcars, aes(wt)) + geom_density() g3 = ggplot(mtcars, aes(mpg)) + geom_density() g1 / # initial plot, place next part underneath (g2 | g3) # groups g2 and g3 side by side Not that you want this, but just to demonstrate the flexibility. p1 &lt;- ggplot(mtcars) + geom_point(aes(mpg, disp)) p2 &lt;- ggplot(mtcars) + geom_boxplot(aes(gear, disp, group = gear)) p3 &lt;- ggplot(mtcars) + geom_smooth(aes(disp, qsec)) p4 &lt;- ggplot(mtcars) + geom_bar(aes(carb)) p5 &lt;- ggplot(mtcars) + geom_violin(aes(cyl, mpg, group = cyl)) p1 + p2 + (p3 + p4 + plot_layout(ncol = 1) ) + p5 + plot_layout(widths = c(2, 1)) You’ll typically want to use facets to show subsets of the same data, and tools like patchwork to show different kinds of plots together. Fine control ggplot2 makes it easy to get good looking graphs quickly. However the amount of fine control is extensive. The following plot is hideous (aside from the background, which is totally rad), but illustrates the point. ggplot(aes(x = carat, y = price), data = diamonds) + annotation_custom( rasterGrob( lambosun, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;), interpolate = FALSE ),-Inf, Inf, -Inf, Inf ) + geom_point(aes(color = clarity), alpha = .5) + scale_y_log10(breaks = c(1000, 5000, 10000)) + xlim(0, 10) + scale_color_brewer(type = &#39;div&#39;) + facet_wrap( ~ cut, ncol = 3) + theme_minimal() + theme( axis.ticks.x = element_line(color = &#39;darkred&#39;), axis.text.x = element_text(angle = -45), axis.text.y = element_text(size = 20), strip.text = element_text(color = &#39;forestgreen&#39;), strip.background = element_blank(), panel.grid.minor = element_line(color = &#39;lightblue&#39;), legend.key = element_rect(linetype = 4), legend.position = &#39;bottom&#39; ) Themes In the last example you saw two uses of a theme- a built-in version that comes with ggplot (theme_minimal), and specific customization (theme(…)). The built-in themes provide ready-made approaches that might already be enough for a finished product. For the theme function, each argument, and there are many, takes on a specific value or an element function: element_rect element_line element_text element_blank Each of those element functions has arguments specific to it. For example, for element_text you can specify the font size, while for element line you could specify the line type. Note that the base theme of ggplot, and I would say every plotting package, is probably going to need manipulation before a plot is ready for presentation. For example, the ggplot theme doesn’t work well for web presentation, and is even worse for print. You will almost invariably need to tweak it. I suggest using and saving your own custom theme for easy application for any visualization package you use frequently. Extensions ggplot2 now has its own extension system, and there is even a website to track the extensions. Examples include: additional themes maps interactivity animations marginal plots network graphs time series aligning multiple ggplot visualizations, possibly of different types Here’s an example with gganimate. library(gganimate) load(&#39;data/gapminder.RData&#39;) gap_plot = gapminder_2019 %&gt;% filter(giniPercap != 40) gap_plot_filter = gap_plot %&gt;% filter(country %in% c(&#39;United States&#39;, &#39;Mexico&#39;, &#39;Canada&#39;)) initial_plot = ggplot(gap_plot, aes(x = year, y = giniPercap, group = country)) + geom_line(alpha = .05) + geom_path( aes(color = country), lwd = 2, arrow = arrow( length = unit(0.25, &quot;cm&quot;) ), alpha = .5, data = gap_plot_filter, show.legend = FALSE ) + geom_text( aes(color = country, label = country), nudge_x = 5, nudge_y = 2, size = 2, data = gap_plot_filter, show.legend = FALSE ) + theme_clean() + transition_reveal(year) animate(initial_plot, end_pause = 50, nframes = 150, rewind = TRUE) As one can see, ggplot2 is only the beginning. You’ll have a lot of tools at your disposal. Furthermore, many modeling and other packages will produce ggplot graphics to which you can add your own layers and tweak like you would any other ggplot. Python The R community really lucked out with ggplot, and the basic philosophy behind it is missing from practically every other static plotting packages or tools. Python’s version of base R plotting is matplotlib, which continues to serve people well. But like R base plots, it can take a lot of work to get anything remotely visually appealing. Seaborn is another option, but still, just isn’t in the same league. If using Python though, you’re in luck! You get most of the basic functionality of ggplot2 via the plotnine module. A jupyter notebook demonstrating most of the previous is available here. import pandas as pd from plotnine import * from plotnine.data import mtcars ( ggplot(mtcars, aes(x = &#39;wt&#39;, y = &#39;mpg&#39;)) + geom_point(aes(color = &#39;factor(gear)&#39;)) + stat_smooth(method = &#39;lm&#39;) + facet_wrap(&#39;~ cyl&#39;) ) &lt;ggplot: (-9223372029681066422)&gt; /Users/micl/anaconda3/lib/python3.6/site-packages/plotnine/scales/scale.py:93: MatplotlibDeprecationWarning: The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. if cbook.iterable(self.breaks) and cbook.iterable(self.labels): /Users/micl/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead. return ptp(axis=axis, out=out, **kwargs) /Users/micl/anaconda3/lib/python3.6/site-packages/plotnine/utils.py:553: MatplotlibDeprecationWarning: The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. return cbook.iterable(var) and not is_string(var) Summary ggplot2 ggplot2 is an easy to use, but powerful visualization tool. It allows one to think in many dimensions for any graph, and extends well beyond the basics. Use it to easily create more interesting visualizations. ggplot2 Exercises Exercise 0 Load the ggplot2 package if you haven’t already. Exercise 1 Create two plots, one a scatterplot (e.g. with geom_point) and one with lines (e.g. geom_line) with a data set of your choosing (all of the following are base R or available after loading ggplot2. Some suggestions: faithful: Waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. msleep: mammals sleep dataset with sleep times and weights etc. diamonds: used in the slides economics: US economic time series. txhousing: Housing sales in TX. midwest: Midwest demographics. mpg: Fuel economy data from 1999 and 2008 for 38 popular models of car Recall the basic form for ggplot. ggplot(aes(x=*, y=*, other), data=*) + geom_*() + otherLayers, theme etc. Themes to play with: theme_bw theme_classic theme_dark theme_gray theme_light theme_linedraw theme_minimal theme_clean (requires the visibly package and an appreciation of the Lamborghini background from the previous visualization) Exercise 2 Play around and change the arguments to the following. You’ll need to install the maps package. For example, do points for all county midpoints. For that you’d need to change the x and y for the point geom to an aesthetic based on the longitude and latitude, as well as add its data argument to use the seats data frame. Make the color of the points or text based on subregion. This will require adding the fill argument to the polygon geom and removing the NA setting. In addition, add the argument show.legend=F (outside the aesthetic), or you’ll have a problematic legend (recall what we said before about too many colors!). Try making color based on subregion too. See if you can use element_blank on a theme argument to remove the axis information. See ?theme for ideas. library(maps) mi = map_data(&quot;county&quot;, &quot;michigan&quot;) seats = mi %&gt;% group_by(subregion) %&gt;% summarise_at(vars(lat, long), function(x) median(range(x))) # inspect the data # head(mi) # head(seats) ggplot(mi, aes(long, lat)) + geom_polygon(aes(group = subregion), fill = NA, colour = &quot;grey60&quot;) + geom_text(aes(label = subregion), data = seats, size = 1, angle = 45) + geom_point(x=-83.748333, y=42.281389, color=&#39;#1e90ff&#39;, size=3) + theme_minimal() + theme(panel.grid=element_blank()) The reason the legend is hard to make out is because the transparency is mapped to the colors and size. While I personally have never come across this being a desirable situation, nor can fathom why it would be the default, it can be fixed via + guides(colour = guide_legend(override.aes = list(alpha = 1)), size = guide_legend(override.aes = list(alpha = 1))). Apparently because that’s fun to have to do every time and easy to remember.↩︎ The author of patchwork is the same as that for the scico package. He also provides gganimate, lime and other useful tools.↩︎ "],
["interactive.html", "Interactive Visualization Packages Piping for Visualization htmlwidgets Plotly Highcharter Graph networks leaflet DT Shiny Interactive and Visual Data Exploration Interactive Visualization Exercises", " Interactive Visualization Packages As mentioned, ggplot2 is the most widely used package for visualization in R. However, it is not interactive by default. Many packages use htmlwidgets, d3 (JavaScript library), and other tools to provide interactive graphics. What’s great is that while you may have to learn new packages, you don’t necessarily have to change your approach or thinking about a plot, or learn some other language. Many of these packages can be lumped into more general packages that try to provide a plotting system (similar to ggplot2), versus those that just aim to do a specific type of plot well. Here are some to give a sense of this. General (click to visit the associated website): plotly - used also in Python, Matlab, Julia - can convert ggplot2 images to interactive ones (with varying degrees of success) highcharter also very general wrapper for highcharts.js and works with some R packages out of the box rbokeh like plotly, it also has cross language support Specific functionality: DT interactive data tables leaflet maps with OpenStreetMap visNetwork Network visualization In what follows we’ll see some of these in action. Note that unlike the previous chapter, the goal here is not to dive deeply, but just to get an idea of what’s available. Piping for Visualization One of the advantages to piping is that it’s not limited to dplyr style data management functions. Any R function can be potentially piped to, and several examples have already been shown. Many newer visualization packages take advantage of piping, and this facilitates data exploration. We don’t have to create objects just to do a visualization. New variables can be easily created and subsequently manipulated just for visualization. Furthermore, data manipulation not separated from visualization. htmlwidgets The htmlwidgets package makes it easy to create visualizations based on JavaScript libraries. If you’re not familiar with JavaScript, you actually are very familiar with its products, as it’s basically the language of the web, visual or otherwise. The R packages using it typically are pipe-oriented and produce interactive plots. In addition, you can use the htmlwidgets package to create your own functions that use a particular JavaScript library (but someone probably already has, so look first). Plotly We’ll begin our foray into the interactive world with a couple demonstrations of plotly. To give some background, you can think of plotly similar to RStudio, in that it has both enterprise (i.e. pay for) aspects and open source aspects. Just like RStudio, you have full access to what it has to offer via the open source R package. You may see old help suggestions referring to needing an account, but this is no longer necessary. When using plotly, you’ll note the layering approach similar to what we had with ggplot2. Piping is used before plotting to do some data manipulation, after which we seamlessly move to the plot itself. The =~ is essentially the way we denote aesthetics46. Plotly is able to be used in both R and Python. R library(plotly) midwest %&gt;% filter(inmetro == T) %&gt;% plot_ly(x = ~ percbelowpoverty, y = ~ percollege) %&gt;% add_markers() Python The following does the same plot in Python import pandas as pd import plotly.express as px midwest = pd.DataFrame(r.midwest) # from previous chunk using reticulate plt = px.scatter(midwest, x = &#39;percbelowpoverty&#39;, y = &#39;percollege&#39;) plt.show() # opens in browser Modes plotly has modes, which allow for points, lines, text and combinations. Traces, add_*, work similar to geoms. library(mgcv) library(modelr) library(glue) mtcars %&gt;% mutate( amFactor = factor(am, labels = c(&#39;auto&#39;, &#39;manual&#39;)), hovertext = glue(&#39;weight: {wt} &lt;br&gt; mpg: {mpg} &lt;br&gt; {amFactor}&#39;) ) %&gt;% add_predictions(gam(mpg ~ s(wt, am, bs = &#39;fs&#39;), data = mtcars)) %&gt;% arrange(am) %&gt;% plot_ly() %&gt;% add_markers( x = ~ wt, y = ~ mpg, color = ~ amFactor, opacity = .5, text = ~ hovertext, hoverinfo = &#39;text&#39;, showlegend = F ) %&gt;% add_lines( x = ~ wt, y = ~ pred, color = ~ amFactor ) While you can use plotly as a one-liner47, this would only be good for quick peeks while doing data exploration. It would generally be far too limiting otherwise. plot_ly(midwest, x = ~percollege, color = ~state, type = &quot;box&quot;) And here is a Python example or two using plotly express. plt = px.box(midwest, x = &#39;state&#39;, y = &#39;percollege&#39;, color = &#39;state&#39;, notched=True) plt.show() # opens in browser tips = px.data.tips() # built-in dataset px.violin( tips, y = &quot;tip&quot;, x = &quot;smoker&quot;, color = &quot;sex&quot;, box = True, points = &quot;all&quot;, hover_data = tips.columns ).show() ggplotly One of the strengths of plotly is that we can feed a ggplot object to it, and turn our formerly static plots into interactive ones. It would have been easy to use geom_smooth to get a similar result, so let’s do so. gp = mtcars %&gt;% mutate(amFactor = factor(am, labels = c(&#39;auto&#39;, &#39;manual&#39;)), hovertext = paste(wt, mpg, amFactor)) %&gt;% arrange(wt) %&gt;% ggplot(aes(x = wt, y = mpg, color = amFactor)) + geom_smooth(se = F) + geom_point(aes(color = amFactor)) ggplotly() Note that this is not a one-to-one transformation. The plotly image will have different line widths and point sizes. It will usually be easier to change it within the ggplot process than tweaking the ggplotly object. Be prepared to spend time getting used to plotly. It has (in my opinion) poor documentation, is not nearly as flexible as ggplot2, has hidden (and arbitrary) defaults that can creep into a plot based on aspects of the data (rather than your settings), and some modes do not play nicely with others. That said, it works great for a lot of things, and I use it regularly. Highcharter Highcharter is also fairly useful for a wide variety of plots, and is based on the highcharts.js library. If you have data suited to one of its functions, getting a great interactive plot can be ridiculously easy. In what follows we use quantmod to create an xts (time series) object of Google’s stock price, including opening and closing values. The highcharter object has a ready-made plot for such data48. library(highcharter) library(quantmod) google_price = getSymbols(&quot;GOOG&quot;, auto.assign = FALSE) hchart(google_price) Graph networks visNetwork The visNetwork package is specific to network visualizations and similar, and is based on the vis.js library. Networks require nodes and edges to connect them. These take on different aspects, and so are created in separate data frames. set.seed(1352) nodes = data.frame(id = 0:5, label = c(&#39;Bobby&#39;, &#39;Janie&#39;,&#39;Timmie&#39;, &#39;Mary&#39;, &#39;Johnny&#39;, &#39;Billy&#39;), group = c(&#39;friend&#39;, &#39;frenemy&#39;,&#39;frenemy&#39;, rep(&#39;friend&#39;, 3)), value = sample(10:50, 6)) edges = data.frame(from = c(0,0,0,1,1,2,2,3,3,3,4,5,5), to = sample(0:5, 13, replace = T), value = sample(1:10, 13, replace = T)) %&gt;% filter(from!=to) library(visNetwork) visNetwork(nodes, edges, height=300, width=800) %&gt;% visNodes(shape=&#39;circle&#39;, font=list(), scaling=list(min=10, max=50, label=list(enable=T))) %&gt;% visLegend() sigmajs The sigmajs package allows one to use the corresponding JS library to create some clean and nice visualizations for graphs. The following creates library(sigmajs) nodes &lt;- sg_make_nodes(30) edges &lt;- sg_make_edges(nodes) # add transitions n &lt;- nrow(nodes) nodes$to_x &lt;- runif(n, 5, 10) nodes$to_y &lt;- runif(n, 5, 10) nodes$to_size &lt;- runif(n, 5, 10) nodes$to_color &lt;- sample(c(&quot;#ff5500&quot;, &quot;#00aaff&quot;), n, replace = TRUE) sigmajs() %&gt;% sg_nodes(nodes, id, label, size, color, to_x, to_y, to_size, to_color) %&gt;% sg_edges(edges, id, source, target) %&gt;% sg_animate( mapping = list( x = &quot;to_x&quot;, y = &quot;to_y&quot;, size = &quot;to_size&quot;, color = &quot;to_color&quot; ), delay = 0 ) %&gt;% sg_settings(animationsTime = 3500) %&gt;% sg_button(&quot;animate&quot;, # button label &quot;animate&quot;, # event name class = &quot;btn btn-warning&quot;) animate Plotly I mention plotly capabilities here as again, it may be useful to stick to one tool that you can learn well, and again, could allow you to bounce to python as well. import plotly.graph_objects as go import networkx as nx G = nx.random_geometric_graph(50, 0.125) edge_x = [] edge_y = [] for edge in G.edges(): x0, y0 = G.nodes[edge[0]][&#39;pos&#39;] x1, y1 = G.nodes[edge[1]][&#39;pos&#39;] edge_x.append(x0) edge_x.append(x1) edge_x.append(None) edge_y.append(y0) edge_y.append(y1) edge_y.append(None) edge_trace = go.Scatter( x=edge_x, y=edge_y, line=dict(width=0.5, color=&#39;#888&#39;), hoverinfo=&#39;none&#39;, mode=&#39;lines&#39;) node_x = [] node_y = [] for node in G.nodes(): x, y = G.nodes[node][&#39;pos&#39;] node_x.append(x) node_y.append(y) node_trace = go.Scatter( x=node_x, y=node_y, mode=&#39;markers&#39;, hoverinfo=&#39;text&#39;, marker=dict( showscale=True, colorscale=&#39;Blackbody&#39;, reversescale=True, color=[], size=10, colorbar=dict( thickness=15, title=&#39;Node Connections&#39;, xanchor=&#39;left&#39;, titleside=&#39;right&#39; ), line_width=2)) node_adjacencies = [] node_text = [] for node, adjacencies in enumerate(G.adjacency()): node_adjacencies.append(len(adjacencies[1])) node_text.append(&#39;# of connections: &#39;+str(len(adjacencies[1]))) node_trace.marker.color = node_adjacencies node_trace.text = node_text fig = go.Figure(data=[edge_trace, node_trace], layout=go.Layout( title=&#39;&lt;br&gt;Network graph made with Python&#39;, titlefont_size=16, showlegend=False, hovermode=&#39;closest&#39;, margin=dict(b=20,l=5,r=5,t=40), annotations=[ dict( text=&quot;Python code: &lt;a href=&#39;https://plot.ly/ipython-notebooks/network-graphs/&#39;&gt; https://plot.ly/ipython-notebooks/network-graphs/&lt;/a&gt;&quot;, showarrow=False, xref=&quot;paper&quot;, yref=&quot;paper&quot;, x=0.005, y=-0.002 ) ], xaxis=dict(showgrid=False, zeroline=False, showticklabels=False), yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)) ) fig.show() leaflet The leaflet package from RStudio is good for quick interactive maps, and it’s quite flexible and has some nice functionality to take your maps further. Unfortunately, it actually doesn’t always play well with many markdown formats. hovertext &lt;- paste(sep = &quot;&lt;br/&gt;&quot;, &quot;&lt;b&gt;&lt;a href=&#39;http://umich.edu/&#39;&gt;University of Michigan&lt;/a&gt;&lt;/b&gt;&quot;, &quot;Ann Arbor, MI&quot; ) library(leaflet) leaflet() %&gt;% addTiles() %&gt;% addPopups(lng = -83.738222, lat = 42.277030, popup = hovertext) DT It might be a bit odd to think of data frames visually, but they can be interactive also. One can use the DT package for interactive data frames. This can be very useful when working in collaborative environments where one shares reports, as you can embed the data within the document itself. library(DT) ggplot2movies::movies %&gt;% select(1:6) %&gt;% filter(rating &gt; 8, !is.na(budget), votes &gt; 1000) %&gt;% datatable() The other thing to be aware of is that tables can be visual, it’s just that many academic outlets waste this opportunity. Simple bolding, italics, and even sizing, can make results pop more easily for the audience. The DT package allows for coloring and even simple things like bars that connotes values. The following gives some idea of its flexibility. iris %&gt;% # arrange(desc(Petal.Length)) %&gt;% datatable(rownames = F, options = list(dom = &#39;firtp&#39;), class = &#39;row-border&#39;) %&gt;% formatStyle(&#39;Sepal.Length&#39;, fontWeight = styleInterval(5, c(&#39;normal&#39;, &#39;bold&#39;))) %&gt;% formatStyle(&#39;Sepal.Width&#39;, color = styleInterval(c(3.4, 3.8), c(&#39;#7f7f7f&#39;, &#39;#00aaff&#39;, &#39;#ff5500&#39;)), backgroundColor = styleInterval(3.4, c(&#39;#ebebeb&#39;, &#39;aliceblue&#39;))) %&gt;% formatStyle( &#39;Petal.Length&#39;, # color = &#39;transparent&#39;, background = styleColorBar(iris$Petal.Length, &#39;#5500ff&#39;), backgroundSize = &#39;100% 90%&#39;, backgroundRepeat = &#39;no-repeat&#39;, backgroundPosition = &#39;center&#39; ) %&gt;% formatStyle( &#39;Species&#39;, color = &#39;white&#39;, transform = &#39;rotateX(45deg) rotateY(20deg) rotateZ(30deg)&#39;, backgroundColor = styleEqual(unique(iris$Species), c(&#39;#1f65b7&#39;, &#39;#66b71f&#39;, &#39;#b71f66&#39;)) ) I would in no way recommend using the bars, unless the you want a visual instead of the value and can show all possible values. I would not recommend angled tag options at all, as that is more or less a prime example of chartjunk. However, subtle use of color and emphasis, as with the Sepal columns, can make tables of results that your audience will actually spend time exploring. Shiny Shiny is a framework that can essentially allow you to build an interactive website/app. Like some of the other packages mentioned, it’s provided by RStudio developers. However, most of the more recently developed interactive visualization packages will work specifically within the shiny and rmarkdown setting. You can make shiny apps just for your own use and run them locally. But note, you are using R, a statistical programming language, to build a webpage, and it’s not necessarily particularly well-suited for it. Much of how you use R will not be useful in building a shiny app, and so it will definitely take some getting used to, and you will likely need to do a lot of tedious adjustments to get things just how you want. Shiny apps have two main components, a part that specifies the user interface, and a server function that will do all the work. With those in place (either in a single ‘app.R’ file or in separate files), you can then simply click run app or use the function. This example is taken from the shiny help file, and you can actually run it as is. library(shiny) # Running a Shiny app object app &lt;- shinyApp( ui = bootstrapPage( numericInput(&#39;n&#39;, &#39;Number of obs&#39;, 10), plotOutput(&#39;plot&#39;) ), server = function(input, output) { output$plot &lt;- renderPlot({ ggplot2::qplot(rnorm(input$n), xlab = &#39;Is this normal?!&#39;) }) } ) runApp(app) You can share your app code/directory with anyone and they’ll be able to run it also. However, this is great mostly just for teaching someone how to do shiny, which most people aren’t going to do. Typically you’ll want someone to use the app itself, not run code. In that case you’ll need a web server. You can get up to 5 free ‘running’ applications at shinyapps.io. However, you will notably be limited in the amount of computing resources that can be used to run the apps in a given month. Even minor usage of those could easily overtake the free settings. For personal use it’s plenty though. Dash Dash is a similar approach to interactivity as Shiny brought to you by the plotly gang. The nice thing about it is crossplatform support for R and Python. R library(dash) library(dashCoreComponents) library(dashHtmlComponents) app &lt;- Dash$new() df &lt;- readr::read_csv(file = &quot;data/gapminder_small.csv&quot;) %&gt;% drop_na() continents &lt;- unique(df$continent) data_gdp_life &lt;- with(df, lapply(continents, function(cont) { list( x = gdpPercap[continent == cont], y = lifeExp[continent == cont], opacity=0.7, text = country[continent == cont], mode = &#39;markers&#39;, name = cont, marker = list(size = 15, line = list(width = 0.5, color = &#39;white&#39;)) ) } ) ) app$layout( htmlDiv( list( dccGraph( id = &#39;life-exp-vs-gdp&#39;, figure = list( data = data_gdp_life, layout = list( xaxis = list(&#39;type&#39; = &#39;log&#39;, &#39;title&#39; = &#39;GDP Per Capita&#39;), yaxis = list(&#39;title&#39; = &#39;Life Expectancy&#39;), margin = list(&#39;l&#39; = 40, &#39;b&#39; = 40, &#39;t&#39; = 10, &#39;r&#39; = 10), legend = list(&#39;x&#39; = 0, &#39;y&#39; = 1), hovermode = &#39;closest&#39; ) ) ) ) ) ) app$run_server() Python Here is a python example. Save as app.py then at the terminal run python app.py. # -*- coding: utf-8 -*- import dash import dash_core_components as dcc import dash_html_components as html import pandas as pd external_stylesheets = [&#39;https://codepen.io/chriddyp/pen/bWLwgP.css&#39;] app = dash.Dash(__name__, external_stylesheets=external_stylesheets) df = pd.read_csv(&#39;data/gapminder_small.csv&#39;) app.layout = html.Div([ dcc.Graph( id=&#39;life-exp-vs-gdp&#39;, figure={ &#39;data&#39;: [ dict( x=df[df[&#39;continent&#39;] == i][&#39;gdpPercap&#39;], y=df[df[&#39;continent&#39;] == i][&#39;lifeExp&#39;], text=df[df[&#39;continent&#39;] == i][&#39;country&#39;], mode=&#39;markers&#39;, opacity=0.7, marker={ &#39;size&#39;: 15, &#39;line&#39;: {&#39;width&#39;: 0.5, &#39;color&#39;: &#39;white&#39;} }, name=i ) for i in df.continent.unique() ], &#39;layout&#39;: dict( xaxis={&#39;type&#39;: &#39;log&#39;, &#39;title&#39;: &#39;GDP Per Capita&#39;}, yaxis={&#39;title&#39;: &#39;Life Expectancy&#39;}, margin={&#39;l&#39;: 40, &#39;b&#39;: 40, &#39;t&#39;: 10, &#39;r&#39;: 10}, legend={&#39;x&#39;: 0, &#39;y&#39;: 1}, hovermode=&#39;closest&#39; ) } ) ]) if __name__ == &#39;__main__&#39;: app.run_server(debug=True) Interactive and Visual Data Exploration As seen above, just a couple visualization packages can go a very long way. It’s now very easy to incorporate interactivity, so you should use it even if only for your own data exploration. In general, interactivity allows for even more dimensions to be brought to a graphic, and can be more fun too! However, they must serve a purpose. Too often, interactivity can simply serve as distraction, and can actually detract from the data story. Make sure to use them when they can enhance the narrative you wish to express. Interactive Visualization Exercises Exercise 0 Install and load the plotly package. Load the tidyverse package if necessary (so you can use dplyr and ggplot2), and install/load the ggplot2movies for the IMDB data. Exercise 1 Using dplyr, group by year, and summarize to create a new variable that is the Average rating. Refer to the tidyverse section if you need a refresher on what’s being done here. Then create a plot with plotly for a line or scatter plot (for the latter, use the add_markers function). It will take the following form, but you’ll need to supply the plotly arguments. library(ggplot2movies) movies %&gt;% group_by(year) %&gt;% summarise(Avg_Rating = mean(rating)) plot_ly() %&gt;% add_markers() Exercise 2 This time group by year and Drama. In the summarize create average rating again, but also a variable representing the average number of votes. In your plotly line, use the size and color arguments to represent whether the average number of votes and whether it was drama or not respectively. Use add_markers. Note that Drama will be treated as numeric since it’s a 0-1 indicator. This won’t affect the plot, but if you want, you might use mutate to change it to a factor with labels ‘Drama’ and ‘Other’. Exercise 3 Create a ggplot of your own design and then use ggplotly to make it interactive. Often you’ll get an error because you used = instead of =~. Also, I find trying to set single values for things like size unintuitive, and it may be implemented differently for different traces (e.g. setting size in marker traces requires size=I(value)).↩︎ You can with ggplot2 as well, but I intentionally made no mention of it. You should learn how to use the package generally before learning how to use its shortcuts.↩︎ This is the sort of thing that takes the ‘wow factor’ out of a lot of stuff you see on the web. You’ll find a lot of those cool plots are actually lazy folks using default settings of stuff that make only take a single line of code.↩︎ "],
["thinking_vis.html", "Thinking Visually Information Color Contrast Scaling Size Transparency Accessibility File Types Summary A casual list of things to avoid Thinking Visually Exercises", " Thinking Visually Information A starting point for data visualization regards the information you want to display, and then how you want to display it in order to tell the data’s story. As in statistical modeling, parsimony is the goal, but not at the cost of the more compelling story. We don’t want to waste the time of the audience or be redundant, but we also want to avoid unnecessary clutter, chart junk, and the like. We’ll start with a couple examples. Consider the following. So what’s wrong with this? Plenty. Aside from being boring, the entire story can be said with a couple words- males are taller than females (even in the Star Wars universe). There is no reason to have a visualization. And if a simple group difference is the most exciting thing you have to talk about, not many are going to be interested. Minor issues can also be noted, including unnecessary border around the bars, unnecessary vertical gridlines, and an unnecessary X axis label. You might think the following is an improvement, but I would say it’s even worse. Now the y axis has been changed to distort the difference, perceptually suggesting a height increase of over 54%. Furthermore, color is used but the colors are chosen poorly, and add no information, thus making the legend superfluous. And finally, the above doesn’t even convey the information people think it does, assuming they are even standard error bars, which one typically has to guess about in many journal visualizations of this kind49. Now we add more information, but more problems! The above has unnecessary border, gridlines, and emphasis. The labels, while possibly interesting, do not relate anything useful to the graph, and many are illegible. It imposes a straight (and too wide of a) straight line on a nonlinear relationship. And finally, color choice is both terrible and tends to draw one’s eye to the female data points. Here is what it looks like to someone with the most common form of colorblindness. If the points were less clumpy on gender, it would be very difficult to distinguish the groups. And here is what it might look like when printed. Now consider the following. We have six pieces of information in one graph- name (on hover), homeworld (shape), age (size), gender (color), mass (x), and height (y). The colors are evenly spaced from one another, and so do not draw one’s attention to one group over another, or even to the line over groups. Opacity allows the line to be added and the points to overlap without loss of information. We technically don’t need a caption, legend or gridlines, because hovering over the data tells us everything we’d want to know about a given data point. The interactivity additionally allows one to select and zoom on specific areas. Whether this particular scheme is something you’d prefer or not, the point is that we get quite a bit of information without being overwhelming, and the data is allowed to express itself cleanly. Here are some things to keep in mind when creating visualizations for scientific communication. Your audience isn’t dumb Assume your audience, which in academia is full of people with advanced degrees or those aspiring to obtain one, and in other contexts comprises people who are interested in your story, can handle more than a bar graph. If the visualization is good and well-explained50, they’ll be fine. See the data visualization and maps sections of 2017: The Year in Visual Stories and Graphics at the New York Times. Good data visualization of even complex relationships can be appreciated by more than an academic audience. Assume you can at least provide visualizations on that level of complexity and be okay. It won’t always work, but at least put the same effort you’d appreciate yourself. Clarity is key Sometimes the clearest message is a complicated one. That’s okay, science is an inherently fuzzy process. Make sure your visualization tells the story you think is important, and don’t dumb the story down in the visualization. People will likely remember the graphic before they’ll remember the table of numbers. By the same token, don’t needlessly complicate something that is straightforward. Perhaps a scatter plot with some groupwise coloring is enough. That’s fine. All of this is easier said than done, and there is no right way to do data visualizations. Prepare to experiment, and focus on visuals that display patterns that will be more readily perceived. Avoid clutter In striving for clarity, there are pitfalls to avoid. Gridlines, 3d, unnecessary patterning, and chartjunk in general will only detract from the message. As an example, gridlines might even seem necessary, but even faint ones can potentially hinder the pattern recognition you hope will take place, perceptually imposing clumps of data that do not exist. In addition, they practically insist on a level of data precision that in many situations you simply don’t have. What’s more, with interactivity they literally convey nothing additional, as a simple hover-over or click on a data point will reveal the precise values. Use sparingly, if at all. Color isn’t optional It’s odd for me to have to say this, as it’s been the case for many years, but no modern scientific outlet should be a print-first outfit, and if they are, you shouldn’t care to send your work there. The only thing you should be concerned with is how it will look online, because that’s how people will interact with your work first and foremost. That means that color is essentially a requirement for any visualization, so use it well in yours. Think interactively It might be best to start by making the visualization you want to make, with interactivity and anything else you like. You can then reduce as necessary for publication or other outlets, and keep the fancy one as supplemental, or accessible on your own website to show off. Color There is a lot to consider regarding color. Until recently, the default color schemes of most visualization packages were poor at best. Thankfully, ggplot2, its imitators and extenders, in both the R world and beyond, have made it much easier to have a decent color scheme by default51. However, the defaults are still potentially problematic, so you should be prepared to go with something else. In other cases, you may just simply prefer something else. For example, for me, the gray background of ggplot2 defaults is something I have to remove for every plot52. Viridis A couple packages will help you get started in choosing a decent color scheme. One is viridis. As stated in the package description: These color maps are designed in such a way that they will analytically be perfectly perceptually-uniform, both in regular form and also when converted to black-and-white. They are also designed to be perceived by readers with the most common form of color blindness. So basically you have something that will take care of your audience without having to do much. There are four palettes. These color schemes might seem a bit odd from what you’re used to. But recall that the goal is good communication, and these will allow you to convey information accurately, without implicit bias, and be acceptable in different formats. In addition, there is ggplot2 functionality to boot, e.g. scale_color_viridis, and it will work for discrete or continuously valued data. For more, see the vignette. I also invite you to watch the introduction of the original module in Python, where you can learn more about the issues in color selection, and why viridis works. You can use the following functions for with ggplot2: scale_color_viridis_c scale_color_viridis_d scale_fill_viridis_c scale_fill_viridis_d Scientific colors Yet another set of palettes are available via the scico package, and are specifically geared toward for scientific presentation. These perceptually-uniform color maps will handle data variations equally all along the colour bar and still work for black and white print. Just another option beyond viridis palettes. Perceptually uniform Perceptually ordered Color-vision-deficiency friendly Readable as black-and-white print You can use the following functions for with ggplot2: scale_color_scico scale_color_scico_d scale_fill_scico scale_fill_scico_d RColorBrewer Color Brewer offers a collection of palettes that will generally work well in a variety of situations, especially for discrete data. While there are print and color-blind friendly palettes, not all adhere to those restrictions. Specifically though, you have palettes for the following data situations: Qualitative (e.g. Dark253) Sequential (e.g. Reds) Diverging (e.g. RdBu) There is a ggplot2 function, scale_color_brewer, you can use as well. For more, see colorbrewer.org. There you can play around with the palettes to help make your decision. You can use the following functions for with ggplot2: scale_color_brewer scale_fill_brewer/span&gt; In R, you have several schemes that work well right out of the box: ggplot2 default palette viridis scico RColorBrewer Furthermore, they’ll work well with discrete or continuous data. You will have to do some work to come up with better, so they should be your default. Sometimes though, one can’t help oneself. Contrast Thankfully, websites have mostly gotten past the phase where there text looks like this. The goal of scientific communication is to, well, communicate. Making text hard to read is pretty much antithetical to this. So contrast comes into play with text as well as color. In general, you should consider a 7 to 1 contrast ratio for text, minimally 4 to 1. -Here is text at 2 to 1 -Here is text at 4 to 1 -Here is text at 7 to 1 (this document) -Here is black I personally don’t like stark black, and find it visually irritating, but obviously that would be fine to use for most people. Contrast concerns regard color as well. When considering color, one should also think about the background for plots, or perhaps the surrounding text. The following function will check for this. Ideally one would pass AAA. # default ggplot2 discrete color against the default ggplot2 gray background visibly::color_contrast_checker(foreground = &#39;#F8766D&#39;, background = &#39;gray92&#39;) ratio AA AALarge AAA AAALarge 1 2.25 fail fail fail fail # the dark viridis would work visibly::color_contrast_checker(foreground = &#39;#440154&#39;, background = &#39;gray92&#39;) ratio AA AALarge AAA AAALarge 1 12.7 pass pass pass pass You can’t win all battles however. It will be difficult to choose colors that are perceptually even, avoid color-blindness issues, have good contrast, work to convey the information you need, and are aesthetically pleasing. The main thing to do is simply make the attempt. Scaling Size You might not be aware, but there is more than one way to scale the size of objects, e.g. in a scatterplot. Consider the following, where in both cases dots are scaled by the person’s body-mass index (BMI). What’s the difference? The first plot scales the dots by their area, while the second scales the radius, but otherwise they are identical. It’s not generally recommended to scale the radius, as our perceptual system is more attuned to the area. Packages like ggplot2 and plotly will automatically do this, but some might not, so you should check. Transparency Using transparency is a great way to keep detailed information available to the audience without being overwhelming. Consider the following. Fifty individual trajectories are shown on the left, but it doesn’t cause any issue graphically. The right has 10 lines plus a fitted line, 20 points and a ribbon to provide a sense of variance. Using transparency and a viridis color scheme allows it to be perceived cleanly. Without transparency, it just looks ugly, and notably busier if nothing else. This plot is using the exact same scico palette. In addition, transparency can be used to add additional information to a plot. In the following scatter plot, we can get a better sense of data density from the fact that the plot is darker where points overlap more. Here we apply transparency to a density plot to convey a group difference in distributions, while still being able to visualize the whole distribution of each group. Had we not done so, we might not be able to tell what’s going on with some of the groups at all. In general, a good use of transparency can potentially help any visualization, but consider it especially when trying to display many points, or otherwise have overlapping data. Accessibility Among many things (apparently) rarely considered in typical academic or other visualization is accessibility. The following definition comes from the World Wide Web Consortium. Web accessibility means that people with disabilities can use the Web. More specifically, Web accessibility means that people with disabilities can perceive, understand, navigate, and interact with the Web, and that they can contribute to the Web. Web accessibility also benefits others, including older people with changing abilities due to aging. The main message to get is that not everyone is able to use the web in the same manner. While you won’t be able to satisfy everyone who might come across your work, putting a little thought into your offering can go along way, and potentially widen your audience. We talked about this previously, but when communicating visually, one can do simple things like choosing a colorblind-friendly palette, or using a font contrast that will make it easier on the eyes of those reading your work. There are even browser plugins to test your web content for accessibility. In addition, there are little things like adding a title to inserted images, making links more noticeable etc., all of which can help consumers of your information. File Types It’s one thing to create a visualization, but at some point you’re likely going to want to share it. RStudio will allow for the export of any visualization created in the Plots or Viewer tab. In addition, various packages may have their own save function, that may allow you to specify size, type or other aspects. Here we’ll discuss some of the options. png: These are relatively small in size and ubiquitous on the web. You should feel fine in this format. It does not scale however, so if you make a smaller image and someone zooms, it will become blurry. gif: These are the type used for all the silly animations you see on the web. Using them is fine if you want to make an animation, but know that it can go longer than a couple seconds, and there is no requirement for it to be asinine. jpg: Commonly used for photographs, which isn’t the case with data generated graphs. Given their relative size I don’t see much need for these. svg: These take a different approach to imaging and can scale. You can make a very small one and it (potentially) can still look great when zoomed in to a much larger size. Often useful for logos, but possibly in any situation. As I don’t know what screen will see my visualizations, I generally opt for svg. It may be a bit slower/larger, but in my usage and for my audience size, this is of little concern relative to it looking proper. They also work for pdf if you’re still creating those, and there are also lighter weight versions in R, e.g. svglite. Beyond that I use png, and have no need for others. Here is a discussion on stackexchange that summarizes some of the above. The initial question is old but there have been recent updates to the responses. Note also, you can import files directly into your documents with R, markdown, HTML tags, or \\(\\LaTeX\\). See ?png for a starting point. The following demonstrates an image insert for HTML output, with a couple options for centering and size. &lt;img src=\"file.jpg\" style=\"display:block; margin: 0 auto;\" width=50%&gt; This uses markdown to get the same result ![](img/future.jpg){width=50%} Summary The goal of this section was mostly just to help you realize that there are many things to consider when visualizing information and attempting to communicate the contents of data. The approach is not the same as what one would do in say, an artistic venture, or where there is nothing specific to impart to an audience. Even some of the most common things you see published are fundamentally problematic, so you can’t even use what people traditionally do as a guide. However, there are many tools available to help you. Another thing to keep in mind is that there is no right way to do a particular visualization, and many ways, to have fun with it. A casual list of things to avoid I’m just putting things that come to mind here as I return to this document. Mostly it is personal opinion, though often based on various sources in the data visualization realm or simply my own experience. Pie Pie charts and their cousins, e.g. bar charts (and stacked versions), wind rose plots, radar plots etc., either convey too little information, or make otherwise simple information more difficult to process perceptually. The basic pie chart is really only able to convey proportional data. Beyond that, anything done with a pie chart can almost always be done better, at the very least with a bar chart, but you should really consider better ways to convey your data. Alternatives: bar densities/stacked densities parallel sets/sankey Histograms Anyone that’s used R’s hist function knows the frustration here. Use density plots instead. They convey the same information but better, and typical defaults are usually fine. However, you should really consider the information and audience- is a histogram or density plot really displaying what you want to show? Alternatives: density quantile dotplot Using 3D without adding any communicative value You will often come across use of 3D in scientific communication which is fairly poor and makes the data harder to interpret. In general, when going beyond two dimensions, your first thought should be to use color, size, etc. and finally, prefer interactivity to 3D. Where it is useful is in things like showing structure (e.g. molecular, geographical), or continuous multi-way interactions. Alternatives: multiple 2d/faceting Using too many colors Some put a completely non-scientifically based number on this, but the idea holds. For example, if you’re trying to show U.S. state grouping by using a different color for all 50 states, no one’s going to be able to tell the yellow for Alabama vs. the slightly different yellow for Idaho. Alternatives would be to show the information via a map or use a hover over display. Using valenced colors when data isn’t applicable Often we have data that can be thought of as having a positive/negative or valenced nuance. For example, we might want to show values relative to some cut point, or they might naturally have positive and negative values (e.g. sentiment, standardized scores). Oftentimes though, doing so would mean possibly arbitrarily picking a cut point and unnaturally discretizing the data. The following shows a plot of water risk for many countries. The first plots the color along a continuum with increasing darkness as one goes along, which is appropriate for this score of positive numeric values from 0-5. We can clearly see problematic ones while still getting a sense of where other countries lie along that score. The other plot arbitrarily codes a different color scheme, which might suggest some countries are fundamentally different than others. However, if the goal is to show values relative to the median, then it accurately conveys countries above and below that value. If the median is not a useful value (e.g. to take some action upon), then the former plot would likely be preferred. Showing maps that just display population Many of the maps I see on the web cover a wide range of data and can be very visually appealing, but pretty much just tell me where the most populated areas are, because the value conveyed is highly correlated with it. Such maps are not very interesting, so make sure that your geographical depiction is more informative than this. Biplots A lot of folks doing PCA resort to biplots for interpretation, where a graphical model would be much more straightforward. See this chapter for example. Thinking Visually Exercises Exercise 1 The following uses the diamonds data set that comes with ggplot2. Use the scale_color_viridis or scale_color_scico function to add a more accessible palette. Use ? to examine your options. # devtools::install_github(&quot;thomasp85/scico&quot;) # to use scientific colors library(ggplot2) ggplot(aes(x = carat, y = price), data = diamonds) + geom_point(aes(color = price)) + ???? Exercise 2 Now color it by the cut instead of price. Use scale_color_viridis/scioc_d. See the helpfile via ?scale_color_* to see how to change the palette. Thinking exercises For your upcoming presentation, who is your audience? Error bars for group means can overlap and still be statistically different (the test regards the difference in means). Furthermore visuals of this sort often don’t bother to say whether it is standard deviation, standard error, or 2*standard error, or even something else.↩︎ People seem to think there are text limits for captions. There are none.↩︎ Even Matlab finally joined the club, except that they still screwed up with their default coloring scheme.↩︎ Hadley states “The grey background gives the plot a similar colour (in a typographical sense) to the remainder of the text, ensuring that the graphics fit in with the flow of a text without jumping out with a bright white background. Finally, the grey background creates a continuous field of colour which ensures that the plot is perceived as a single visual entity.”. The part about it being ugly is apparently left out. ☺ Also, my opinion is that it has the opposite effect, making the visualization jump out because nothing on the web is typically gray by default. If anything the page background is white, and having a white/transparent background would perhaps be better, but honestly, don’t you want a visualization to jump out?↩︎ Don’t even think about asking what the Dark1 palette is.↩︎ "],
["summary.html", "Summary", " Summary With the right tools, data exploration can be: easier faster more efficient more fun! Use them to wring your data dry of what it has to offer. See the references for recommended next steps and… Embrace a richer understanding of your data! "],
["references.html", "References", " References This document can be seen as an introduction to more extensive documents on similar subject matter. Some of these have only recently come out, and others have recently been updated. "]
]
