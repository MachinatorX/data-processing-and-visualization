[
["index.html", "Data Processing &amp; Visualization", " Data Processing &amp; Visualization Michael Clark https://m-clark.github.io/data-processing-and-visualization/ 2018-07-22 "],
["1_01_intro.html", "Intro Outline Preparation Other Python", " Intro This document is the basis for multiple workshops, whose common goal is to provide some tools, tips, packages etc. that make data processing and visualization in R easier. It is oriented toward those who have had some exposure to R in an applied fashion, but would also be useful to someone coming to R from another programming language. It is not an introduction to R, nor does this document have anything to do with statistical analysis (yet). The goal here is primarily to instill awareness, specifically of tools that will make your data exploration easier, and to understand some of the why behind the tools, so that one can better implement them. Outline Part 1: Data Processing Understanding Base R Approaches to Data Processing Overview of Data Structures Input/Output Getting Acquainted with Other Approaches to Data Processing Pipes, and how to use them tidyverse data.table Misc. Part 2: Visualization Thinking Visually Visualizing Information Color Contrast and more… ggplot2 Aesthetics Layers Themes and more… Adding Interactivity Package demos Shiny Part 3: Programming Basics Using R more fully Dealing with objects Iterative programming Writing functions Going further Vectorization Regular expressions Part 4: Presentation Possible future addition Part 5: Modeling Possible future addition Preparation To follow along with the examples, complete the following steps: Download the zip file at https://github.com/m-clark/data-processing-and-visualization/raw/master/workshop_project.zip. Be mindful of where you put it. Unzip it. Be mindful of where you put the resulting folder. Open RStudio. File/Open Project and click on the blue icon (workshop_project.Rproj) in the folder you just created. File/Open Click on the ReadMe file, and, well, read it. You will then have easy access to much of the data, code, etc. used in the examples. Other Color coding in text: emphasis package function object/class link Some key packages used in the following demonstrations and exercises: tidyverse (several packages), data.table, ggplot2movies Python Python notebooks for the data processing section may be found here. Many others are also used, feel free to install as we come across them. Here are a few. DT, highcharter, magrittr, maps, mgcv (already comes with base R), plotly, quantmod, readr, visNetwork "],
["1_02_dataStructures.html", "Part I: Data Processing Data Structures", " Part I: Data Processing Data Structures R has several core data structures, and we’ll take a look at each. Vectors Factors Lists Matrices/arrays Data frames The more you know about R data structures, the more you’ll know how to use them, the more you’ll know why things go wrong if they do, and the further you’ll be able to go with your data. Vectors Vectors form the basis of R data structures. Two main types are atomic and lists, but we’ll talk about lists separately. Here is an R vector. The elements of the vector are numeric values. x = c(1, 3, 2, 5, 4) x [1] 1 3 2 5 4 All elements of an atomic vector are the same type. Example types include: character numeric (double) integer logical In addition, there are special kinds of values like NA (‘not available’ i.e. missing), NULL, NaN (not a number), Inf (infinite) and so forth. You can use typeof to examine an object’s type, or use an is function, e.g. is.logical, to check if an object is a specific type. Character strings When dealing with text, objects of class character are what you’d typically be dealing with. x = c(&#39;... Of Your Fake Dimension&#39;, &#39;Ephemeron&#39;, &#39;Dryswch&#39;, &#39;Isotasy&#39;, &#39;Memory&#39;) x Not much to it, but be aware there is no real limit to what is represented as a character vector. For example, in a data frame, you could have a column where each entry is one of the works of Shakespeare. Factors An important type of vector is a factor. Factors are used to represent categorical data structures. Although not exactly precise, one can think of factors as integers with labels. So the underlying representation of a variable for sex is 1:2 with labels ‘Male’ and ‘Female’. They are a special class with attributes, or metadata, that contains the information about the levels. x = factor(rep(letters[1:3], e=10)) x [1] a a a a a a a a a a b b b b b b b b b b c c c c c c c c c c Levels: a b c attributes(x) $levels [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; $class [1] &quot;factor&quot; The underlying representation is numeric, but it is important to remember that factors are categorical. Thus, they can’t be used as numbers would be, as the following demonstrates. x_num = as.numeric(x) # convert to a numeric object sum(x_num) [1] 60 sum(x) Error in Summary.factor(structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, : &#39;sum&#39; not meaningful for factors Strings vs. Factors The main thing to note is that factors are generally a statistical phenomenon, and are required to do statistical things with data that would otherwise be a simple character string. If you know the relatively few levels the data can take, you’ll generally want to use factors, or at least know that statistical packages and methods may require them. In addition, factors allow you to easily overcome the silly default alphabetical ordering of category levels in some very popular visualization packages. For other things, such as text analysis, you’ll almost certainly want character strings instead, and in many cases it will be required. It’s also worth noting that a lot of base R and other behavior will coerce strings to factors. This made a lot more sense in the early days of R, but is not really necessary these days. Some packages to note to help you with processing strings and factors: forcats stringr Logicals Logical scalar/vectors are those that take on one of two values TRUE or FALSE. They are especially useful in flagging whether to run certain parts of code, and indexing certain parts of data structures (e.g. taking rows that correspond to TRUE). We’ll talk about the latter usage more later in the document. Here is a logical vector. my_logic = c(TRUE, FALSE, TRUE, FALSE, TRUE, TRUE) Note also that logicals are also treated as binary 0:1, and so, for example, taking the mean will provide the proportion of TRUE values. !my_logic [1] FALSE TRUE FALSE TRUE FALSE FALSE as.numeric(my_logic) [1] 1 0 1 0 1 1 mean(my_logic) [1] 0.6666667 Numeric and integer The most common type of data structure you’ll deal with are integer and numeric vectors. class(1:3) [1] &quot;integer&quot; rnorm(5) [1] 0.3298353 -0.1903690 -1.5795276 -0.9216247 0.3670749 Matrices With multiple dimensions, we are dealing with arrays. Matrices are 2-d arrays, and extremely commonly used for scientific computing1. The vectors making up a matrix must all be of the same type. For example, all values in a matrix might be numeric, or all character strings. Creating a matrix Creating a matrix can be done in a variety of ways. # create vectors x = 1:4 y = 5:8 z = 9:12 rbind(x, y, z) # row bind [,1] [,2] [,3] [,4] x 1 2 3 4 y 5 6 7 8 z 9 10 11 12 cbind(x, y, z) # column bind x y z [1,] 1 5 9 [2,] 2 6 10 [3,] 3 7 11 [4,] 4 8 12 matrix(c(x, y, z), nrow=3, ncol=4, byrow=TRUE) [,1] [,2] [,3] [,4] [1,] 1 2 3 4 [2,] 5 6 7 8 [3,] 9 10 11 12 Lists Lists in R are highly flexible objects, and probably the most commonly used for applied data science. Unlike vectors, whose elements must be of the same type, lists can contain anything as their elements, even other lists. Here is a list. We use the list function to create it. x = list(1, &quot;apple&quot;, list(3, &quot;cat&quot;)) x [[1]] [1] 1 [[2]] [1] &quot;apple&quot; [[3]] [[3]][[1]] [1] 3 [[3]][[2]] [1] &quot;cat&quot; We often want to loop some function over a list. for (elem in x) print(class(elem)) [1] &quot;numeric&quot; [1] &quot;character&quot; [1] &quot;list&quot; Lists can, and often do, have named elements. x = list(&quot;a&quot; = 25, &quot;b&quot; = -1, &quot;c&quot; = 0) x[&quot;b&quot;] $b [1] -1 Almost all standard models in base R and other packages return an object that is a list. Knowing how to work with a list will allow you to easily access the contents of the model object for further processing. Python has similar structures, lists and dictionaries, where the latter works similarly to R’s named list. Data Frames Data frames are a very commonly used data structure. Elements of a data frame can be different types, and this is because the data.frame class is actually just a list. As such, everything about lists applies to them. But they can also be indexed by row or column as well, just like matrices. There are other very common types of object classes associated with packages that are both a data.frame and some other type of structure (e.g. tibbles in the tidyverse). Usually your data frame will come directly from import or manipulation of other R objects (e.g. matrices). However, you should know how to create one from scratch. Creating a data frame The following will create a data frame with two columns, a and b. mydf = data.frame(a = c(1,5,2), b = c(3,8,1)) Much to the disdain of the tidyverse, we can add row names also. rownames(mydf) = paste0(&#39;row&#39;, 1:3) mydf a b row1 1 3 row2 5 8 row3 2 1 Everything about lists applies to data.frames, so we can add, select, and remove elements of a data frame just like lists. However, we’ll visit this later, because we’ll have much more flexibility with data frames than we would lists for common data analysis and visualization. Data Structure Exercises Excercise #1 Create an object that is a matrix and/or a data.frame, and inspect its class or structure (use the class or str functions) on the object you just created. Exercise #2 Create a list of 3 elements, the first of which contains character strings, the second numbers, and the third, the data.frame or matrix you just created. Thinking Exercises How is a factor different from a character vector? How is a data.frame the same as and different from a matrix? How is a data.frame the same as and different from a list? Despite what the tidyverse would have you believe.↩ "],
["1_03_io.html", "Input/Output", " Input/Output Until you get comfortable getting data into R, you’re not going to use it as much as you would. You should at least be able to read in common data formats like comma/tab-separated, Excel, etc. Standard methods of reading in tabular data include the following functions: read.table read.csv readLines Base R also comes with the foreign package for reading in other types of files, especially other statistical packages. However, while you may see it still in use, it’s not as useful as what’s found in other packages. Reading in data is usually a one-off event, such that you’ll never need to use the package again after the data is loaded. In that case, you might use the following approach, so that you don’t need to attach the whole package. readr::read_csv(&#39;fileloc/filename.csv&#39;) You can use that for any package, which can help avoid naming conflicts by not loading a bunch of different packages. Furthermore, if you need packages that do have a naming conflict, using this approach will ensure the function from the package you want will be used. Other approaches There are some better and faster ways to read in data than the base R approach. A package for reading in foreign statistical files is haven, which has functions like read_spss and read_dta for SPSS and Stata files respectively. The package readxl is a clean way to read Excel files that doesn’t require any outside packages or languages. The package rio uses haven, readxl etc., but with just two functions for everything: import, export (also convert). Faster approaches For faster versions of base R functions, readr has read_csv, read_delim, and others. These make assumptions about what type each vector is after an initial scan of the data, then proceed accordingly. If you don’t have ‘big’ data, the subsequent speed gain won’t help much, however, such an approach actually can be used as a diagnostic to pick up potential data entry errors, as warnings are given when unexpected observations occur. The data.table package provides a faster version read.table, and is typically faster than readr approaches (fread). Other Data Be aware that R can handle practically any type of data you want to throw at it. Some examples include: JSON SQL XML YAML MongoDB NETCDF text (e.g. a novel) shapefiles (e.g. for geographic data) Google spreadsheets And many, many others. On the horizon feather is designed to make reading/writing data frames efficient, and the really nice thing about it is that it works in both Python and R. It’s still in early stages of development on the R side though. Big Data You may come across the situation where your data cannot be held in memory. One of the first things to be aware of for data processing is that you may not need to have the data all in memory at once. Before shifting to a hardware solution, consider if the following is possible. Chunking: reading and processing the data in chunks Line at a time: dealing with individual lines of data Other data formats: for example SQL databases (sqldf package, src_dbi in dplyr) However, it may be that the end result is still too large. In that case you’ll have to consider a cluster-based or distributed data situation. Of course R will have tools for that as well. DBI sparklyr RHadoop And more. I/O Exercises Exercise 1 Use readr and haven to read the following files. Use the url just like you would any file name. The latter is a Stata file. You can use the RStudio’s menu approach to import the file if you want. ‘https://raw.githubusercontent.com/m-clark/data-processing-and-visualization/master/data/cars.csv’ ‘https://raw.githubusercontent.com/m-clark/data-processing-and-visualization/master/data/presvote.dta’ If you downloaded the data for this workshop, the files can be accessed in that folder Thinking Exercises Why might you use read_csv from the readr package rather than read.csv in base R? What is your definition of ‘big’ data? "],
["1_04_indexing.html", "Indexing", " Indexing What follows is a refresher. Presumably you’ve had enough R exposure to be aware of some of this. However, much of data processing regards data frames, or other tables of mixed data types, so more time will be spent on slicing and dicing of data frames instead. Even so, it would be impossible to use R effectively without knowing how to handle basic data types. Base R Indexing Refresher Slicing vectors letters[4:6] [1] &quot;d&quot; &quot;e&quot; &quot;f&quot; letters[c(13,10,3)] [1] &quot;m&quot; &quot;j&quot; &quot;c&quot; Slicing matrices/data.frames myMatrix[1, 2:3] Label-based indexing mydf[&#39;row1&#39;, &#39;b&#39;] Position-based indexing mydf[1, 2] Mixed indexing mydf[&#39;row1&#39;, 2] If the row/column value is empty, all rows/columns are retained. mydf[&#39;row1&#39;,] mydf[,&#39;b&#39;] Non-contiguous mydf[c(1,3),] Boolean mydf[mydf$a &gt;= 2,] List/Data.frame extraction [ : grab a slice of elements/columns [[ : grab specific elements/columns $ : grab specific elements/columns @: extract slot for S4 objects my_list_or_df[2:4] my_list_or_df[[&#39;name&#39;]] my_list_or_df$name my_list@name Indexing Exercises This following is a refresher of base R indexing only. Here is a matrix, a data.frame and a list. mymatrix = matrix(rnorm(100), 10, 10) mydf = cars mylist = list(mymatrix, thisdf = mydf) Exercise 1 For the matrix, in separate operations, take a slice of rows, a selection of columns, and a single element. Exercise 2 For the data.frame, grab a column in 3 different ways. Exercise 3 For the list grab an element by number and by name. "],
["1_05_pipes.html", "Pipes", " Pipes More detail on much of the following is provided here. Pipes are operators that send what comes before the pipe to what comes after. There are many different pipes, and some packages that use their own. However, the vast majority of packages use the same pipe: %&gt;% Here, we’ll focus on their use with the dplyr package, and the tidyverse more generally. Pipes are also utilized heavily in visualization. Example: mydf %&gt;% select(var1, var2) %&gt;% filter(var1 == &#39;Yes&#39;) %&gt;% summary Start with a data.frame %&gt;% select columns from it %&gt;% filter/subset it %&gt;% get a summary Using variables as they are created One nice thing about pipelines is that we can use variables as soon as they are created, with out having to break out separate objects/steps. mydf %&gt;% mutate(newvar1 = var1 + var2, newvar2 = newvar1/var3) %&gt;% summarise(newvar2avg = mean(newvar2)) Pipes for Visualization (more later) The following provides a means to think about pipes for visualization. It’s just a generic example for now, but we’ll see more later. basegraph %&gt;% points %&gt;% lines %&gt;% layout The dot Most functions are not ‘pipe-aware’ by default. In the following we try to send our data frame to lm for a regression. mydf %&gt;% lm(y ~ x) # error Other pipes could work in this situation, e.g. %$% in magrittr. But generally, when you come upon this situation, you can use a dot to represent the object before the pipe. mydf %&gt;% lm(y ~ x, data=.) # . == mydf Flexibility Piping is not just for , and can be used as well, and would be the primary object for the family of functions we’ll discuss later. The following starts with a character vector. Sends it to a recursive function (named ..). .. is created on-the-fly, and has a single argument (.). After the function is created, it’s used on ., which represents the string before the pipe. Result: pipes between the words2. c(&#39;Ceci&#39;, &quot;n&#39;est&quot;, &#39;pas&#39;, &#39;une&#39;, &#39;pipe!&#39;) %&gt;% { .. &lt;- . %&gt;% if (length(.) == 1) . else paste(.[1], &#39;%&gt;%&#39;, ..(.[-1])) ..(.) } [1] &quot;Ceci %&gt;% n&#39;est %&gt;% pas %&gt;% une %&gt;% pipe!&quot; Put that in your pipe and smoke it René Magritte! Summary Pipes are best used interactively, though you can use them within functions as well, and they are extremely useful for data exploration. Nowadays, more and more packages are being made that are ‘pipe-aware’, especially many visualization packages. See the magrittr package for more pipes. That was a very complicated way to do this paste(c('Ceci', &quot;n'est&quot;, 'pas', 'une', 'pipe!'), collapse=' %&gt;% ').↩ "],
["1_06_tidyverse.html", "Tidyverse", " Tidyverse What is the tidyverse? The tidyverse consists of a few key packages: ggplot2: data visualization dplyr: data manipulation tidyr: data tidying readr: data import purrr: functional programming, e.g. alternate approaches to apply tibble: tibbles, a modern re-imagining of data frames And of course the tidyverse package itself, which will load all of the above in a way that will avoid naming conflicts. library(tidyverse) Loading tidyverse: ggplot2 Loading tidyverse: tibble Loading tidyverse: tidyr Loading tidyverse: readr Loading tidyverse: purrr Loading tidyverse: dplyr Conflicts with tidy packages ------------------------- filter(): dplyr, stats lag(): dplyr, stats In addition, there are other packages like lubridate, rvest, stringr and others in the hadleyverse that are also greatly useful. What is tidy? Tidy data refers to data arranged in a way that makes data processing, analysis, and visualization simpler. In a tidy data set: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Think long before wide. dplyr dplyr provides a grammar of data manipulation (like ggplot2 does for visualization). It is the next iteration of plyr, but there is no longer any need for plyr really. It’s focused on tools for working with data frames, with over 100 functions that might be of specific use to you. It has three main goals: Make the most important data manipulation tasks easier. Do them faster. Use the same interface to work with data frames, data tables or a database. Some key operations include: select: grab columns select helpers: one_of, starts_with, num_range etc. filter/slice: grab rows group_by: grouped operations mutate/transmute: create new variables summarize: summarize/aggregate do: arbitrary operations There are various (SQL-like) join/merge functions: inner_join, left_join etc. And there are a lot of little things like: n, n_distinct, nth, n_groups, count, recode, between In addition, there is no need to quote variable names. An example Let’s say we want to select from our data the following variables: Start with the ID variable The variables X1 through X10, which are not all together, and there are many more X columns The variables var1 and var2, which are the only variables with var in their name Any variable with a name that starts with XYZ How might we go about this in a dataset of possibly hundreds or even thousands of columns? There are several base R approaches that we could go with, but often they will be tedious, or require multiple objects to be created just to get the columns you want. Let’s start with the worst choice. newData = oldData[,c(1,2,3,4, etc.)] Using numeric indexes, or rather magic numbers, is not conducive to readability or reproducibility. If anything changes about the data columns, the numbers may no longer be applicable, and you’d have to redo the line again. We could name the variables explicitly. newData = oldData[,c(&#39;ID&#39;,&#39;X1&#39;, &#39;X2&#39;, etc.)] This would be fine if there are only a handful. But if you’re trying to reduce a 1000 column data set to several dozen it’s tedious, and generally not pretty regardless. A more advanced alternative regards a two-step approach with regular expressions. This requires that you know something about regex (and you should), but it is difficult to read/understand by those who don’t, and often by even yourself if it’s more complicated. In any case, you first will need to create an object that represents the column names first, otherwise it looks unwieldy if used within brackets or a function like subset. cols = c(&#39;ID&#39;, paste0(&#39;X&#39;, 1:10), &#39;var1&#39;, &#39;var2&#39;, grep(colnames(oldData), &#39;^XYZ&#39;, value=T)) newData = oldData[,cols] # or via subset newData = subset(oldData, select = cols) Now consider there is even more to do. What if you also want observations where Z is Yes, Q is No, and only the observations with the top 50 values of var2, ordered by var1 (descending)? Probably the more straightforward way in R to do so would be something like the following, where each part is broken out and we continuously write over the object as we modify it. # three operations and overwriting or creating new objects if we want clarity newData = newData[oldData$Z == &#39;Yes&#39; &amp; oldData$Q == &#39;No&#39;,] newData = newData[order(newData$var2, decreasing=T)[1:50],] newData = newData[order(newData$var1, decreasing=T),] And this is for fairly straightforward operations. Now consider doing all of the previous in one piped operation. The dplyr package will allow us to do something like the following. newData = oldData %&gt;% select(num_range(&#39;X&#39;, 1:10), contains(&#39;var&#39;), starts_with(&#39;XYZ&#39;)) %&gt;% filter(Z == &#39;Yes&#39;, Q == &#39;No&#39;) %&gt;% top_n(n=50, var2) %&gt;% arrange(desc(var1)) Even if it hadn’t been explained before, you might have been able to guess a little as to what was going on. The code is fairly succinct, we don’t have to keep referencing objects repeatedly, and no explicit intermediary objects are created. dplyr and piping is an alternative. You can do all this sort of stuff with base R, for example, with functions like with, within, subset, transform, etc. Though the initial base R approach depicted is fairly concise, in general, it can potentially be: more verbose less legible less amenable to additional data changes requires esoteric knowledge (e.g. regular expressions) often requires creation of new objects (even if we just want to explore) often slower, possibly greatly Running example The following data was scraped initially scraped from the web as follows. It is data from the NBA basketball league for the 2017-2018 season with things like player names, position, team name, points per game, field goal percentage, and various other statistics. We’ll use it as an example to demonstrate various functionality found within dplyr. library(rvest) url = &quot;http://www.basketball-reference.com/leagues/NBA_2018_totals.html&quot; bball = read_html(url) %&gt;% html_nodes(&quot;#totals_stats&quot;) %&gt;% html_table() %&gt;% data.frame() save(bball, file=&#39;data/bball.RData&#39;) However you can just load it into your workspace. Note that when initially gathered from the website, the data is all character strings. We’ll fix this later. The following shows the data as it will eventually be. load(&#39;data/bball.RData&#39;) glimpse(bball[,1:5]) Observations: 664 Variables: 5 $ Rk &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 23, 23, 24, 25, 25, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 40, 40, 41, 42, 43, 44, 45, 46, 4... $ Player &lt;chr&gt; &quot;Alex Abrines&quot;, &quot;Quincy Acy&quot;, &quot;Steven Adams&quot;, &quot;Bam Adebayo&quot;, &quot;Arron Afflalo&quot;, &quot;Cole Aldrich&quot;, &quot;LaMarcus Aldridge&quot;, &quot;Jarrett Allen&quot;, &quot;Kadeem Allen&quot;, &quot;Tony Allen&quot;, &quot;Al-Farouq Aminu&quot;, &quot;Justin Anderson&quot;, ... $ Pos &lt;chr&gt; &quot;SG&quot;, &quot;PF&quot;, &quot;C&quot;, &quot;C&quot;, &quot;SG&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;C&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;SG&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SG&quot;, &quot;SG&quot;, &quot;PG&quot;, &quot;PG&quot;, &quot;PG&quot;, &quot;PF&quot;,... $ Age &lt;dbl&gt; 24, 27, 24, 20, 32, 29, 32, 19, 25, 36, 27, 24, 24, 29, 19, 23, 33, 20, 23, 32, 29, 25, 31, 31, 31, 30, 28, 28, 28, 22, 24, 21, 20, 33, 25, 27, 29, 29, 31, 28, 24, 21, 29, 31, 31, 31, 23, 23, 20, 25, ... $ Tm &lt;chr&gt; &quot;OKC&quot;, &quot;BRK&quot;, &quot;OKC&quot;, &quot;MIA&quot;, &quot;ORL&quot;, &quot;MIN&quot;, &quot;SAS&quot;, &quot;BRK&quot;, &quot;BOS&quot;, &quot;NOP&quot;, &quot;POR&quot;, &quot;PHI&quot;, &quot;SAS&quot;, &quot;HOU&quot;, &quot;IND&quot;, &quot;MIL&quot;, &quot;OKC&quot;, &quot;TOR&quot;, &quot;CHI&quot;, &quot;HOU&quot;, &quot;DEN&quot;, &quot;ORL&quot;, &quot;TOT&quot;, &quot;NOP&quot;, &quot;CHI&quot;, &quot;ORL&quot;, &quot;TOT&quot;, &quot;ATL&quot;, &quot;MIA... Selecting Columns Often you do not need the entire data set. While this is easily handled in base R (as shown earlier), it can be more clear to use select in dplyr. Now we won’t have to create separate objects, use quotes or $, etc. bball %&gt;% select(Player, Tm, Pos) %&gt;% head() Player Tm Pos 1 Alex Abrines OKC SG 2 Quincy Acy BRK PF 3 Steven Adams OKC C 4 Bam Adebayo MIA C 5 Arron Afflalo ORL SG 6 Cole Aldrich MIN C What if we want to drop some variables? bball %&gt;% select(-Player, -Tm, -Pos) %&gt;% head() Rk Age G GS MP FG FGA FG. X3P X3PA X3P. X2P X2PA X2P. eFG. FT FTA FT. ORB DRB TRB AST STL BLK TOV PF PTS 1 1 24 75 8 1134 115 291 0.395 84 221 0.380 31 70 0.443 0.540 39 46 0.848 26 88 114 28 38 8 25 124 353 2 2 27 70 8 1359 130 365 0.356 102 292 0.349 28 73 0.384 0.496 49 60 0.817 40 216 256 57 33 29 60 149 411 3 3 24 76 76 2487 448 712 0.629 0 2 0.000 448 710 0.631 0.629 160 287 0.557 384 301 685 88 92 78 128 215 1056 4 4 20 69 19 1368 174 340 0.512 0 7 0.000 174 333 0.523 0.512 129 179 0.721 118 263 381 101 32 41 66 138 477 5 5 32 53 3 682 65 162 0.401 27 70 0.386 38 92 0.413 0.485 22 26 0.846 4 62 66 30 4 9 21 56 179 6 6 29 21 0 49 5 15 0.333 0 0 NA 5 15 0.333 0.333 2 6 0.333 3 12 15 3 2 1 1 11 12 Helper functions Sometimes, we have a lot of variables to select, and if they have a common naming scheme, this can be very easy. bball %&gt;% select(Player, contains(&quot;3P&quot;), ends_with(&quot;RB&quot;)) %&gt;% arrange(desc(TRB)) %&gt;% head() Player X3P X3PA X3P. ORB DRB TRB 1 Andre Drummond 0 11 0.000 399 848 1247 2 DeAndre Jordan 0 0 NA 329 842 1171 3 Dwight Howard 1 7 0.143 255 757 1012 4 Karl-Anthony Towns 120 285 0.421 238 774 1012 5 Anthony Davis 55 162 0.340 187 645 832 6 Russell Westbrook 97 326 0.298 152 652 804 The select also has helper functions to make selecting columns even easier. I probably don’t even need to explain what’s being done above, and this is the power of the tidyverse way. Here is the list of helper functions to be aware of: starts_with: starts with a prefix ends_with: ends with a suffix contains: contains a literal string matches: matches a regular expression num_range: a numerical range like x01, x02, x03. one_of: variables in character vector. everything: all variables. Filtering Rows There are repeated header rows in this data3, so we need to drop them. This is also why everything was character string when we first scraped it, because having any character strings in a column coerces the entire column to be character, since all elements of a vector need to be of the same type. Character string is chosen over others because anything can be converted to a string, but not everything can be a number. Filtering by rows requires the basic indexing knowledge we talked about before, especially Boolean indexing. In the following, Rk, or rank, is for all intents and purposes just a row id, but if it equals ‘Rk’ instead of something else, we know we’re dealing with a header row, so we’ll drop it. bball = bball %&gt;% filter(Rk != &quot;Rk&quot;) filter returns rows with matching conditions. slice allows for a numeric indexing approach4. Say we want too look at forwards (SF or PF) over the age of 35. The following will do this, and since some players play on multiple teams, we’ll want only the unique information on the variables of interest. The function distinct allows us to do this. bball %&gt;% filter(Age &gt; 35, Pos == &quot;SF&quot; | Pos == &quot;PF&quot;) %&gt;% distinct(Player, Pos, Age) Player Pos Age 1 Tony Allen SF 36 2 Vince Carter SF 41 3 Nick Collison PF 37 4 Richard Jefferson SF 37 5 Joe Johnson SF 36 6 Zach Randolph PF 36 7 Damien Wilkins SF 38 Maybe we want just the first 10 rows. This is often the case when we perform some operation and need to quickly verify that what we’re doing is working in principle. bball %&gt;% slice(1:10) Rk Player Pos Age Tm G GS MP FG FGA FG. X3P X3PA X3P. X2P X2PA X2P. eFG. FT FTA FT. ORB DRB TRB AST STL BLK TOV PF PTS 1 1 Alex Abrines SG 24 OKC 75 8 1134 115 291 0.395 84 221 0.380 31 70 0.443 0.540 39 46 0.848 26 88 114 28 38 8 25 124 353 2 2 Quincy Acy PF 27 BRK 70 8 1359 130 365 0.356 102 292 0.349 28 73 0.384 0.496 49 60 0.817 40 216 256 57 33 29 60 149 411 3 3 Steven Adams C 24 OKC 76 76 2487 448 712 0.629 0 2 0.000 448 710 0.631 0.629 160 287 0.557 384 301 685 88 92 78 128 215 1056 4 4 Bam Adebayo C 20 MIA 69 19 1368 174 340 0.512 0 7 0.000 174 333 0.523 0.512 129 179 0.721 118 263 381 101 32 41 66 138 477 5 5 Arron Afflalo SG 32 ORL 53 3 682 65 162 0.401 27 70 0.386 38 92 0.413 0.485 22 26 0.846 4 62 66 30 4 9 21 56 179 6 6 Cole Aldrich C 29 MIN 21 0 49 5 15 0.333 0 0 NA 5 15 0.333 0.333 2 6 0.333 3 12 15 3 2 1 1 11 12 7 7 LaMarcus Aldridge C 32 SAS 75 75 2509 687 1347 0.510 27 92 0.293 660 1255 0.526 0.520 334 399 0.837 246 389 635 152 43 90 111 162 1735 8 8 Jarrett Allen C 19 BRK 72 31 1441 234 397 0.589 5 15 0.333 229 382 0.599 0.596 114 147 0.776 144 244 388 49 28 88 82 147 587 9 9 Kadeem Allen PG 25 BOS 18 1 107 6 22 0.273 0 11 0.000 6 11 0.545 0.273 7 9 0.778 4 7 11 12 3 2 9 15 19 10 10 Tony Allen SF 36 NOP 22 0 273 44 91 0.484 4 12 0.333 40 79 0.506 0.505 11 21 0.524 20 27 47 9 11 3 19 49 103 We can use filtering even with variables just created. bball %&gt;% unite(&quot;posTeam&quot;, Pos, Tm) %&gt;% # create a new variable filter(posTeam == &quot;SG_GSW&quot;) %&gt;% # use it for filtering select(Player, posTeam, Age) %&gt;% # use it for selection arrange(desc(Age)) # descending order Player posTeam Age 1 Nick Young SG_GSW 32 2 Klay Thompson SG_GSW 27 3 Patrick McCaw SG_GSW 22 Being able to use a newly created variable on the fly, possibly only to filter or create some other variable, goes a long way toward easy visualization and generation of desired summary statistics. Generating New Data One of the most common data processing tasks beyond subsetting the data is generating new variables. The function mutate takes a vector and returns one of the same dimension. In addition, there is mutate_at, mutate_if, and mutate_all to help with specific scenarios. To demonstrate, we’ll use mutate_at to make appropriate columns numeric, i.e. everything except Player, Pos, and Tm. It takes two inputs, variables and functions to apply. As there are multiple variables and (potentially) multiple functions, we use the vars and funs functions to denote them5. bball = bball %&gt;% mutate_at(vars(-Player, -Pos, -Tm), funs(as.numeric)) glimpse(bball[,1:7]) Observations: 664 Variables: 7 $ Rk &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 23, 23, 24, 25, 25, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 40, 40, 41, 42, 43, 44, 45, 46, 4... $ Player &lt;chr&gt; &quot;Alex Abrines&quot;, &quot;Quincy Acy&quot;, &quot;Steven Adams&quot;, &quot;Bam Adebayo&quot;, &quot;Arron Afflalo&quot;, &quot;Cole Aldrich&quot;, &quot;LaMarcus Aldridge&quot;, &quot;Jarrett Allen&quot;, &quot;Kadeem Allen&quot;, &quot;Tony Allen&quot;, &quot;Al-Farouq Aminu&quot;, &quot;Justin Anderson&quot;, ... $ Pos &lt;chr&gt; &quot;SG&quot;, &quot;PF&quot;, &quot;C&quot;, &quot;C&quot;, &quot;SG&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;C&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;SG&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SG&quot;, &quot;SG&quot;, &quot;PG&quot;, &quot;PG&quot;, &quot;PG&quot;, &quot;PF&quot;,... $ Age &lt;dbl&gt; 24, 27, 24, 20, 32, 29, 32, 19, 25, 36, 27, 24, 24, 29, 19, 23, 33, 20, 23, 32, 29, 25, 31, 31, 31, 30, 28, 28, 28, 22, 24, 21, 20, 33, 25, 27, 29, 29, 31, 28, 24, 21, 29, 31, 31, 31, 23, 23, 20, 25, ... $ Tm &lt;chr&gt; &quot;OKC&quot;, &quot;BRK&quot;, &quot;OKC&quot;, &quot;MIA&quot;, &quot;ORL&quot;, &quot;MIN&quot;, &quot;SAS&quot;, &quot;BRK&quot;, &quot;BOS&quot;, &quot;NOP&quot;, &quot;POR&quot;, &quot;PHI&quot;, &quot;SAS&quot;, &quot;HOU&quot;, &quot;IND&quot;, &quot;MIL&quot;, &quot;OKC&quot;, &quot;TOR&quot;, &quot;CHI&quot;, &quot;HOU&quot;, &quot;DEN&quot;, &quot;ORL&quot;, &quot;TOT&quot;, &quot;NOP&quot;, &quot;CHI&quot;, &quot;ORL&quot;, &quot;TOT&quot;, &quot;ATL&quot;, &quot;MIA... $ G &lt;dbl&gt; 75, 70, 76, 69, 53, 21, 75, 72, 18, 22, 69, 38, 74, 66, 11, 75, 78, 74, 24, 67, 19, 15, 18, 14, 4, 75, 50, 37, 13, 53, 29, 7, 52, 69, 77, 81, 64, 39, 81, 65, 82, 62, 74, 80, 52, 28, 57, 26, 82, 77, 11... $ GS &lt;dbl&gt; 8, 8, 76, 19, 3, 0, 75, 31, 1, 0, 67, 0, 67, 50, 0, 75, 78, 62, 0, 67, 1, 1, 0, 0, 0, 36, 14, 9, 5, 6, 1, 0, 50, 10, 77, 40, 64, 11, 67, 65, 82, 0, 30, 2, 1, 1, 13, 3, 37, 10, 11, 0, 1, 25, 21, 2, 0, ... Now that the data is correctly specified, the following demonstrates how we can use the standard mutate function to create composites of existing variables. bball = bball %&gt;% mutate(trueShooting = PTS / (2 * (FGA + (.44 * FTA))), effectiveFG = (FG + (.5 * X3P)) / FGA, shootingDif = trueShooting - FG.) summary(select(bball, shootingDif)) # select and others don&#39;t have to be piped to use shootingDif Min. :-0.05698 1st Qu.: 0.04909 Median : 0.08877 Mean : 0.08733 3rd Qu.: 0.11710 Max. : 0.50000 NA&#39;s :4 Grouping and Summarizing Data A very common task is to look at group-based statistics, and we can use group_by and summarize to help us in this regard6. Base R has things like aggregate, by, and tapply for this, but they should not be used, as this approach is much more straightforward, flexible, and faster. For this demonstration, I’m going to start putting together several things we’ve demonstrated thus far. Ultimately we’ll create a variable called trueShooting, which represents ‘true shooting percentage’, and get an average for each position, and compare it to the standard field goal percentage. bball %&gt;% select(Pos, FG, FGA, FG., FTA, X3P, PTS) %&gt;% mutate(trueShooting = PTS / (2 * (FGA + (.44 * FTA))), effectiveFG = (FG + (.5 * X3P)) / FGA, shootingDif = trueShooting - FG.) %&gt;% group_by(Pos) %&gt;% summarize(`meanFG%` = mean(FG., na.rm = TRUE), meanTrueShooting = mean(trueShooting, na.rm = TRUE)) # A tibble: 7 x 3 Pos `meanFG%` meanTrueShooting &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 C 0.531 0.575 2 PF 0.454 0.537 3 PG 0.419 0.512 4 PG-SG 0.38 0.539 5 SF 0.409 0.508 6 SF-SG 0.448 0.522 7 SG 0.404 0.516 We can use do on grouped data to go further. This function can be used to create a new list-column in the data, the elements of which can be anything, even the results of an analysis for each group. As such, we can use tidyr’s unnest to get back to a standard data frame. To demonstrate, the following will group data by position, then get the correlation between field-goal percentage and free-throw shooting percentage. For some reason, one individual was labeled as ‘PG-SG’, and only two were labeled ‘SF-SG’, so we’ll change them to just ‘SG’. On your own, I recommend just looking at it to the do line first, and compare to this result. bball %&gt;% mutate(Pos = if_else(Pos==&#39;PG-SG&#39; | Pos==&#39;SF-SG&#39;, &#39;SG&#39;, Pos)) %&gt;% group_by(Pos) %&gt;% do(FgFt_Corr=cor(.$FG., .$FT., use=&#39;complete&#39;)) %&gt;% unnest(FgFt_Corr) # A tibble: 5 x 2 Pos FgFt_Corr &lt;chr&gt; &lt;dbl&gt; 1 C 0.0241 2 PF -0.0564 3 PG -0.0131 4 SF 0.203 5 SG 0.242 As a reminder, data frames are lists. As such, anything can go into the ‘columns’. library(nycflights13) carriers = group_by(flights, carrier) group_size(carriers) [1] 18460 32729 714 54635 48110 54173 685 3260 342 26397 32 58665 20536 5162 12275 601 mods = carriers %&gt;% do(model = lm(arr_delay ~ dep_time, data = .)) mods Source: local data frame [16 x 2] Groups: &lt;by row&gt; # A tibble: 16 x 2 carrier model * &lt;chr&gt; &lt;list&gt; 1 9E &lt;S3: lm&gt; 2 AA &lt;S3: lm&gt; 3 AS &lt;S3: lm&gt; 4 B6 &lt;S3: lm&gt; 5 DL &lt;S3: lm&gt; 6 EV &lt;S3: lm&gt; 7 F9 &lt;S3: lm&gt; 8 FL &lt;S3: lm&gt; 9 HA &lt;S3: lm&gt; 10 MQ &lt;S3: lm&gt; 11 OO &lt;S3: lm&gt; 12 UA &lt;S3: lm&gt; 13 US &lt;S3: lm&gt; 14 VX &lt;S3: lm&gt; 15 WN &lt;S3: lm&gt; 16 YV &lt;S3: lm&gt; mods %&gt;% summarize(carrier = carrier, `Adjusted Rsq` = summary(model)$adj.r.squared) %&gt;% head() # A tibble: 6 x 2 carrier `Adjusted Rsq` &lt;chr&gt; &lt;dbl&gt; 1 9E 0.0513 2 AA 0.0504 3 AS 0.0815 4 B6 0.0241 5 DL 0.0347 6 EV 0.0836 You can use group_by on more than one variable, e.g. group_by(var1, var2) Renaming columns Tidy data doesn’t really have a problem with variable names starting with numbers or incorporating symbols. I would still suggest it is poor practice, because even if your data set looks fine, you’ll encounter problems with modeling and visualization package using that data. However, as a demonstration, we can ‘fix’ some of the variable names. One issue is that when we scraped the data and converted it to a data.frame, the names that started with a number, like 3P for ‘three point baskets made’, were made into X3P, because that’s the way R works by default. In addition, 3P%, i.e. three point percentage made, was made into 3P. Same goes for the 2P (two-pointers) and FT (free-throw) variables. We can use rename to change column names. A basic example is as follows. data %&gt;% rename(new_name = old_name, new_name2 = old_name2) Very straightforward. However, oftentimes we’ll need to change patterns, as with our current problem. The following uses str_replace from stringr to look for a pattern in a name, and replace that pattern with some other pattern. It uses regular expressions for the patterns. bball %&gt;% rename_at(vars(contains(&#39;.&#39;)), str_replace, pattern=&#39;\\\\.&#39;, replacement=&#39;%&#39;) %&gt;% rename_at(vars(starts_with(&#39;X&#39;)), str_replace, pattern=&#39;X&#39;, replacement=&#39;&#39;) %&gt;% glimpse() Observations: 664 Variables: 33 $ Rk &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 23, 23, 24, 25, 25, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 40, 40, 41, 42, 43, 44, 45,... $ Player &lt;chr&gt; &quot;Alex Abrines&quot;, &quot;Quincy Acy&quot;, &quot;Steven Adams&quot;, &quot;Bam Adebayo&quot;, &quot;Arron Afflalo&quot;, &quot;Cole Aldrich&quot;, &quot;LaMarcus Aldridge&quot;, &quot;Jarrett Allen&quot;, &quot;Kadeem Allen&quot;, &quot;Tony Allen&quot;, &quot;Al-Farouq Aminu&quot;, &quot;Justin Ander... $ Pos &lt;chr&gt; &quot;SG&quot;, &quot;PF&quot;, &quot;C&quot;, &quot;C&quot;, &quot;SG&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;C&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;SG&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SG&quot;, &quot;SG&quot;, &quot;PG&quot;, &quot;PG&quot;, &quot;PG&quot;,... $ Age &lt;dbl&gt; 24, 27, 24, 20, 32, 29, 32, 19, 25, 36, 27, 24, 24, 29, 19, 23, 33, 20, 23, 32, 29, 25, 31, 31, 31, 30, 28, 28, 28, 22, 24, 21, 20, 33, 25, 27, 29, 29, 31, 28, 24, 21, 29, 31, 31, 31, 23, 23, 20... $ Tm &lt;chr&gt; &quot;OKC&quot;, &quot;BRK&quot;, &quot;OKC&quot;, &quot;MIA&quot;, &quot;ORL&quot;, &quot;MIN&quot;, &quot;SAS&quot;, &quot;BRK&quot;, &quot;BOS&quot;, &quot;NOP&quot;, &quot;POR&quot;, &quot;PHI&quot;, &quot;SAS&quot;, &quot;HOU&quot;, &quot;IND&quot;, &quot;MIL&quot;, &quot;OKC&quot;, &quot;TOR&quot;, &quot;CHI&quot;, &quot;HOU&quot;, &quot;DEN&quot;, &quot;ORL&quot;, &quot;TOT&quot;, &quot;NOP&quot;, &quot;CHI&quot;, &quot;ORL&quot;, &quot;TOT&quot;, &quot;ATL&quot;... $ G &lt;dbl&gt; 75, 70, 76, 69, 53, 21, 75, 72, 18, 22, 69, 38, 74, 66, 11, 75, 78, 74, 24, 67, 19, 15, 18, 14, 4, 75, 50, 37, 13, 53, 29, 7, 52, 69, 77, 81, 64, 39, 81, 65, 82, 62, 74, 80, 52, 28, 57, 26, 82, ... $ GS &lt;dbl&gt; 8, 8, 76, 19, 3, 0, 75, 31, 1, 0, 67, 0, 67, 50, 0, 75, 78, 62, 0, 67, 1, 1, 0, 0, 0, 36, 14, 9, 5, 6, 1, 0, 50, 10, 77, 40, 64, 11, 67, 65, 82, 0, 30, 2, 1, 1, 13, 3, 37, 10, 11, 0, 1, 25, 21, ... $ MP &lt;dbl&gt; 1134, 1359, 2487, 1368, 682, 49, 2509, 1441, 107, 273, 2072, 519, 1978, 1725, 30, 2756, 2501, 1481, 304, 2269, 141, 279, 182, 121, 61, 1760, 715, 570, 145, 713, 385, 80, 1780, 1603, 2634, 2683, ... $ FG &lt;dbl&gt; 115, 130, 448, 174, 65, 5, 687, 234, 6, 44, 230, 87, 231, 207, 4, 742, 472, 163, 17, 268, 22, 31, 9, 7, 2, 244, 91, 80, 11, 72, 20, 14, 203, 303, 537, 465, 270, 109, 210, 284, 683, 73, 409, 337,... $ FGA &lt;dbl&gt; 291, 365, 712, 340, 162, 15, 1347, 397, 22, 91, 582, 202, 438, 480, 9, 1402, 1168, 346, 41, 651, 47, 79, 22, 16, 6, 540, 215, 168, 47, 192, 59, 21, 564, 690, 1208, 1028, 650, 262, 446, 676, 1484... $ `FG%` &lt;dbl&gt; 0.395, 0.356, 0.629, 0.512, 0.401, 0.333, 0.510, 0.589, 0.273, 0.484, 0.395, 0.431, 0.527, 0.431, 0.444, 0.529, 0.404, 0.471, 0.415, 0.412, 0.468, 0.392, 0.409, 0.438, 0.333, 0.452, 0.423, 0.476... $ `3P` &lt;dbl&gt; 84, 102, 0, 0, 27, 0, 27, 5, 0, 4, 125, 34, 19, 131, 0, 43, 169, 73, 9, 170, 8, 8, 0, 0, 0, 114, 60, 49, 11, 11, 11, 4, 90, 115, 119, 156, 92, 54, 3, 108, 199, 28, 34, 149, 92, 57, 0, 11, 118, 9... $ `3PA` &lt;dbl&gt; 221, 292, 2, 7, 70, 0, 92, 15, 11, 12, 339, 103, 57, 339, 0, 140, 474, 197, 31, 462, 23, 29, 0, 0, 0, 272, 156, 111, 45, 43, 33, 5, 295, 313, 333, 422, 274, 146, 21, 274, 530, 82, 86, 395, 247, ... $ `3P%` &lt;dbl&gt; 0.380, 0.349, 0.000, 0.000, 0.386, NA, 0.293, 0.333, 0.000, 0.333, 0.369, 0.330, 0.333, 0.386, NA, 0.307, 0.357, 0.371, 0.290, 0.368, 0.348, 0.276, NA, NA, NA, 0.419, 0.385, 0.441, 0.244, 0.256,... $ `2P` &lt;dbl&gt; 31, 28, 448, 174, 38, 5, 660, 229, 6, 40, 105, 53, 212, 76, 4, 699, 303, 90, 8, 98, 14, 23, 9, 7, 2, 130, 31, 31, 0, 61, 9, 10, 113, 188, 418, 309, 178, 55, 207, 176, 484, 45, 375, 188, 107, 81,... $ `2PA` &lt;dbl&gt; 70, 73, 710, 333, 92, 15, 1255, 382, 11, 79, 243, 99, 381, 141, 9, 1262, 694, 149, 10, 189, 24, 50, 22, 16, 6, 268, 59, 57, 2, 149, 26, 16, 269, 377, 875, 606, 376, 116, 425, 402, 954, 96, 721, ... $ `2P%` &lt;dbl&gt; 0.443, 0.384, 0.631, 0.523, 0.413, 0.333, 0.526, 0.599, 0.545, 0.506, 0.432, 0.535, 0.556, 0.539, 0.444, 0.554, 0.437, 0.604, 0.800, 0.519, 0.583, 0.460, 0.409, 0.438, 0.333, 0.485, 0.525, 0.544... $ `eFG%` &lt;dbl&gt; 0.540, 0.496, 0.629, 0.512, 0.485, 0.333, 0.520, 0.596, 0.273, 0.505, 0.503, 0.515, 0.549, 0.568, 0.444, 0.545, 0.476, 0.577, 0.524, 0.542, 0.553, 0.443, 0.409, 0.438, 0.333, 0.557, 0.563, 0.622... $ FT &lt;dbl&gt; 39, 49, 160, 129, 22, 2, 334, 114, 7, 11, 59, 28, 104, 72, 5, 487, 148, 39, 5, 76, 2, 7, 4, 4, 0, 164, 17, 17, 0, 20, 20, 6, 32, 80, 259, 182, 108, 35, 59, 160, 292, 22, 124, 148, 101, 47, 30, 1... $ FTA &lt;dbl&gt; 46, 60, 287, 179, 26, 6, 399, 147, 9, 21, 80, 38, 146, 93, 6, 641, 193, 62, 6, 89, 3, 12, 13, 12, 1, 189, 22, 22, 0, 25, 26, 10, 71, 102, 313, 226, 130, 44, 78, 201, 368, 33, 159, 163, 109, 54, ... $ `FT%` &lt;dbl&gt; 0.848, 0.817, 0.557, 0.721, 0.846, 0.333, 0.837, 0.776, 0.778, 0.524, 0.738, 0.737, 0.712, 0.774, 0.833, 0.760, 0.767, 0.629, 0.833, 0.854, 0.667, 0.583, 0.308, 0.333, 0.000, 0.868, 0.773, 0.773... $ ORB &lt;dbl&gt; 26, 40, 384, 118, 4, 3, 246, 144, 4, 20, 97, 25, 84, 94, 5, 156, 67, 44, 1, 33, 3, 4, 9, 7, 2, 30, 7, 5, 2, 4, 5, 1, 69, 15, 77, 70, 57, 15, 130, 25, 61, 14, 97, 19, 13, 6, 60, 8, 40, 18, 16, 70... $ DRB &lt;dbl&gt; 88, 216, 301, 263, 62, 12, 389, 244, 7, 27, 428, 68, 312, 237, 4, 597, 386, 140, 24, 261, 12, 34, 38, 30, 8, 130, 89, 76, 13, 120, 25, 7, 291, 186, 391, 339, 249, 65, 304, 223, 302, 57, 317, 131... $ TRB &lt;dbl&gt; 114, 256, 685, 381, 66, 15, 635, 388, 11, 47, 525, 93, 396, 331, 9, 753, 453, 184, 25, 294, 15, 38, 47, 37, 10, 160, 96, 81, 15, 124, 30, 8, 360, 201, 468, 409, 306, 80, 434, 248, 363, 71, 414, ... $ AST &lt;dbl&gt; 28, 57, 88, 101, 30, 3, 152, 49, 12, 9, 84, 25, 202, 60, 0, 361, 103, 55, 35, 105, 9, 18, 3, 2, 1, 287, 31, 26, 5, 38, 47, 5, 376, 434, 152, 331, 349, 54, 93, 228, 373, 31, 123, 148, 103, 45, 10... $ STL &lt;dbl&gt; 38, 33, 92, 32, 4, 2, 43, 28, 3, 11, 79, 15, 115, 24, 1, 109, 47, 52, 13, 98, 8, 2, 2, 1, 1, 54, 8, 7, 1, 16, 26, 2, 88, 35, 49, 82, 65, 23, 22, 100, 96, 15, 36, 66, 46, 20, 35, 21, 22, 26, 18, ... $ BLK &lt;dbl&gt; 8, 29, 78, 41, 9, 1, 90, 88, 2, 3, 40, 7, 60, 21, 3, 106, 49, 14, 0, 13, 3, 3, 4, 2, 2, 0, 7, 5, 2, 2, 6, 1, 43, 3, 14, 52, 23, 6, 51, 45, 36, 7, 43, 12, 3, 9, 56, 12, 53, 28, 5, 21, 1, 95, 16, ... $ TOV &lt;dbl&gt; 25, 60, 128, 66, 21, 1, 111, 82, 9, 19, 79, 16, 94, 42, 2, 223, 99, 45, 13, 52, 15, 8, 9, 5, 4, 123, 19, 15, 4, 23, 18, 4, 136, 143, 118, 149, 131, 46, 80, 157, 214, 24, 143, 94, 54, 40, 51, 47,... $ PF &lt;dbl&gt; 124, 149, 215, 138, 56, 11, 162, 147, 15, 49, 136, 54, 114, 126, 1, 231, 197, 130, 18, 132, 22, 11, 20, 14, 6, 94, 54, 47, 7, 46, 33, 12, 117, 83, 94, 168, 72, 91, 200, 147, 160, 36, 145, 91, 41... $ PTS &lt;dbl&gt; 353, 411, 1056, 477, 179, 12, 1735, 587, 19, 103, 644, 236, 585, 617, 13, 2014, 1261, 438, 48, 782, 54, 77, 22, 18, 4, 766, 259, 226, 33, 175, 71, 38, 528, 801, 1452, 1268, 740, 307, 482, 836, 1... $ trueShooting &lt;dbl&gt; 0.5670865, 0.5250383, 0.6298611, 0.5695386, 0.5160286, 0.3401361, 0.5697641, 0.6357217, 0.3659476, 0.5137670, 0.5217110, 0.5395026, 0.5823909, 0.5922215, 0.5584192, 0.5979668, 0.5032245, 0.58669... $ effectiveFG &lt;dbl&gt; 0.5395189, 0.4958904, 0.6292135, 0.5117647, 0.4845679, 0.3333333, 0.5200445, 0.5957179, 0.2727273, 0.5054945, 0.5025773, 0.5148515, 0.5490868, 0.5677083, 0.4444444, 0.5445792, 0.4764555, 0.57658... $ shootingDif &lt;dbl&gt; 0.1720864927, 0.1690383240, 0.0008611442, 0.0575386379, 0.1150285978, 0.0071360544, 0.0597640815, 0.0467217120, 0.0929476117, 0.0297669593, 0.1267109527, 0.1085025604, 0.0553908888, 0.1612214543... Merging Data Merging data is yet another very common data task, as data often comes from multiple sources. In order to do this, we need some common identifier among the sources by which to join them. The following is a list of dplyr join functions. inner_join: return all rows from x where there are matching values in y, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned. left_join: return all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned. right_join: return all rows from y, and all columns from x and y. Rows in y with no match in x will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned. semi_join: return all rows from x where there are matching values in y, keeping just columns from x. It differs from an inner join because an inner join will return one row of x for each matching row of y, where a semi join will never duplicate rows of x. anti_join: return all rows from x where there are not matching values in y, keeping just columns from x. full_join: return all rows and all columns from both x and y. Where there are not matching values, returns NA for the one missing. Probably the most common is a left join, where we kind of have one primary data set, and are adding data from another source to it. The following is a simple demonstration. band_members # A tibble: 3 x 2 Name Band &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise 2 Francis Pixies 3 Bubba The New Year band_instruments # A tibble: 3 x 2 Name Instrument &lt;chr&gt; &lt;chr&gt; 1 Seth Synthesizer 2 Francis Guitar 3 Bubba Guitar left_join(band_members, band_instruments) Joining, by = &quot;Name&quot; # A tibble: 3 x 3 Name Band Instrument &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise Synthesizer 2 Francis Pixies Guitar 3 Bubba The New Year Guitar When we don’t have a one to one match, the result of the different types of join will become more apparent. band_members # A tibble: 4 x 2 Name Band &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise 2 Francis Pixies 3 Bubba The New Year 4 Stephen Pavement band_instruments # A tibble: 4 x 2 Name Instrument &lt;chr&gt; &lt;chr&gt; 1 Seth Synthesizer 2 Francis Guitar 3 Bubba Guitar 4 Steve Rage left_join(band_members, band_instruments) Joining, by = &quot;Name&quot; # A tibble: 4 x 3 Name Band Instrument &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise Synthesizer 2 Francis Pixies Guitar 3 Bubba The New Year Guitar 4 Stephen Pavement &lt;NA&gt; right_join(band_members, band_instruments) Joining, by = &quot;Name&quot; # A tibble: 4 x 3 Name Band Instrument &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise Synthesizer 2 Francis Pixies Guitar 3 Bubba The New Year Guitar 4 Steve &lt;NA&gt; Rage inner_join(band_members, band_instruments) Joining, by = &quot;Name&quot; # A tibble: 3 x 3 Name Band Instrument &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise Synthesizer 2 Francis Pixies Guitar 3 Bubba The New Year Guitar full_join(band_members, band_instruments) Joining, by = &quot;Name&quot; # A tibble: 5 x 3 Name Band Instrument &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Seth Com Truise Synthesizer 2 Francis Pixies Guitar 3 Bubba The New Year Guitar 4 Stephen Pavement &lt;NA&gt; 5 Steve &lt;NA&gt; Rage anti_join(band_members, band_instruments) Joining, by = &quot;Name&quot; # A tibble: 1 x 2 Name Band &lt;chr&gt; &lt;chr&gt; 1 Stephen Pavement anti_join(band_instruments, band_members) Joining, by = &quot;Name&quot; # A tibble: 1 x 2 Name Instrument &lt;chr&gt; &lt;chr&gt; 1 Steve Rage Merges can get quite complex, and involve multiple data sources. In many cases you may have to do a lot of processing before getting to the merge, but dplyr’s joins will help quite a bit. tidyr The tidyr package can be thought of as a specialized subset of dplyr’s functionality, as well as an update to the previous reshape and reshape2 packages7. Some of its functions for manipulating data you’ll want to be familiar with are: gather: wide to long spread: long to wide unite: paste together multiple columns into one separate: complement of unite unnest: unnest ‘list columns’ The following example shows how we take a ‘wide-form’ data set, where multiple columns represent different stock prices, and turn it into two columns, one representing stock name, and one for the price. library(tidyr) stocks &lt;- data.frame( time = as.Date(&#39;2009-01-01&#39;) + 0:9, X = rnorm(10, 0, 1), Y = rnorm(10, 0, 2), Z = rnorm(10, 0, 4) ) stocks %&gt;% head time X Y Z 1 2009-01-01 -0.06546867 0.5816049 -2.8910160 2 2009-01-02 2.30947902 -1.9926465 -1.3353190 3 2009-01-03 -0.33040393 0.3562913 3.1471085 4 2009-01-04 -0.83435315 1.6770826 3.0540476 5 2009-01-05 -0.52591356 0.1699624 -1.5776236 6 2009-01-06 0.29455561 0.7914883 -0.5646063 stocks %&gt;% gather(stock, price, -time) %&gt;% head() time stock price 1 2009-01-01 X -0.06546867 2 2009-01-02 X 2.30947902 3 2009-01-03 X -0.33040393 4 2009-01-04 X -0.83435315 5 2009-01-05 X -0.52591356 6 2009-01-06 X 0.29455561 Note that the latter is an example of tidy data while the former is not. Why do we generally prefer such data? Precisely because the most common data operations, grouping, filtering, etc., would work notably more efficiently with such data. This is especially the case for visualization. The following demonstrates the separate function utilized for a very common data processing task- dealing with names. Here’ we’ll separate player into first and last names based on the space. bball %&gt;% separate(Player, into=c(&#39;firstName&#39;, &#39;lastName&#39;), sep=&#39; &#39;) %&gt;% select(1:5) %&gt;% head() Rk firstName lastName Pos Age 1 1 Alex Abrines SG 24 2 2 Quincy Acy PF 27 3 3 Steven Adams C 24 4 4 Bam Adebayo C 20 5 5 Arron Afflalo SG 32 6 6 Cole Aldrich C 29 Note that this won’t necessarily apply to every name. So further processing may be required. More Tidyverse dplyr functions: There are over a hundred functions that perform very common tasks. You really need to be aware of them as their use will come up often. broom: Convert statistical analysis objects from R into tidy data frames, so that they can more easily be combined, reshaped and otherwise processed with tools like dplyr, tidyr and ggplot2. tidy*: a lot of packages out there that are now ‘tidy’, though not a part of the official tidyverse. Some examples of the ones I’ve used: tidycensus tidybayes tidytext modelr Seriously, there are a lot. Personal Opinion The dplyr grammar is clear for a lot of standard data processing tasks, and some not so common. Extremely useful for data exploration and visualization. No need to create/overwrite existing objects Can overwrite columns and use as they are created Makes it easy to look at anything, and do otherwise tedious data checks Drawbacks: Not as fast as data.table or even some base R approaches for many things8 The mindset can make for unnecessary complication e.g. There is no need to pipe to create a single new variable Some approaches, are not very intuitive Notably less ability to work with some very common data structures (e.g. matrices) All in all, if you’ve only been using base R approaches, the tidyverse will change your R life! It makes all the sorts of things you do all the time easier and clearer. Highly recommended! tidyverse Exercises Exercise 0 Install and load the dplyr ggplot2movies packages. Look at the help file for the movies data set, which contains data from IMDB. install.packages(&#39;ggplot2movies&#39;) library(ggplot2movies) data(&#39;movies&#39;) Exercise 1 Using the movies data set, perform each of the following actions separately. Exercise 1a Use mutate to create a centered version of the rating variable. A centered variable is one whose mean has been subtracted from it. The process will take the following form: data %&gt;% mutate(new_var_name = &#39;?&#39;) Exercise 1b Use filter to create a new data frame that has only movies from the years 2000 and beyond. Use the greater than or equal operator &gt;=. Exercise 1c Use select to create a new data frame that only has the title, year, budget, length, rating and votes variables. There are at least 3 ways to do this. Exercise 1d Rename the length column to length_in_min (i.e. length in minutes). Exercise 2 Use group_by to group the data by year, and summarize to create a new variable that is the average budget. The summarize function works just like mutate in this case. Use the mean function to get the average, but you’ll also need to use the argument na.rm = TRUE within it because the earliest years have no budget recorded. Exercise 3 Use gather to create a ‘tidy’ data set from the following. dat = data_frame(id = 1:10, x = rnorm(10), y = rnorm(10)) Exercise 4 Now put several actions together in one set of piped operations. Filter movies released after 1990 select the same variables as before but also the mpaa, Action, and Drama variables group by mpaa and (your choice) Action or Drama get the average rating It should spit out something like the following: # A tibble: 10 x 3 # Groups: mpaa [?] mpaa Drama AvgRating &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 &quot;&quot; 0 5.94 2 &quot;&quot; 1 6.20 3 NC-17 0 4.28 4 NC-17 1 4.62 5 PG 0 5.19 6 PG 1 6.15 7 PG-13 0 5.44 8 PG-13 1 6.14 9 R 0 4.86 10 R 1 5.94 You may be thinking- ‘it’s 2017 and why on earth would anyone do that?!’. Peruse most sports websites and you’ll see that fundamental web design basics escape them. See also, financial sites.↩ If you’re following good programming practices, you’ll likely never use slice.↩ For only one function, you could have just supplied as.numeric as the second argument.↩ As Hadley Wickham is from New Zealand, and his examples use summarise, you’ll probably see it about as much as you do the other spelling, especially since it will come up first in autocomplete.↩ Some still use reshape2 but there is no reason to and it is no longer developed.↩ There is multidplyr, but it doesn’t appear to have been updated since well before dplyr itself underwent major changes.↩ "],
["1_07_datatable.html", "data.table", " data.table Another package for data processing that has been useful to many is data.table. It works in a notably different way than dplyr. However, you’d use it for the same reasons, e.g. subset, grouping, update, ordered joins etc., but with key advantages in speed and memory efficiency. Like dplyr, the data objects are both data.frames and a package specific class. library(data.table) dt = data.table(x=sample(1:10, 6), g=1:3, y=runif(6)) class(dt) [1] &quot;data.table&quot; &quot;data.frame&quot; Basics In general, data.table works with brackets as in base R data frames. However, in order to use data.table effectively you’ll need to forget the data frame similarity. The brackets actually work like a function call, with several key arguments. Consider the following notation to start. x[i, j, by, keyby, with = TRUE, ...] Importantly: you don’t use the brackets as you would with data.frames. What i and j can be are fairly complex. In general, you use i for filtering by rows. dt[2] dt[2,] x g y 1: 6 2 0.8306729 x g y 1: 6 2 0.8306729 You use j to select (by name!) or create new columns. We can define a new column with the := operator. dt[,x] dt[,z := x+y] # dt now has a new column dt[,z] dt[g&gt;1, mean(z), by=g] dt [1] 4 6 8 1 5 7 [1] 4.517813 6.830673 8.259457 1.308802 5.746635 7.306428 g V1 1: 2 6.288654 2: 3 7.782943 x g y z 1: 4 1 0.5178128 4.517813 2: 6 2 0.8306729 6.830673 3: 8 3 0.2594571 8.259457 4: 1 1 0.3088019 1.308802 5: 5 2 0.7466354 5.746635 6: 7 3 0.3064280 7.306428 Because j is an argument, dropping columns is awkward. dt[,-y] # creates negative values of y dt[,-&#39;y&#39;, with=F] # drops y, but now needs quotes ## dt[,y := NULL] # drops y, but this is just a base R approach ## dt$y = NULL [1] -0.5178128 -0.8306729 -0.2594571 -0.3088019 -0.7466354 -0.3064280 x g z 1: 4 1 4.517813 2: 6 2 6.830673 3: 8 3 8.259457 4: 1 1 1.308802 5: 5 2 5.746635 6: 7 3 7.306428 Data table does not make unnecessary copies. For example if we do the following… DT = data.table(A=5:1,B=letters[5:1]) DT2 = DT DT3 = copy(DT) DT2 and DT are just names for the same table. You’d actually need to use the copy function to make an explicit copy, otherwise whatever you do to DT2 will be done to DT. DT2[,q:=1] DT A B q 1: 5 e 1 2: 4 d 1 3: 3 c 1 4: 2 b 1 5: 1 a 1 DT3 A B 1: 5 e 2: 4 d 3: 3 c 4: 2 b 5: 1 a Grouped operations We can now attempt a ‘group-by’ operation, along with creation of a new variable. Note that these operations actually modify the dt object in place, a key distinction with dplyr. Fewer copies means less of a memory hit. dt1 = dt2 = dt dt[,sum(x,y), by=g] # sum of all x and y values g V1 1: 1 5.826615 2: 2 12.577308 3: 3 15.565885 dt1[,newvar := sum(x), by=g] # add new variable to the original data dt1 x g y z newvar 1: 4 1 0.5178128 4.517813 5 2: 6 2 0.8306729 6.830673 11 3: 8 3 0.2594571 8.259457 15 4: 1 1 0.3088019 1.308802 5 5: 5 2 0.7466354 5.746635 11 6: 7 3 0.3064280 7.306428 15 We can also create groupings on the fly. For a new summary data set, we’ll take the following approach- we create a grouping based on whether g is a value of one or not, then get the mean and sum of x for those two categories. The corresponding dplyr approach is also shown (but not evaluated) for comparison. dt2[, list(mean_x = mean(x), sum_x = sum(x)), by = g==1] g mean_x sum_x 1: TRUE 2.5 5 2: FALSE 6.5 26 ## dt2 %&gt;% group_by(g==1) %&gt;% summarise(mean_x=mean(x), sum_x=sum(x)) Faster! As mentioned, the reason to use data.table is speed. If you have large data or large operations it’ll be useful. Joins Joins can not only be faster but and easy to do. Note that the i argument can be a data.table object itself. I compare its speed to the comparable dplyr’s left_join function. dt1 = setkey(dt1, x) dt1[dt2] left_join(dt1, dt2, by=&#39;x&#39;) func mean (microseconds) dt_join 678.1 dplyr_join 1514 Group by We can use the setkey function to order a data set by a certain column(s). This ordering is done by reference; again, no copy is made. Doing this will allow for faster grouped operations, though you likely will only see the speed gain with very large data. The timing regards creating a new variable test_dt0 = data.table(x=rnorm(10000000), g = sample(letters, 10000000, replace=T)) test_dt1 = copy(test_dt0) test_dt2 = setkey(test_dt1, g) identical(test_dt0, test_dt1) [1] FALSE identical(test_dt1, test_dt2) [1] TRUE test_dt0 = test_dt0[,mean:=mean(x),by=g] test_dt1 = test_dt1[,mean:=mean(x),by=g] test_dt2 = test_dt2[,mean:=mean(x),by=g] func mean (milliseconds) test_dt0 427.4 test_dt1 146.7 test_dt2 146.7 String matching The chin function returns a vector of the positions of (first) matches of its first argument in its second, where both arguments are character vectors. Essentially it’s just like the %in% function for character vectors. Consider the following. We sample the first 14 letters 1000 times with replacement and see which ones match in a subset of another subset of letters. I compare the same operation to stringr and the stringi package whose functionality stringr using. lets_1 = sample(letters[1:14], 1000, replace=T) lets_1 %chin% letters[13:26] %&gt;% head(10) [1] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE # stri_detect_regex(lets_1, paste(letters[13:26], collapse=&#39;|&#39;)) Reading files If you use data.table for nothing else, you’d still want to consider it strongly for reading in large text files. The function fread may be quite useful in being memory efficient too. I compare it to readr. fread(&#39;data/cars.csv&#39;) func mean (microseconds) dt 719.4 readr 5079 More speed The following demonstrates some timings from here. I reproduced it on my own machine based on 50 million observations. The grouped operations that are applied are just a sum and length on a vector. By the way, never, ever use aggregate. For anything. fun elapsed 1: aggregate 114.35 2: by 24.51 3: sapply 11.62 4: tapply 11.33 5: dplyr 10.97 6: lapply 10.65 7: data.table 2.71 Ever. Really. Another thing to note is that the tidy approach is more about clarity and code efficiency relative to base R, as well as doing important background data checks and returning more usable results. In practice, it likely won’t be notably faster except in some cases, like with aggregate. Pipe with data.table Piping can be done with data.table objects too, using the brackets, but it’s awkward at best. mydt[,newvar:=mean(x),][,newvar2:=sum(newvar), by=group][,-&#39;y&#39;, with=FALSE] mydt[,newvar:=mean(x), ][,newvar2:=sum(newvar), by=group ][,-&#39;y&#39;, with=FALSE ] Probably better to just use a standard pipe and dot approach if you really need it. mydt[,newvar:=mean(x),] %&gt;% .[,newvar2:=sum(newvar), by=group] %&gt;% .[,-&#39;y&#39;, with=FALSE] Summary Faster and more memory-efficient methods are great to have. If you have large data this is one package that can help. Especially for group-by and joins. Drawbacks: Complex The syntax can be awkward It doesn’t work like a data.frame, which can be confusing Piping with brackets isn’t really feasible, and the dot approach is awkward Does not have its own ‘verse’, though many packages use it If speed and/or memory is (potentially) a concern, data.table. For interactive exploration, dplyr. Piping allows one to use both, so no need to choose. And on the horizon… dtplyr Coming soon to an R near you, or possibly abandonware, only time will tell. However, what the dtplyr package purports to do is implement the data.table back-end for dplyr so that you can seamlessly use them together. The following shows times for a grouped operation of a data frame of two variables, a random uniform draw of 5e7 values, and a grouping variable of 500k groups. package timing dplyr 10.97 data.table 2.71 dtplyr 2.7 Just for giggles I did the same in Python with a pandas DataFrame and group-by operation, and it took over seven seconds. But remember, R is slow! As of this writing, dplyr went through a notable update that changed some core functionality. Unfortunately, dtplyr has not been updated since then, so it will likely only work for the most standard of operations at present. data.table Exercises Exercise 0 Install and load the data.table package. Create the following data table. mydt = data.table(expand.grid(x=1:3, y=c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;)), z=sample(1:20, 9)) Exercise 1 Create a new object that contains only the ‘a’ group. Think back to how you use a logical to select rows. Exercise 2 Create a new object that is the sum of z grouped by x. You don’t need to name the sum variable. "],
["2_01_thinkingvis.html", "Part II: Visualization Thinking Visually", " Part II: Visualization Thinking Visually Information A starting point for data visualization regards the information you want to display, and then how you want to display it in order to tell the data’s story. As in statistical modeling, parsimony is the goal, but not at the cost of the more compelling story. We don’t want to waste the time of the audience or be redundant, but we also want to avoid unnecessary clutter, chart junk, and the like. We’ll start with a couple examples. Consider the following. So what’s wrong with this? Plenty. Aside from being boring, the entire story can be said with a couple words- males are taller than females (even in the Star Wars universe). There is no reason to have a visualization. And if a simple group difference is the most exciting thing you have to talk about, not many are going to be interested. Minor issues can also be noted, including unnecessary border around the bars, unnecessary vertical gridlines, and an unnecessary X axis label. You might think the following is an improvement, but I would say it’s even worse. Now the y axis has been changed to distort the difference, perceptually suggesting a height increase of over 54%. Furthermore, color is used but the colors are chosen poorly, and add no information, thus making the legend superfluous. And finally, the above doesn’t even convey the information people think it does, assuming they are even standard error bars, which one typically has to guess about in many journal visualizations of this kind9. Now we add more information, but more problems! The above has unnecessary border, gridlines, and emphasis. The labels, while possibly interesting, do not relate anything useful to the graph, and many are illegible. It imposes a straight (and too wide of a) straight line on a nonlinear relationship. And finally, color choice is both terrible and tends to draw one’s eye to the female data points. Here is what it looks like to someone with the most common form of colorblindness. If the points were less clumpy on gender, it would be very difficult to distinguish the groups. And here is what it might look like when printed. Now consider the following. We have six pieces of information in one graph- name (on hover), homeworld (shape), age (size), gender (color), mass (x), and height (y). The colors are evenly spaced from one another, and so do not draw one’s attention to one group over another, or even to the line over groups. Opacity allows the line to be added and the points to overlap without loss of information. We technically don’t need a caption, legend or gridlines, because hovering over the data tells us everything we’d want to know about a given data point. Whether this particular scheme is something you’d prefer or not, the point is that we get quite a bit of information without being overwhelming, and the data is allowed to express itself cleanly. Here are some things to keep in mind when creating visualizations for scientific communication. Your audience isn’t dumb Assume your audience, which in academia is full of people with advanced degrees or those aspiring to obtain one, and in other contexts comprises people who are interested in your story, can handle more than a bar graph. If the visualization is good and well-explained10, they’ll be fine. See the data visualization and maps sections of 2016: The Year in Visual Stories and Graphics at the New York Times. Good data visualization of even complex relationships can be appreciated by more than an academic audience. Assume you can at least provide visualizations on that level of complexity and be okay. It won’t always work, but at least put the same effort you’d appreciate yourself. Clarity is key Sometimes the clearest message is a complicated one. That’s okay, science is an inherently fuzzy process. Make sure your visualization tells the story you think is important, and don’t dumb the story down in the visualization. People will likely remember the graphic before they’ll remember the table of numbers. By the same token, don’t needlessly complicate something that is straightforward. Perhaps a scatter plot with some groupwise coloring is enough. That’s fine. All of this is easier said than done, and there is no right way to do data visualizations. Prepare to experiment, and focus on visuals that display patterns that will be more readily perceived. Avoid clutter In striving for clarity, there are pitfalls to avoid. Gridlines, 3d, unnecessary patterning, and chartjunk in general will only detract from the message. As an example, gridlines might even seem necessary, but even faint ones can potentially hinder the pattern recognition you hope will take place, perceptually imposing clumps of data that do not exist. In addition, they practically insist on a level of data precision that in many situations you simply don’t have. What’s more, with interactivity they literally convey nothing additional, as a simple hover-over or click on a data point will reveal the precise values. Use sparingly, if at all. Color isn’t optional It’s odd for me to have to say this, as it’s been the case for many years, but no modern scientific outlet should be a print-first outfit, and if they are, you shouldn’t care to send your work there. The only thing you should be concerned with is how it will look online, because that’s how people will interact with your work first and foremost. That means that color is essentially a requirement for any visualization, so use it well in yours. Think interactively It might be best to start by making the visualization you want to make, with interactivity and anything else you like. You can then reduce as necessary for publication or other outlets, and keep the fancy one as supplemental, or accessible on your own website to show off. Color There is a lot to consider regarding color. Until recently, the default color schemes of most visualization packages were poor at best. Thankfully, ggplot2, its imitators and extenders, in both the R world and beyond, have made it much easier to have a decent color scheme by default11. However, the defaults are still potentially problematic, so you should be prepared to go with something else. In other cases, you may just simply prefer something else. For example, for me, the gray background of ggplot2 defaults is something I have to remove for every plot12. Viridis A couple packages will help you get started in choosing a decent color scheme. One is viridis. As stated in the package description: These color maps are designed in such a way that they will analytically be perfectly perceptually-uniform, both in regular form and also when converted to black-and-white. They are also designed to be perceived by readers with the most common form of color blindness. So basically you have something that will take care of your audience without having to do much. There are four palettes. These color schemes might seem a bit odd from what you’re used to. But recall that the goal is good communication, and these will allow you to convey information accurately, without implicit bias, and be acceptable in different formats. In addition, there is ggplot2 functionality to boot, e.g. scale_color_viridis, and it will work for discrete or continuously valued data. For more, see the vignette. I also invite you to watch the introduction of the original module in Python, where you can learn more about the issues in color selection, and why viridis works. RColorBrewer Color Brewer offers another collection of palettes that will generally work well in a variety of situations. While there are print and color-blind friendly palettes, not all adhere to those restrictions. Specifically though, you have palettes for the following data situations: Qualitative (e.g. Dark213) Sequential (e.g. Reds) Diverging (e.g. RdBu) There is a ggplot2 function, scale_color_brewer, you can use as well. For more, see colorbrewer.org. There you can play around with the palettes to help make your decision. In R, you have three schemes that work well right out of the box: ggplot2 default palette viridis RColorBrewer Furthermore, they’ll work well with discrete or continuous data. You will have to do some work to come up with better, so they should be your default. Sometimes though, one can’t help oneself. Contrast Thankfully, websites have mostly gotten past the phase where there text looks like this. The goal of scientific communication is to, well, communicate. Making text hard to read is pretty much antithetical to this. So contrast comes into play with text as well as color. In general, you should consider a 7 to 1 contrast ratio for text, minimally 4 to 1. -Here is text at 2 to 1 -Here is text at 4 to 1 -Here is text at 7 to 1 -Here is black I personally don’t like stark black, and find it visually irritating, but obviously that would be fine to use for most people. Scaling Size You might not be aware, but there is more than one way to scale the size of objects, e.g. in a scatterplot. Consider the following, where in both cases dots are scaled by the person’s body-mass index (BMI). What’s the difference? The first plot scales the dots by their area, while the second scales the radius, but otherwise they are identical. It’s not generally recommended to scale the radius, as our perceptual system is more attuned to the area. Packages like ggplot2 and plotly will automatically do this, but some might not, so you should check. Transparency Using transparency is a great way to keep detailed information available to the audience without being overwhelming. Consider the following. Fifty individual trajectories are shown on the left, but it doesn’t cause any issue graphically. The right has 10 lines plus a fitted line, 20 points and a ribbon to provide a sense of variance. Using transparency and a viridis color scheme allows it to be perceived cleanly. Without transparency, it just looks ugly, and notably busier if nothing else. In addition, transparency can be used to add additional information to a plot. In the following scatter plot, we can get a better sense of data density from the fact that the plot is darker where points overlap more. Here we apply it to a density plot to convey a group difference in distributions, while still being able to visualize the whole distribution of each group. Had we not done so, we might not be able to tell what’s going on with some of the groups at all. In general, a good use of transparency can potentially help any visualization, but consider it especially when trying to display many points, or otherwise have overlapping data. Accessibility Among many things (apparently) rarely considered in typical academic or other visualization is accessibility. The following definition comes from the World Wide Web Consortium. Web accessibility means that people with disabilities can use the Web. More specifically, Web accessibility means that people with disabilities can perceive, understand, navigate, and interact with the Web, and that they can contribute to the Web. Web accessibility also benefits others, including older people with changing abilities due to aging. The main message to get is that not everyone is able to use the web in the same manner. While you won’t be able to satisfy everyone who might come across your work, putting a little thought into your offering can go along way, and potentially widen your audience. We talked about this previously, but when communicating visually, one can do simple things like choosing a colorblind-friendly palette, or using a font contrast that will make it easier on the eyes of those reading your work. There are even browser plugins to test your web content for accessibility. In addition, there are little things like adding a title to inserted images, making links more noticeable etc., all of which can help consumers of your information. File Types It’s one thing to create a visualization, but at some point you’re likely going to want to share it. RStudio will allow for the export of any visualization created in the Plots or Viewer tab. In addition, various packages may have their own save function, that may allow you to specify size, type or other aspects. Here we’ll discuss some of the options. png: These are relatively small in size and ubiquitous on the web. You should feel fine in this format. It does not scale however, so if you make a smaller image and someone zooms, it will become blurry. gif: These are the type used for all the stupid animations you see on the web. Using them is fine if you want to make an animation, but know that it can go longer than a couple seconds and there is no requirement for it to be asinine. jpg: Commonly used for photographs, which isn’t the case with data generated graphs. Given their relative size I don’t see much need for these. svg: These take a different approach to imaging and can scale. You can make a very small one and it (potentially) can still look great when zoomed in to a much larger size. Often useful for logos, but possibly in any situation. As I don’t know what screen will see my visualizations, I generally opt for svg. It may be a bit slower/larger, but in my usage and for my audience size, this is of little concern relative to it looking proper. They also work for pdf if you’re still creating those, and there are also lighter weight versions in R, e.g. svglite. Beyond that I use png, and have no need for others. Here is a discussion on stackexchange that summarizes some of the above. The initial question is old but there have been recent updates to the responses. Summary The goal of this section was mostly just to help you realize that there are many things to consider when visualizing information and attempting to communicate the contents of data. The approach is not the same as what one would do in say, an artistic venture, or where there is nothing specific to impart to an audience. Even some of the most common things you see published are fundamentally problematic, so you can’t even use what people traditionally do as a guide. However, there are many tools available to help you. Another thing to keep in mind is that there is no right way to do a particular visualization, and many ways, to have fun with it. A casual list of things to avoid I’m just putting things that come to mind here as I return to this document. Mostly it is personal opinion, though often based on various sources in the data visualization realm or simply my own experience. Pie Pie charts and their cousins, e.g. bar charts (and stacked versions), wind rose plots, radar plots etc., either convey too little information or make otherwise simple information more difficult to process perceptually. Histograms Anyone that’s used R’s hist function knows the frustration here. Use density plots instead. They convey the same information but better, and typical defaults are usually fine. Using 3D without adding any communicative value You will often come across use of 3D in scientific communication which is fairly poor and makes the data harder to interpret. In general, when going beyond two dimensions, your first thought should be to use color, size, etc. and finally prefer interactivity to 3D. Where it is useful is in things like showing structure (e.g. molecular, geographical), or continuous multi-way interactions. Using too many colors Some put a completely non-scientifically based number on this, but the idea holds. For example, if you’re trying to show U.S. state grouping by using a different color for all 50 states, no one’s going to be able to tell the yellow for Alabama vs. the slightly different yellow for Idaho. Alternatives would be to show the information via a map or use a hover over display. Showing maps that just display population Most of the maps I see on the web cover a wide range of data and can be very visually appealing, but pretty much just tell me where the most populated areas are, which is utterly uninteresting. Make sure that your geographical depiction is more informative than this. Biplots A lot of folks doing PCA resort to biplots for interpretation, where a graphical model would be much more straightforward. See this chapter for example. Thinking Visually Exercises Exercise 1 The following uses the diamonds data set that comes with ggplot2. Use the scale_color_viridis function to add a more accessible palette. library(ggplot2); library(viridis) ggplot(aes(x=carat, y=price), data=diamonds) + geom_point(aes(color=price)) + ???? Exercise 2 Now color it by the cut. To use the scale_color_viridis approach, you’ll have to change one of the arguments to the function (set discrete=T). Reproduce this but using one of the other viridis palettes. See the helpfile via ?scale_color_viridis to see how to change the palette. Thinking exercises For your upcoming presentation, who is your audience? Error bars for group means can overlap and still be statistically different (the test regards the difference in means). Furthermore visuals of this sort often don’t bother to say whether it is standard deviation, standard error, or 2*standard error, or even something else.↩ People seem to think there are text limits for captions. There are none.↩ Even Matlab finally joined the club, except that they still screwed up with their default coloring scheme.↩ Hadley states “The grey background gives the plot a similar colour (in a typographical sense) to the remainder of the text, ensuring that the graphics fit in with the flow of a text without jumping out with a bright white background. Finally, the grey background creates a continuous field of colour which ensures that the plot is perceived as a single visual entity.”. The part about it being ugly is apparently left out. ☺ Also, my opinion is that it has the opposite effect, making the visualization jump out because nothing on the web is gray by default. If anything the page background is white, and having a white/transparent background would perhaps be better, but honestly, don’t you want a visualization to jump out?↩ Don’t even think about asking what the Dark1 palette is.↩ "],
["2_02_ggplot2.html", "ggplot2", " ggplot2 The most popular visualization package in R is ggplot2. It’s so popular, it or its aesthetic is copied in other languages/programs as well. It entails a grammar of graphics (hence the gg), and learning that grammar is key to using it effectively. Some of the strengths of ggplot2 include: The ease of getting a good looking plot Easy customization A lot of necessary data processing is done for you Clear syntax Easy multidimensional approach Decent default color scheme as a default Lots of extensions Every graph is built from the same few parts, and it’s important to be aware of a few key ideas, which we will cover in turn. Layers (and geoms) Piping Aesthetics Facets Scales Themes Extensions Note that while you can obviously use base R for visualization, it’s never going to be easier, nor as flexible as ggplot2. If you’re used to using base R visuals, you should be prepared to leave them behind. Layers In general, we start with a base layer and add to it. In most cases you’ll start as follows. # recall that starwars is in the dplyr package ggplot(aes(x=height, y=mass), data=starwars) This would just produce a plot background, but nothing else. However, with the foundation in place, we’re now ready to add something to it. Let’s add some points (the outlier is Jabba the Hut). ggplot(aes(x=height, y=mass), data=starwars) + geom_point() Perhaps we want to change labels or theme. These would be additional layers to the plot. ggplot(aes(x=height, y=mass), data=starwars) + geom_point() + labs(x=&#39;Height in cm&#39;, y=&#39;Weight in kg&#39;) + theme_dark() Each layer is consecutively added by means of a pipe operator, and layers may regard geoms, scales, labels, facets etc. You may have many different layers to produce one plot, and there really is no limit. However some efficiencies may be possible for a given situation. For example, it’s more straightforward to use geom_smooth than calculate fits, standard errors etc. and then add multiple geoms to produce the same thing. This is the sort of thing you’ll get used to as you use ggplot more. Piping As we saw, layers are added via piping (+). The first layers added after the base are typically geoms, or geometric objects that represent the data, and include things like: points lines density text In case you’re wondering why ggplot doesn’t use %&gt;% as in the tidyverse and other visualization packages, it’s because ggplot2 was using pipes before it was cool, well before those came along. Otherwise, the concept is the same as we saw in the data processing section. ggplot(aes(x=myvar, y=myvar2), data=mydata) + geom_point() Our base is provided via the ggplot function, and specifies the data at the very least, but commonly also the x and y aesthetics. The geom_point function adds a layer of points, and now we would have a scatterplot. Alternatively, you could have specified the x and y aesthetic at the geom_point layer, but if you’re going to have the same x, y, color, etc. aesthetics regardless of layer, put it in the base. Otherwise, doing it by layer gives you more flexibility if needed. Geoms even have their own data argument, allowing you to combine information from several sources for a single visualization. Aesthetics Aesthetics map data to various visual aspects of the plot, including size, color etc. The function used in ggplot to do this is aes. aes(x=myvar, y=myvar2, color=myvar3, group=g) The best way to understand what goes into the aes function is if the value is varying. For example, if I want the size of points to be a certain value, I would code the following. ... + geom_point(..., size=4) However, if I want the size to be associated with the data in some way, I use it as an aesthetic. ... + geom_point(aes(size=myvar)) The same goes for practically any aspect of a geom- size, color, fill, etc. If it is a fixed value, set it outside the aesthetic. If it varies based on the data, put it within an aesthetic. Examples Let’s get more of a feel for things by seeing some examples that demonstrate some geoms and aesthetics. To begin, after setting the base aesthetic, we’ll set some explicit values for the geom. library(ggplot2) data(&quot;diamonds&quot;); data(&#39;economics&#39;) ggplot(aes(x=carat, y=price), data=diamonds) + geom_point(size=.5, color=&#39;peru&#39;) Next we use two different geoms, one using a different data source. Note that geoms often have arguments specific to them, as label is for geom_text. It would be ignored by geom_line. ggplot(aes(x=date, y=unemploy), data=economics) + geom_line() + geom_text(aes(label=unemploy), vjust=-.5, data=filter(economics, date==&#39;2009-10-01&#39;)) In the following, one setting, alpha (transparency), is not mapped to the data, while size and color are14. ggplot(aes(x=carat, y=price), data=diamonds) + geom_point(aes(size=carat, color=clarity), alpha=.05) Stats There are many statistical functions built in, and it is a key strength of ggplot that you don’t have to do a lot of processing for very common plots. Quantile regression lines: ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_quantile() Loess (or additive model) smooth. This shows how we can do some fine-tuning and use model-based approaches for visualization. data(mcycle, package=&#39;MASS&#39;) ggplot(aes(x=times, y=accel), data=mcycle) + geom_point() + geom_smooth(formula=y ~ s(x, bs=&#39;ad&#39;), method=&#39;gam&#39;) Bootstrapped confidence intervals: ggplot(mtcars, aes(cyl, mpg)) + geom_point() + stat_summary(fun.data = &quot;mean_cl_boot&quot;, colour = &quot;orange&quot;, alpha=.75, size = 1) The take-home message here is to always let ggplot do the work for you if at all possible. Scales Often there are many things we want to change about the plot, for example, the size and values of axis labels, the range of sizes for points to take, the specific colors we want to use, and so forth. Be aware that there are a great many options here, and you will regularly want to use them. A very common thing you’ll do is change the labels for the axes. You definitely don’t have to go and change the variable name itself to do this, just use the labs function. There are also functions for individual parts, e.g. xlab, ylab and ggtitle. ggplot(aes(x=times, y=accel), data=mcycle) + geom_smooth(se=F) + labs(x=&#39;milliseconds after impact&#39;, y=&#39;head acceleration&#39;, title=&#39;Motorcycle Accident&#39;) A frequent operation is changing the x and y look in the form of limits and tick marks. Like labs, there is a general lims function and specific functions for just the specific parts. In addition, we may want to get really detailed using scale_x_* or scale_y_*. ggplot(mpg, aes(displ, hwy, size=cyl)) + geom_point() + ylim(c(0,60)) ggplot(mpg, aes(displ, hwy, size=cyl)) + geom_point() + scale_y_continuous(limits=c(0,60), breaks=seq(0,60,by=12), minor_breaks=seq(6,60,by=6)) Another common option is to change the size of points in some way. While we assign the aesthetic as before, it comes with defaults that might not work for a given situation. Play around with the range values. ggplot(mpg, aes(displ, hwy, size=cyl)) + geom_point() + scale_size(range=c(1,3)) Now that you know more about color issues, you may want to apply something besides the default options. The following shows a built-in color scale for a color aesthetic that is treated as continuous, and one that is discrete and which we want to supply our own colors (these actually come from plotly’s default color scheme). ggplot(mpg, aes(displ, hwy, color=cyl)) + geom_point() + scale_color_gradient2() ggplot(mpg, aes(displ, hwy, color=factor(cyl))) + geom_point() + scale_color_manual(values=c(&quot;#1f77b4&quot;, &quot;#ff7f0e&quot;, &quot;#2ca02c&quot;, &quot;#d62728&quot;)) We can even change the scale of the data itself. ggplot(mpg, aes(displ, hwy)) + geom_point() + scale_x_log10() In short, scale alterations are really useful for getting just the plot you want, and there is a lot of flexibility for you to work with. Facets Facets allow for paneled display, a very common operation. In general, we often want comparison plots. The facet_grid function will produce a grid, and often this is all that’s needed. However, facet_wrap is more flexible, while possibly taking a bit extra to get things just the way you want. Both use a formula approach to specify the grouping. facet_grid Facet by cylinder. ggplot(mtcars, aes(wt, mpg)) + geom_point() + facet_grid(~ cyl) Facet by vs and cylinder. ggplot(mtcars, aes(wt, mpg)) + geom_point() + facet_grid(vs ~ cyl, labeller = label_both) facet_wrap Specify the number of columns or rows with facet_wrap. ggplot(mtcars, aes(wt, mpg)) + geom_point() + facet_wrap(vs ~ cyl, labeller = label_both, ncol=2) Fine control ggplot2 makes it easy to get good looking graphs quickly. However the amount of fine control is extensive. The following plot is hideous (aside from the background, which is totally rad), but illustrates the point. ggplot(aes(x=carat, y=price), data=diamonds) + annotation_custom(rasterGrob(lambosun, width=unit(1,&quot;npc&quot;), height=unit(1,&quot;npc&quot;), interpolate = FALSE), -Inf, Inf, -Inf, Inf) + geom_point(aes(color=clarity), alpha=.5) + scale_y_log10(breaks=c(1000,5000,10000)) + xlim(0, 10) + scale_color_brewer(type=&#39;div&#39;) + facet_wrap(~cut, ncol=3) + theme_minimal() + theme(axis.ticks.x=element_line(color=&#39;darkred&#39;), axis.text.x=element_text(angle=-45), axis.text.y=element_text(size=20), strip.text=element_text(color=&#39;forestgreen&#39;), strip.background=element_blank(), panel.grid.minor=element_line(color=&#39;lightblue&#39;), legend.key=element_rect(linetype=4), legend.position=&#39;bottom&#39;) Themes In the last example you saw two uses of a theme- a built-in version that comes with ggplot (theme_minimal), and specific customization (theme(…)). The built-in themes provide ready-made approaches that might be enough for a finished product. For the theme function, each argument, and there are many, takes on a specific value or an element function: element_rect element_line element_text element_blank Each of those element functions has arguments specific to it. For example, for element_text you can specify the font size, while for element line you could specify the line type. Note that the base theme of ggplot, and I would say every plotting package, is probably going to need manipulation before a plot is ready for presentation. For example, the ggplot theme doesn’t work well for web presentation, and is even worse for print. You will almost invariably need to tweak it. I suggest using and saving your own custom theme for easy application for any visualization package you use frequently. Extensions ggplot2 now has its own extension system, and there is even a website to track the extensions. Examples include: additional themes maps interactivity animations marginal plots network graphs time series aligning multiple ggplot visualizations, possibly of different types As one can see, ggplot2 is only the beginning. You’ll have a lot of tools at your disposal. Furthermore, many modeling and other packages will produce ggplot graphics to which you can add your own layers and tweak like you would any other ggplot. Summary ggplot2 ggplot2 is an easy to use, but powerful visualization tool. It allows one to think in many dimensions for any graph, and extends well beyond the basics. Use it to easily create more interesting visualizations. ggplot2 Exercises Exercise 0 Install and load the ggplot2 package if you haven’t already. Exercise 1 Create two plots, one a scatterplot (e.g. with geom_point) and one with lines (e.g. geom_line) with a data set of your choosing (all of the following are base R or available after loading ggplot2. Some suggestions: faithful: Waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. msleep: mammals sleep dataset with sleep times and weights etc. diamonds: used in the slides economics: US economic time series. txhousing: Housing sales in TX. midwest: Midwest demographics. mpg: Fuel economy data from 1999 and 2008 for 38 popular models of car Recall the basic form for ggplot. ggplot(aes(x=*, y=*, other), data=*) + geom_*() + otherLayers, theme etc. Themes to play with: theme_bw theme_classic theme_dark theme_gray theme_light theme_linedraw theme_minimal theme_trueMinimal (requires the lazerhawk package and an appreciation of the Lamborghini background from the previous visualization) Exercise 2 Play around and change the arguments to the following. You’ll need to install the maps package. For example, do points for all county midpoints. For that you’d need to change the x and y for the point geom to an aesthetic based on the longitude and latitude, as well as add its data argument to use the seats data frame. Make the color of the points or text based on subregion. This will require adding the fill argument to the polygon geom and removing the NA setting. In addition, add the argument show.legend=F (outside the aesthetic), or you’ll have a problematic legend (recall what we said before about too many colors!). Try making color based on subregion too. See if you can use element_blank on a theme argument to remove the axis information. See ?theme for ideas. library(maps) mi = map_data(&quot;county&quot;, &quot;michigan&quot;) seats = mi %&gt;% group_by(subregion) %&gt;% summarise_at(vars(lat, long), function(x) median(range(x))) # inspect the data # head(mi) # head(seats) ggplot(mi, aes(long, lat)) + geom_polygon(aes(group = subregion), fill = NA, colour = &quot;grey60&quot;) + geom_text(aes(label = subregion), data = seats, size = 1, angle = 45) + geom_point(x=-83.748333, y=42.281389, color=&#39;#1e90ff&#39;, size=3) + theme_minimal() + theme(panel.grid=element_blank()) The reason the legend is hard to make out is because the transparency is mapped to the colors and size. While I personally have never come across this being a desirable situation, nor can fathom why it would be the default, it can be fixed via + guides(colour = guide_legend(override.aes = list(alpha = 1)), size = guide_legend(override.aes = list(alpha = 1))). Apparently because that’s fun to have to do every time and easy to remember.↩ "],
["2_03_interactive.html", "Interactive Visualization", " Interactive Visualization Packages As mentioned, ggplot2 is the most widely used package for visualization in R. However, it is not interactive by default. Many packages use htmlwidgets, d3 (JavaScript library), and other tools to provide interactive graphics. What’s great is that while you may have to learn new packages, you don’t necessarily have to change your approach or thinking about a plot, or learn some other language. Many of these packages can be lumped into more general packages that try to provide a plotting system (similar to ggplot2), versus those that just aim to do a specific type of plot well. Here are some to give a sense of this. General (click to visit the associated website): plotly - used also in Python, Matlab, Julia - can convert ggplot2 images to interactive ones (with varying degrees of success) highcharter also very general wrapper for highcharts.js and works with some R packages out of the box rbokeh like plotly, it also has cross language support Specific functionality: DT interactive data tables leaflet maps with OpenStreetMap visNetwork Network visualization In what follows we’ll see some of these in action. Note that unlike the previous chapter, the goal here is not to dive deeply, but just to get an idea of what’s available. Piping for Visualization One of the advantages to piping is that it’s not limited to dplyr style data management functions. Any R function can be potentially piped to, and several examples have already been shown. Many newer visualization packages take advantage of piping, and this facilitates data exploration. We don’t have to create objects just to do a visualization. New variables can be easily created and subsequently manipulated just for visualization. Furthermore, data manipulation not separated from visualization. htmlwidgets The htmlwidgets package makes it easy to create visualizations based on JavaScript libraries. If you’re not familiar with JavaScript, you actually are very familiar with its products, as it’s basically the language of the web, visual or otherwise. The R packages using it typically are pipe-oriented and produce interactive plots. In addition, you can use the htmlwidgets package to create your own functions that use a particular JavaScript library (but someone probably already has, so look first). Plotly We’ll begin our foray into the interactive world with a couple demonstrations of plotly. To give some background, you can think of plotly similar to RStudio, in that it has both enterprise (i.e. pay for) aspects and open source aspects. Just like RStudio, you have full access to what it has to offer via the open source R package. You may see old help suggestions referring to needing an account, but this is no longer necessary. When using plotly, you’ll note the layering approach similar to what we had with ggplot2. Piping is used before plotting to do some data manipulation, after which we seamlessly move to the plot itself. The =~ is essentially the way we denote aesthetics15. library(plotly) midwest %&gt;% filter(inmetro==T) %&gt;% plot_ly(x=~percbelowpoverty, y=~percollege) %&gt;% add_markers() plotly has modes, which allow for points, lines, text and combinations. Traces, add_*, work similar to geoms. library(mgcv); library(modelr); library(glue) mtcars %&gt;% mutate(amFactor = factor(am, labels=c(&#39;auto&#39;, &#39;manual&#39;)), hovertext = glue(&#39;weight: {wt} &lt;br&gt; mgp: {mpg} &lt;br&gt; {amFactor}&#39;)) %&gt;% add_predictions(gam(mpg~s(wt, am, bs=&#39;fs&#39;), data=mtcars)) %&gt;% plot_ly() %&gt;% add_markers(x=~wt, y=~mpg, color=~amFactor, opacity=.5, text=~hovertext, hoverinfo=&#39;text&#39;, showlegend=F) %&gt;% add_lines(x=~wt, y=~pred, color=~amFactor, name=&#39;gam prediction&#39;) While you can use plotly as a one-liner16, this would only be good for quick peeks while doing data exploration. It would generally be far too limiting otherwise. plot_ly(midwest, x = ~percollege, color = ~state, type = &quot;box&quot;) ggplotly One of the strengths of plotly is that we can feed a ggplot object to it, and turn our formerly static plots into interactive ones. It would have been easy to use geom_smooth to get a similar result, so let’s do so. gp = mtcars %&gt;% mutate(amFactor = factor(am, labels=c(&#39;auto&#39;, &#39;manual&#39;)), hovertext = paste(wt, mpg, amFactor)) %&gt;% arrange(wt) %&gt;% ggplot(aes(x=wt, y=mpg, color=amFactor)) + geom_smooth(se=F) + geom_point(aes(color=amFactor)) ggplotly() Note that this is not a one-to-one transformation. The plotly image will have different line widths and point sizes. It will usually be easier to change it within the ggplot process than tweaking the ggplotly object. Be prepared to spend time getting used to plotly. It has terrible documentation, is not nearly as flexible as ggplot2, has hidden (and arbitrary) defaults that can creep into a plot based on aspects of the data (rather than your settings), and some modes do not play nicely with others. That said, it works great for a lot of things. Highcharter Highcharter is also fairly useful for a wide variety of plots, and is based on the highcharts.js library. If you have data suited to one of its functions, getting a great interactive plot can be ridiculously easy. In what follows we use quantmod to create an xts (time series) object of Google’s stock price, including opening and closing values. The highcharter object has a ready-made plot for such data17. library(highcharter); library(quantmod) google_price = getSymbols(&quot;GOOG&quot;, auto.assign = FALSE) hchart(google_price) visNetwork The visNetwork package is specific to network visualizations and similar, and is based on the vis.js library. Networks require nodes and edges to connect them. These take on different aspects, and so are created in separate data frames. set.seed(1352) nodes = data.frame(id = 0:5, label = c(&#39;Bobby&#39;, &#39;Janie&#39;,&#39;Timmie&#39;, &#39;Mary&#39;, &#39;Johnny&#39;, &#39;Billy&#39;), group = c(&#39;friend&#39;, &#39;frenemy&#39;,&#39;frenemy&#39;, rep(&#39;friend&#39;, 3)), value = sample(10:50, 6)) edges = data.frame(from = c(0,0,0,1,1,2,2,3,3,3,4,5,5), to = sample(0:5, 13, replace = T), value = sample(1:10, 13, replace = T)) %&gt;% filter(from!=to) library(visNetwork) visNetwork(nodes, edges, height=300, width=800) %&gt;% visNodes(shape=&#39;circle&#39;, font=list(), scaling=list(min=10, max=50, label=list(enable=T))) %&gt;% visLegend() leaflet The leaflet package from RStudio is good for quick interactive maps, and it’s quite flexible and has some nice functionality to take your maps further. Unfortunately, it actually doesn’t play well with many markdown formats. hovertext &lt;- paste(sep = &quot;&lt;br/&gt;&quot;, &quot;&lt;b&gt;&lt;a href=&#39;http://umich.edu/&#39;&gt;University of Michigan&lt;/a&gt;&lt;/b&gt;&quot;, &quot;Ann Arbor, MI&quot; ) library(leaflet) leaflet() %&gt;% addTiles() %&gt;% addPopups(lng=-83.738222, lat=42.277030, popup=hovertext) DT It might be a bit odd to think of data frames visually, but they can be interactive also. One can use the DT package for interactive data frames. This can be very useful when working in collaborative environments where one shares reports, as you can embed the data within the document itself. library(DT) ggplot2movies::movies %&gt;% select(1:6) %&gt;% filter(rating&gt;8, !is.na(budget), votes &gt; 1000) %&gt;% datatable() The other thing to be aware of is that tables can be visual, it’s just that many academic outlets waste this opportunity. Simple bolding, italics, and even sizing, can make results pop more easily for the audience. The DT package allows for coloring and even simple things like bars that connotes values. The following gives some idea of its flexibility. iris %&gt;% # arrange(desc(Petal.Length)) %&gt;% datatable(rownames=F, options=list(dom=&#39;firtp&#39;), class = &#39;row-border&#39;) %&gt;% formatStyle(&#39;Sepal.Length&#39;, fontWeight = styleInterval(5, c(&#39;normal&#39;, &#39;bold&#39;))) %&gt;% formatStyle( &#39;Sepal.Width&#39;, color = styleInterval(c(3.4, 3.8), c(&#39;#7f7f7f&#39;, &#39;#00aaff&#39;, &#39;#ff5500&#39;)), backgroundColor = styleInterval(3.4, c(&#39;#ebebeb&#39;, &#39;aliceblue&#39;)) ) %&gt;% formatStyle( &#39;Petal.Length&#39;, # color = &#39;transparent&#39;, background = styleColorBar(iris$Petal.Length, &#39;#5500ff&#39;), backgroundSize = &#39;100% 90%&#39;, backgroundRepeat = &#39;no-repeat&#39;, backgroundPosition = &#39;center&#39; ) %&gt;% formatStyle( &#39;Species&#39;, color=&#39;white&#39;, transform = &#39;rotateX(45deg) rotateY(20deg) rotateZ(30deg)&#39;, backgroundColor = styleEqual( unique(iris$Species), c(&#39;#1f65b7&#39;, &#39;#66b71f&#39;, &#39;#b71f66&#39;) ) ) I would in no way recommend using the bars, unless the you want a visual instead of the value and can show all possible values. I would not recommend angled tag options at all, as that is more or less a prime example of chartjunk. However, subtle use of color and emphasis can make tables of results that your audience will actually spend time exploring. Shiny Shiny is a framework that can essentially allow you to build an interactive website/app. Like some of the other packages mentioned, it’s provided by RStudio developers. However, most of the more recently developed interactive visualization packages will work specifically within the shiny and rmarkdown setting. You can make shiny apps just for your own use and run them locally. But note, you are using R to build a webpage, and it’s not particularly well-suited for it. Much of how you use R will not be useful in building a shiny app, and so it will definitely take some getting used to, and you will likely need to do a lot of tedious adjustments to get things just how you want. Shiny apps have two main components, a part that specifies the user interface, and a server function that will do all the work. With those in place (either in a single ‘app.R’ file or in separate files), you can then simply run app. This example is taken from the shiny help file, and you can actually run it as is. library(shiny) # Running a Shiny app object app &lt;- shinyApp( ui = bootstrapPage( numericInput(&#39;n&#39;, &#39;Number of obs&#39;, 100), plotOutput(&#39;plot&#39;) ), server = function(input, output) { output$plot &lt;- renderPlot({ hist(runif(input$n)) }) } ) runApp(app) You can share your app code/directory with anyone and they’ll be able to run it also. However, this is great mostly just for teaching someone how to do shiny, which most people aren’t going to do. Typically you’ll want someone to use the app itself, not run code. In that case you’ll need a web server. You can get up to 5 free ‘running’ applications at shinyapps.io. However, you will notably be limited in the amount of computing resources that can be used to run the apps in a given month. Even minor usage of those could easily overtake the free settings. For personal use it’s plenty though. Interactive and Visual Data Exploration As seen above, just a couple visualization packages can go a very long way. It’s now very easy to incorporate interactivity, so you should use it even if only for your own data exploration. In general, interactivity allows for even more dimensions to be brought to a graphic, and can be more fun too! However, they must serve a purpose. Too often they are simply distraction, and can actually detract from the data story. Make sure to use them when they can enhance the narrative you wish to express. Interactive Visualization Exercises Exercise 0 Install and load the plotly package. Load the tidyverse package if necessary (so you can use dplyr and ggplot2), and install/load the ggplot2movies for the IMDB data. Exercise 1 Using dplyr group by year, and summarize to create a new variable that is the Average rating. Then create a plot with plotly for a line or scatter plot (for the latter, use the add_markers function). It will take the following form, but you’ll need to supply the plotly arguments. library(ggplot2movies) movies %&gt;% group_by(year) %&gt;% summarise(Avg_Rating=mean(rating)) plot_ly() %&gt;% add_markers() Exercise 2 This time group by year and Drama. In the summarize create average rating again, but also a variable representing the average number of votes. In your plotly line, use the size and color arguments to represent whether the average number of votes and whether it was drama or not respectively. Use add_markers. Note that Drama will be treated as numeric since it’s a 0-1 indicator. This won’t affect the plot, but if you want, you might use mutate to change it to a factor with labels ‘Drama’ and ‘Other’. Exercise 3 Create a ggplot of your design and then use ggplotly to make it interactive. Often you’ll get an error because you used = instead of =~. Also, I find trying to set single values for things like size unintuitive, and it may be implemented differently for different traces (e.g. setting size in marker traces requires size=I(value)).↩ You can with ggplot2 as well, but I intentionally made no mention of it. You should learn how to use the package generally before learning how to use its shortcuts.↩ This is the sort of thing that takes the ‘wow factor’ out of a lot of stuff you see on the web. You’ll find a lot of those cool plots are actually lazy folks using default settings of stuff that make only take a single line of code.↩ "],
["3_01_programming.html", "Part III: Programming Basics", " Part III: Programming Basics Becoming a better programmer is in many ways like learning any language. While it may be literal, there is much nuance, and many ways are available to express yourself in order to solve some problem. However, it doesn’t take much in the way of practice to develop a few skills that will not only last, but go a long way toward saving you time and allowing you to explore your data, models, and visualizations more extensively. So let’s get to it! R Objects Object Inspection &amp; Exploration Let’s say you’ve imported your data into R. If you are going to be able to do anything with it, you’ll have had to create an R object that represents that data. What is that object? By now you know it’s a data frame, specifically, class data.frame or possibly a tibble if you’re working within the tidyverse. If you want to look at it, you might be tempted to look at it this way with View, or clicking on it in your Environment viewer. View(diamonds) While this is certainly one way to inspect it, it’s not very useful. There’s far too much information to get much out of it, and information you may need to know is absent. Consider the following: str(diamonds) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 53940 obs. of 10 variables: $ carat : num 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... $ cut : Ord.factor w/ 5 levels &quot;Fair&quot;&lt;&quot;Good&quot;&lt;..: 5 4 2 4 2 3 3 3 1 3 ... $ color : Ord.factor w/ 7 levels &quot;D&quot;&lt;&quot;E&quot;&lt;&quot;F&quot;&lt;&quot;G&quot;&lt;..: 2 2 2 6 7 7 6 5 2 5 ... $ clarity: Ord.factor w/ 8 levels &quot;I1&quot;&lt;&quot;SI2&quot;&lt;&quot;SI1&quot;&lt;..: 2 3 5 4 2 6 7 3 4 5 ... $ depth : num 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... $ table : num 55 61 65 58 58 57 57 55 61 61 ... $ price : int 326 326 327 334 335 336 336 337 337 338 ... $ x : num 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... $ y : num 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... $ z : num 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... glimpse(diamonds) Observations: 53,940 Variables: 10 $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.30, 0.23, 0.22, 0.31, 0.20, 0.32, 0.30, 0.30, 0.30, 0.30, 0.30, 0.23, 0.23, 0.31, 0.31, 0.23, 0.24, 0.30, 0.23, 0.23, 0.23, 0.23, 0.23, 0... $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Very Good, Fair, Very Good, Good, Ideal, Premium, Ideal, Premium, Premium, Ideal, Good, Good, Very Good, Good, Very Good, Very Good, Very Go... $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I, E, H, J, J, G, I, J, D, F, F, F, E, E, D, F, E, H, D, I, I, J, D, D, H, F, H, H, E, H, F, G, I, E, D, I, J, I, I, I, I, D, D, D, I, G, I... $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, SI1, SI2, SI2, I1, SI2, SI1, SI1, SI1, SI2, VS2, VS1, SI1, SI1, VVS2, VS1, VS2, VS2, VS1, VS1, VS1, VS1, VS1, VS1, VS1, VS1, SI1, VS2, SI... $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64.0, 62.8, 60.4, 62.2, 60.2, 60.9, 62.0, 63.4, 63.8, 62.7, 63.3, 63.8, 61.0, 59.4, 58.1, 60.4, 62.5, 62.2, 60.5, 60.9, 60.0, 59.8, 60.7, 5... $ table &lt;dbl&gt; 55.0, 61.0, 65.0, 58.0, 58.0, 57.0, 57.0, 55.0, 61.0, 61.0, 55.0, 56.0, 61.0, 54.0, 62.0, 58.0, 54.0, 54.0, 56.0, 59.0, 56.0, 55.0, 57.0, 62.0, 62.0, 58.0, 57.0, 57.0, 61.0, 57.0, 57.0, 57.0, 59.0, 5... $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 342, 344, 345, 345, 348, 351, 351, 351, 351, 352, 353, 353, 353, 354, 355, 357, 357, 357, 402, 402, 402, 402, 402, 402, 402, 402, 403, 403,... $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.25, 3.93, 3.88, 4.35, 3.79, 4.38, 4.31, 4.23, 4.23, 4.21, 4.26, 3.85, 3.94, 4.39, 4.44, 3.97, 3.97, 4.28, 3.96, 3.96, 4.00, 4.04, 3.97, 4... $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.28, 3.90, 3.84, 4.37, 3.75, 4.42, 4.34, 4.29, 4.26, 4.27, 4.30, 3.92, 3.96, 4.43, 4.47, 4.01, 3.94, 4.30, 3.97, 3.99, 4.03, 4.06, 4.01, 4... $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.73, 2.46, 2.33, 2.71, 2.27, 2.68, 2.68, 2.70, 2.71, 2.66, 2.71, 2.48, 2.41, 2.62, 2.59, 2.41, 2.47, 2.67, 2.40, 2.42, 2.41, 2.42, 2.42, 2... The str function looks at the structure of the object, while glimpse perhaps provides a possibly more readable version, and is just str specifically suited toward data frames. In both cases, we get info about the object and the various things within it. While you might be doing this with data frames, you should be doing it with any of the objects you’re interested in. Consider a regression model object. lm_mod = lm(mpg ~ ., data=mtcars) str(lm_mod, 0) List of 12 - attr(*, &quot;class&quot;)= chr &quot;lm&quot; str(lm_mod, 1) List of 12 $ coefficients : Named num [1:11] 12.3034 -0.1114 0.0133 -0.0215 0.7871 ... ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;(Intercept)&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... $ residuals : Named num [1:32] -1.6 -1.112 -3.451 0.163 1.007 ... ..- attr(*, &quot;names&quot;)= chr [1:32] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... $ effects : Named num [1:32] -113.65 -28.6 6.13 -3.06 -4.06 ... ..- attr(*, &quot;names&quot;)= chr [1:32] &quot;(Intercept)&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... $ rank : int 11 $ fitted.values: Named num [1:32] 22.6 22.1 26.3 21.2 17.7 ... ..- attr(*, &quot;names&quot;)= chr [1:32] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... $ assign : int [1:11] 0 1 2 3 4 5 6 7 8 9 ... $ qr :List of 5 ..- attr(*, &quot;class&quot;)= chr &quot;qr&quot; $ df.residual : int 21 $ xlevels : Named list() $ call : language lm(formula = mpg ~ ., data = mtcars) $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb .. ..- attr(*, &quot;variables&quot;)= language list(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) .. ..- attr(*, &quot;factors&quot;)= int [1:11, 1:10] 0 1 0 0 0 0 0 0 0 0 ... .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..- attr(*, &quot;term.labels&quot;)= chr [1:10] &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; ... .. ..- attr(*, &quot;order&quot;)= int [1:10] 1 1 1 1 1 1 1 1 1 1 .. ..- attr(*, &quot;intercept&quot;)= int 1 .. ..- attr(*, &quot;response&quot;)= int 1 .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; .. ..- attr(*, &quot;predvars&quot;)= language list(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:11] &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ... .. .. ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... $ model :&#39;data.frame&#39;: 32 obs. of 11 variables: ..- attr(*, &quot;terms&quot;)=Classes &#39;terms&#39;, &#39;formula&#39; language mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb .. .. ..- attr(*, &quot;variables&quot;)= language list(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) .. .. ..- attr(*, &quot;factors&quot;)= int [1:11, 1:10] 0 1 0 0 0 0 0 0 0 0 ... .. .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. .. ..- attr(*, &quot;term.labels&quot;)= chr [1:10] &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; ... .. .. ..- attr(*, &quot;order&quot;)= int [1:10] 1 1 1 1 1 1 1 1 1 1 .. .. ..- attr(*, &quot;intercept&quot;)= int 1 .. .. ..- attr(*, &quot;response&quot;)= int 1 .. .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; .. .. ..- attr(*, &quot;predvars&quot;)= language list(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) .. .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:11] &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ... .. .. .. ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... - attr(*, &quot;class&quot;)= chr &quot;lm&quot; Here we look at the object at the lowest level of detail (0), which basically just tells us that it’s a list of stuff. But if we go into more depth, we can see that there is quite a bit going on in here! Coefficients, the data frame used in the model (i.e. only the variables used and no NA), and much more are available to us, and we can pluck out any piece of it. lm_mod$coefficients (Intercept) cyl disp hp drat wt qsec vs am gear carb 12.30337416 -0.11144048 0.01333524 -0.02148212 0.78711097 -3.71530393 0.82104075 0.31776281 2.52022689 0.65541302 -0.19941925 lm_mod$model %&gt;% head() mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Let’s do a summary of it, something you’ve probably done many times. summary(lm_mod) Call: lm(formula = mpg ~ ., data = mtcars) Residuals: Min 1Q Median 3Q Max -3.4506 -1.6044 -0.1196 1.2193 4.6271 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 12.30337 18.71788 0.657 0.5181 cyl -0.11144 1.04502 -0.107 0.9161 disp 0.01334 0.01786 0.747 0.4635 hp -0.02148 0.02177 -0.987 0.3350 drat 0.78711 1.63537 0.481 0.6353 wt -3.71530 1.89441 -1.961 0.0633 . qsec 0.82104 0.73084 1.123 0.2739 vs 0.31776 2.10451 0.151 0.8814 am 2.52023 2.05665 1.225 0.2340 gear 0.65541 1.49326 0.439 0.6652 carb -0.19942 0.82875 -0.241 0.8122 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.65 on 21 degrees of freedom Multiple R-squared: 0.869, Adjusted R-squared: 0.8066 F-statistic: 13.93 on 10 and 21 DF, p-value: 3.793e-07 But you can assign that to an object and inspect it too! lm_mod_summary = summary(lm_mod) str(lm_mod_summary) List of 11 $ call : language lm(formula = mpg ~ ., data = mtcars) $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb .. ..- attr(*, &quot;variables&quot;)= language list(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) .. ..- attr(*, &quot;factors&quot;)= int [1:11, 1:10] 0 1 0 0 0 0 0 0 0 0 ... .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. .. .. ..$ : chr [1:11] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... .. .. .. ..$ : chr [1:10] &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; ... .. ..- attr(*, &quot;term.labels&quot;)= chr [1:10] &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; ... .. ..- attr(*, &quot;order&quot;)= int [1:10] 1 1 1 1 1 1 1 1 1 1 .. ..- attr(*, &quot;intercept&quot;)= int 1 .. ..- attr(*, &quot;response&quot;)= int 1 .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; .. ..- attr(*, &quot;predvars&quot;)= language list(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:11] &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ... .. .. ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... $ residuals : Named num [1:32] -1.6 -1.112 -3.451 0.163 1.007 ... ..- attr(*, &quot;names&quot;)= chr [1:32] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... $ coefficients : num [1:11, 1:4] 12.3034 -0.1114 0.0133 -0.0215 0.7871 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:11] &quot;(Intercept)&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Std. Error&quot; &quot;t value&quot; &quot;Pr(&gt;|t|)&quot; $ aliased : Named logi [1:11] FALSE FALSE FALSE FALSE FALSE FALSE ... ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;(Intercept)&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... $ sigma : num 2.65 $ df : int [1:3] 11 21 11 $ r.squared : num 0.869 $ adj.r.squared: num 0.807 $ fstatistic : Named num [1:3] 13.9 10 21 ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;value&quot; &quot;numdf&quot; &quot;dendf&quot; $ cov.unscaled : num [1:11, 1:11] 49.883532 -1.874242 -0.000841 -0.003789 -1.842635 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:11] &quot;(Intercept)&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... .. ..$ : chr [1:11] &quot;(Intercept)&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... - attr(*, &quot;class&quot;)= chr &quot;summary.lm&quot; If we pull the coefficients from this object, we are not just getting the values, but the table that’s printed in the summary. And we can now get that ready for publishing for example18. lm_mod_summary$coefficients %&gt;% kableExtra::kable(digits = 2) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 12.30 18.72 0.66 0.52 cyl -0.11 1.05 -0.11 0.92 disp 0.01 0.02 0.75 0.46 hp -0.02 0.02 -0.99 0.33 drat 0.79 1.64 0.48 0.64 wt -3.72 1.89 -1.96 0.06 qsec 0.82 0.73 1.12 0.27 vs 0.32 2.10 0.15 0.88 am 2.52 2.06 1.23 0.23 gear 0.66 1.49 0.44 0.67 carb -0.20 0.83 -0.24 0.81 After a while, you’ll know what’s in the objects you use most often, and that will allow you more easily work with the content they contain, allowing you to work with them more efficiently. Methods Consider the following: summary(diamonds) carat cut color clarity depth table price x y z Min. :0.2000 Fair : 1610 D: 6775 SI1 :13065 Min. :43.00 Min. :43.00 Min. : 326 Min. : 0.000 Min. : 0.000 Min. : 0.000 1st Qu.:0.4000 Good : 4906 E: 9797 VS2 :12258 1st Qu.:61.00 1st Qu.:56.00 1st Qu.: 950 1st Qu.: 4.710 1st Qu.: 4.720 1st Qu.: 2.910 Median :0.7000 Very Good:12082 F: 9542 SI2 : 9194 Median :61.80 Median :57.00 Median : 2401 Median : 5.700 Median : 5.710 Median : 3.530 Mean :0.7979 Premium :13791 G:11292 VS1 : 8171 Mean :61.75 Mean :57.46 Mean : 3933 Mean : 5.731 Mean : 5.735 Mean : 3.539 3rd Qu.:1.0400 Ideal :21551 H: 8304 VVS2 : 5066 3rd Qu.:62.50 3rd Qu.:59.00 3rd Qu.: 5324 3rd Qu.: 6.540 3rd Qu.: 6.540 3rd Qu.: 4.040 Max. :5.0100 I: 5422 VVS1 : 3655 Max. :79.00 Max. :95.00 Max. :18823 Max. :10.740 Max. :58.900 Max. :31.800 J: 2808 (Other): 2531 summary(diamonds$clarity) I1 SI2 SI1 VS2 VS1 VVS2 VVS1 IF 741 9194 13065 12258 8171 5066 3655 1790 summary(lm_mod) Call: lm(formula = mpg ~ ., data = mtcars) Residuals: Min 1Q Median 3Q Max -3.4506 -1.6044 -0.1196 1.2193 4.6271 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 12.30337 18.71788 0.657 0.5181 cyl -0.11144 1.04502 -0.107 0.9161 disp 0.01334 0.01786 0.747 0.4635 hp -0.02148 0.02177 -0.987 0.3350 drat 0.78711 1.63537 0.481 0.6353 wt -3.71530 1.89441 -1.961 0.0633 . qsec 0.82104 0.73084 1.123 0.2739 vs 0.31776 2.10451 0.151 0.8814 am 2.52023 2.05665 1.225 0.2340 gear 0.65541 1.49326 0.439 0.6652 carb -0.19942 0.82875 -0.241 0.8122 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.65 on 21 degrees of freedom Multiple R-squared: 0.869, Adjusted R-squared: 0.8066 F-statistic: 13.93 on 10 and 21 DF, p-value: 3.793e-07 summary(lm_mod_summary) Length Class Mode call 3 -none- call terms 3 terms call residuals 32 -none- numeric coefficients 44 -none- numeric aliased 11 -none- logical sigma 1 -none- numeric df 3 -none- numeric r.squared 1 -none- numeric adj.r.squared 1 -none- numeric fstatistic 3 -none- numeric cov.unscaled 121 -none- numeric How is it that one function works on all these different types of objects? That’s not all. In RStudio, type summary. and hit the tab key. When you load additional packages, you’ll see even more methods for the summary function. When you call summary on an object, the appropriate type of summary function will be used depending on the class of the object. If there is no specific type, e.g. when we called summary on something that already had summary called on it, it will just use a default version listing the contents. To see all the methods for summary, type the following, and you’ll see all that is currently available for your R session. methods(&#39;summary&#39;) [1] summary,ANY-method summary,diagonalMatrix-method summary,quantmod-method summary,sparseMatrix-method summary.aareg* summary.agnes* [7] summary.aov summary.aovlist* summary.areg.boot* summary.aspell* summary.cch* summary.check_packages_in_dir* [13] summary.clara* summary.clustCombi summary.cohesiveBlocks* summary.connection summary.corAR1* summary.corARMA* [19] summary.corCAR1* summary.corCompSymm* summary.corExp* summary.corGaus* summary.corIdent* summary.corLin* [25] summary.corNatural* summary.corRatio* summary.corSpher* summary.corStruct* summary.corSymm* summary.coxph* [31] summary.coxph.penal* summary.crq summary.crqs summary.data.frame summary.Date summary.default [37] summary.diana* summary.dissimilarity* summary.Duration* summary.dynrqs summary.ecdf* summary.factor [43] summary.fanny* summary.find.matches* summary.formula* summary.gam summary.ggplot* summary.glm [49] summary.gls* summary.gmmhd summary.igraph* summary.impute* summary.infl* summary.Interval* [55] summary.List* summary.lm summary.lme* summary.lmList* summary.loess* summary.manova [61] summary.matrix summary.mChoice* summary.Mclust summary.mclustBIC summary.MclustBootstrap summary.MclustDA [67] summary.MclustDR summary.MclustDRsubsel summary.mclustICL summary.mlm* summary.modelStruct* summary.mona* [73] summary.mslm summary.multinom* summary.nlrq summary.nls* summary.nlsList* summary.nnet* [79] summary.packageStatus* summary.pam* summary.pdBlocked* summary.pdCompSymm* summary.pdDiag* summary.PDF_Dictionary* [85] summary.PDF_Stream* summary.pdIdent* summary.pdIdnot* summary.pdLogChol* summary.pdMat* summary.pdNatural* [91] summary.pdSymm* summary.pdTens* summary.Period* summary.phylo* summary.phymltest* summary.POSIXct [97] summary.POSIXlt summary.ppr* summary.prcomp* summary.princomp* summary.proc_time summary.prop.part* [103] summary.psych* summary.pyears* summary.ratetable* summary.reStruct* summary.rpart* summary.rq [109] summary.rqs summary.rqss summary.shingle* summary.silhouette* summary.slm summary.srcfile [115] summary.srcref summary.stepfun summary.stl* summary.survexp* summary.survfit* summary.survfitms* [121] summary.survreg* summary.table summary.transcan* summary.trellis* summary.tukeysmooth* summary.varComb* [127] summary.varConstPower* summary.varExp* summary.varFixed* summary.varFunc* summary.varIdent* summary.varPower* [133] summary.warnings summary.yearmon* summary.yearqtr* summary.zoo* see &#39;?methods&#39; for accessing help and source code Say you are new to a modeling package, and as such, you might want to see what all you can do with the resulting object. The following lists all the functions that can be used on that class of object. library(brms) methods(class = &#39;brmsfit&#39;) [1] add_ic as.array as.data.frame as.matrix as.mcmc autocor bayes_factor bayes_R2 bridge_sampler [10] coef control_params expose_functions family fitted fixef formula getCall hypothesis [19] kfold launch_shinystan log_lik log_posterior logLik loo_linpred loo_model_weights loo_predict loo_predictive_interval [28] loo LOO marginal_effects marginal_smooths model_weights model.frame neff_ratio ngrps nobs [37] nsamples nuts_params pairs parnames plot post_prob posterior_average posterior_interval posterior_linpred [46] posterior_predict posterior_samples posterior_summary pp_average pp_check pp_mixture predict predictive_error print [55] prior_samples prior_summary ranef residuals rhat stancode standata stanplot summary [64] update VarCorr vcov waic WAIC see &#39;?methods&#39; for accessing help and source code This allows you to quickly get oriented to a package and the objects it produces, and provides utility you might not have even known to look for in the first place! S4 classes Everything we’ve been dealing with at this point are S3 objects, classes, and methods. R is a dialect of the S language, and the S3 name reflects the version of S at the time of R’s creation. S4 was the next iteration of S, but I’m not going to say much about the S4 system of objects other than they are a separate type of object with their own methods. For practical use you might not see much difference, but if you see an S4 object, it will have slots accessible via @. car_matrix = mtcars %&gt;% as.matrix() %&gt;% # convert from df to matrix Matrix::Matrix() # convert to Matrix class (S4) typeof(car_matrix) [1] &quot;S4&quot; str(car_matrix) Formal class &#39;dgeMatrix&#39; [package &quot;Matrix&quot;] with 4 slots ..@ x : num [1:352] 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ..@ Dim : int [1:2] 32 11 ..@ Dimnames:List of 2 .. ..$ : chr [1:32] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... .. ..$ : chr [1:11] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... ..@ factors : list() Mostly, I just find S4 objects slightly more annoying to work with for applied work, but you should be at least somewhat familiar with them so that you won’t be thrown off course when they appear. Others Indeed there are more types of R objects, but they will probably not be of much notice to the applied user. As an example, the text2vec package uses R6. I can only say that you’ll just have to cross that bridge should you get to it. Inspecting Functions You might not think of them as such, but in R, everything’s an object, including functions. You can inspect them like anything else. str(lm) function (formula, data, subset, weights, na.action, method = &quot;qr&quot;, model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, offset, ...) ## lm 1 function (formula, data, subset, weights, na.action, method = &quot;qr&quot;, 2 model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, 3 contrasts = NULL, offset, ...) 4 { 5 ret.x &lt;- x 6 ret.y &lt;- y 7 cl &lt;- match.call() 8 mf &lt;- match.call(expand.dots = FALSE) 9 m &lt;- match(c(&quot;formula&quot;, &quot;data&quot;, &quot;subset&quot;, &quot;weights&quot;, &quot;na.action&quot;, 10 &quot;offset&quot;), names(mf), 0L) 11 mf &lt;- mf[c(1L, m)] 12 mf$drop.unused.levels &lt;- TRUE 13 mf[[1L]] &lt;- quote(stats::model.frame) 14 mf &lt;- eval(mf, parent.frame()) 15 if (method == &quot;model.frame&quot;) 16 return(mf) 17 else if (method != &quot;qr&quot;) 18 warning(gettextf(&quot;method = &#39;%s&#39; is not supported. Using &#39;qr&#39;&quot;, 19 method), domain = NA) 20 mt &lt;- attr(mf, &quot;terms&quot;) One of the primary reasons for R’s popularity is the accessibility of the underlying code. People can very easily access the code for some function, modify it, extend it, etc. From an applied perspective, if you want to get better at writing code, or modify existing code, all you have to do is dive in! We’ll talk more about writing functions later. Objects Exercises With one function, find out what the class, number of rows, number of columns are of the following object, including what kind of object the last three columns are. library(dplyr) ?starwars Help Files Many applied users of R are quick to search the web for available help when they come to a problem. This is great, you’ll find a lot of information out there. However, it will likely take you a bit to sort through things and find exactly what you need. Strangely, many users of R I see don’t use the help files first, and yet this is typically the quickest way to answer many of the questions they’ll have. Let’s start with an example. We’ll use the sample function to get a random sample of 10 values from the range of numbers 1 through 5. So, go ahead and do so! sample(?) Don’t know what to put? Consult the help file! We get a brief description of a function at the top, then we see how to actually use it, i.e. the form the syntax should take. We find out there is even an additional function, sample.int, that we could use. Next we see what arguments are possible. First we need an x, so what is the thing we’re trying to sample from? The numbers 1 through 5. Next is the size, which is how many values we want, in this case 10. So let’s try it. nums = 1:5 sample(nums, 10) Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when &#39;replace = FALSE&#39; Uh oh- we have a problem with the replace argument! We can see in the help file that, by default, it is FALSE19, but if we want to sample 10 times from only 5 numbers, we’ll need to change it to TRUE. Now we are on our way! The help file gives detailed information about the sampling that is possible, which actually is not as simple as one would think! The Value is important, as it tells us what we can expect the function to return, whether a data frame, list, or whatever. We even get references, other functions that might be of interest (See Also), and examples. There is a lot to digest for this function! Not all functions have all this information, but most do, and if they are adhering to standards they will20. However, all functions have this same documentation form, which puts R above and beyond most programming languages in this regard. Once you look at a couple of help files, you’ll always be able to quickly find the information you need from any other. An alternative would be to use broom::tidy on the lm object itself, or use pander instead of kableExtra to work on the lm object.↩ An odd choice in my opinion. For the things I do, I need sample with replacement far more than I don’t, but that may not be the same for everyone.↩ Ahem, tidyverse.↩ "],
["3_02_iterative.html", "Iterative Programming", " Iterative Programming Almost everything you do when dealing with data will need to be done again, and again, and again. If you are copy-pasting your way to repetitively do the same thing, you’re not only doing things inefficiently, you’re almost certainly setting yourself up for trouble if practically anything changes about the data or process. In order to avoid this, you need to be familiar with basic programming, and a starting point is to use iterative programming. Let’s look at the following. Let’s say we want to get the means of some columns in our data set. Do you do something like this? means1 = mean(df$x) means2 = mean(df$y) means3 = mean(df$z) means4 = mean(df$q) Now consider what you have to change if you change a variable name, decide to do a median, or the data object name changes. Practically breathing on the data will cause you to have to redo that code, and possibly every line of it. For loops A for loop will help us get around the problem. The idea is that we want to perform a particular action for every iteration of some sequence. That sequence may be over columns, rows, lines in a text, whatever. Here is a loop. for (column in c(&#39;x&#39;,&#39;y&#39;,&#39;z&#39;,&#39;q&#39;)) { mean(df[,column]) } What’s going on here? We’ve created an iterative process in which, for every element in c('x','y','z','q'), we are going to do something. We use the completely arbitrary word column as a placeholder to index which of the four columns we’re dealing with at a given point in the process. On the first iteration, column will equal x, on the second y, and so on. We then take the mean of df[,column], which will be df[,'x'], then df[,'y'], etc. Here is an example with the nycflights data, which regards flights that departed New York City in 2013. The weather data set has columns for things like temperature, humidity, and so forth.21 weather = data.frame(nycflights13::weather) for (column in c(&#39;temp&#39;,&#39;humid&#39;,&#39;wind_speed&#39;,&#39;precip&#39;)) { print(mean(weather[,column], na.rm = TRUE)) } [1] 55.26039 [1] 62.53006 [1] 10.51749 [1] 0.004469079 Now if the data name changes, the columns we want change, or we want to calculate something else, we usually end up only changing one thing, rather than at least changing one, and probably many more things. In addition, the amount of code is the same whether the loop goes over 100 columns or 4. Let’s do things a little differently. columns = c(&#39;temp&#39;, &#39;humid&#39;, &#39;wind_speed&#39;, &#39;precip&#39;) nyc_means = rep(NA, length(columns)) for (i in seq_along(columns)) { nyc_means[i] = mean(weather[ ,columns[i]], na.rm = TRUE) } nyc_means [1] 55.260392127 62.530058972 10.517488384 0.004469079 By creating a columns object, if anything changes about the columns we want, that’s the only line in the code that would need to be changed. The i is now a place holder for a number that goes from 1 to the length of columns (i.e. 4). We make an empty nyc_means object that’s the length of the columns, so that each element will eventually be the mean of the corresponding column. In the following I remove precipitation and add visibility and air pressure. columns = c(&#39;temp&#39;, &#39;humid&#39;, &#39;wind_speed&#39;, &#39;visib&#39;, &#39;pressure&#39;) nyc_means = rep(NA, length(columns)) for (i in seq_along(columns)) { nyc_means[i] = mean(weather[ ,columns[i]], na.rm = TRUE) } nyc_means %&gt;% round(2) [1] 55.26 62.53 10.52 9.26 1017.90 Had we been copy-pasting, this would require deleting or commenting out a line in our code, pasting two more, and changing each one after pasting to represent the new columns. That’s tedious and not a fun way to code. A slight speed gain Note that you do not have to create an empty object like we did. The following works also. columns = c(&#39;temp&#39;,&#39;humid&#39;,&#39;wind_speed&#39;, &#39;visib&#39;, &#39;pressure&#39;) nyc_means = c() for (i in seq_along(columns)) { nyc_means[i] = mean(weather[ ,columns[i]], na.rm = TRUE) } nyc_means %&gt;% round(2) [1] 55.26 62.53 10.52 9.26 1017.90 However, the other approach is slightly faster, because memory is already allocated as needed, rather than updating it every iteration of the loop. This speed gain can become noticeable when dealing with thousands of columns and complex operations. While alternative When you look at some people’s R code, you may see a loop of a different sort. columns = c(&#39;temp&#39;,&#39;humid&#39;,&#39;wind_speed&#39;, &#39;visib&#39;, &#39;pressure&#39;) nyc_means = c() i = 1 while (i &lt;= length(columns)) { nyc_means[i] = mean(weather[ ,columns[i]], na.rm = TRUE) i = i + 1 } nyc_means %&gt;% round(2) [1] 55.26 62.53 10.52 9.26 1017.90 This involves a while statement. It states, while i is less than or equal to the length (number) of columns, compute the value of the ith element of nyc_means as the mean of ith column of weather. After that, increase the value of i. So, we start with i = 1, compute that subsequent mean, i now equals 2, do the process again, and so on. The process will stop as soon as the value of i is greater than the length of columns. There is zero difference to using the while approach vs. the for loop. While is often used when there is a check to be made, e.g. in modeling functions that have to stop the estimation process at some point, or else they’d go on indefinitely. In that case the while syntax is probably more natural. Either is fine. (Dots and) Loops Understanding loops is fundamental toward spending less time processing data and more time toward exploring it. Your code will be more succinct and more able to handle the usual changes that come with dealing with data. Now that you have a sense of it, know that once you are armed with the sorts of things we’ll be talking about next- apply functions, writing functions, and vectorization - you’ll likely have little need to write explicit loops. While there is always a need for iterative processing of data, R provides even more efficient means to do so. apply family A family of functions comes with R that allows for a succinct way of looping when it is appropriate. As such, even when you loop in R, you don’t have to do so explicitly. Common functions in this family include: apply arrays, matrices, data.frames lapply, sapply, vapply lists, data.frames, vectors tapply grouped operations (table apply) mapply multivariate version of sapply replicate performs an operation N times As an example we’ll consider standardizing variables, i.e. taking a set of numbers, subtracting the mean, and dividing by the standard deviation. This results in a variable with mean of 0 and standard deviation of 1. Let’s start with a loop approach. for (i in 1:ncol(mydf)){ x = mydf[,i] for (j in 1:length(x)){ x[j] = (x[j] - mean(x))/sd(x) } } The above would be a really bad way to use R. It goes over each column individually, then over each value of the column. Conversely, apply will take a matrix or data frame, and apply a function over the margin, row or column, you want to loop over. The first argument is the data you’re considering, the margin is the second argument (1 for rows, 2 for columns22), and the function you want to apply to those rows is the third argument. The following example is much cleaner compared to the loop, and now you’d have a function you can use elsewhere if needed. stdize &lt;- function(x) { (x-mean(x)) / sd(x) } apply(mydf, 2, stdize) # 1 for rows, 2 for columnwise application Many of the other apply functions work similarly, taking an object and a function to do the work on the object (possibly implicit), possibly with other arguments specified if necessary. Apply functions It is important to be familiar with and regularly use the apply family for efficient data processing. A summary of benefits includes: Cleaner/simpler code Environment kept clear of unnecessary objects Potentially more reproducible more likely to use generalizable functions Parallelizable Note that apply functions are NOT necessarily faster than explicit loops, and if you create an empty object for the loop as discussed previously, the explicit loop will likely be faster. On top of that, functions like replicate and mapply are especially slow. However, the apply family can ALWAYS potentially be faster than standard R loops do to parallelization. With base R’s parallel package, there are parallel versions of the apply family, e.g.parApply, parLapply etc. As every modern computer has at least four cores to play with, you’ll always potentially have nearly a 4x speedup by using the parallel apply functions. Apply functions and similar approaches should be a part of your regular R experience. We’ll talk about other options that may have even more benefits, but you need to know the basics of how apply functions work in order to use those. I use R every day, and very rarely use explicit loops. Note that there is no speed difference for a for loop vs. using while. And if you must use an explicit loop, create an empty object of the dimension/form you need, and then fill it in via the loop. This will be notably faster. I pretty much never use an explicit double loop, as a little more thinking about the problem will usually provide a more efficient path to solving the problem. purrr The purr package allows you to take the apply family approach to the tidyverse. Consider the following. We’ll use the map function to map the sum function to each element in the list, the same way we would with lapply. x = list(1:3, 4:6, 7:9) map(x, sum) [[1]] [1] 6 [[2]] [1] 15 [[3]] [1] 24 The map functions take some getting used to, and in my experience they are typically slower than the apply functions, sometimes notably so. However they allow you stay within the tidy realm, which has its own benefits, and have more control over the nature of the output23, which is especially important in reproducibility, package development, producing production-level code, etc. The key idea is that the map functions will always return something the same length as the input given to it. The purrr functions want a list or vector, i.e. they don’t work with data.frame objects like we’ve done with mutate and summarize except in the sense that data.frames are lists. ## mtcars %&gt;% ## map(scale) # returns a list, not shown mtcars %&gt;% map_df(scale) # returns a df # A tibble: 32 x 11 mpg cyl disp hp drat wt qsec vs am gear carb &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.151 -0.105 -0.571 -0.535 0.568 -0.610 -0.777 -0.868 1.19 0.424 0.735 2 0.151 -0.105 -0.571 -0.535 0.568 -0.350 -0.464 -0.868 1.19 0.424 0.735 3 0.450 -1.22 -0.990 -0.783 0.474 -0.917 0.426 1.12 1.19 0.424 -1.12 4 0.217 -0.105 0.220 -0.535 -0.966 -0.00230 0.890 1.12 -0.814 -0.932 -1.12 5 -0.231 1.01 1.04 0.413 -0.835 0.228 -0.464 -0.868 -0.814 -0.932 -0.503 6 -0.330 -0.105 -0.0462 -0.608 -1.56 0.248 1.33 1.12 -0.814 -0.932 -1.12 7 -0.961 1.01 1.04 1.43 -0.723 0.361 -1.12 -0.868 -0.814 -0.932 0.735 8 0.715 -1.22 -0.678 -1.24 0.175 -0.0278 1.20 1.12 -0.814 0.424 -0.503 9 0.450 -1.22 -0.726 -0.754 0.605 -0.0687 2.83 1.12 -0.814 0.424 -0.503 10 -0.148 -0.105 -0.509 -0.345 0.605 0.228 0.253 1.12 -0.814 0.424 0.735 # ... with 22 more rows mtcars %&gt;% map_dbl(sum) # returns a numeric (double) vector of column sums mpg cyl disp hp drat wt qsec vs am gear carb 642.900 198.000 7383.100 4694.000 115.090 102.952 571.160 14.000 13.000 118.000 90.000 diamonds %&gt;% map_at(c(&#39;carat&#39;, &#39;depth&#39;, &#39;price&#39;), function(x) as.integer(x &gt; median(x))) %&gt;% as_data_frame() # A tibble: 53,940 x 10 carat cut color clarity depth table price x y z &lt;int&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 Ideal E SI2 0 55 0 3.95 3.98 2.43 2 0 Premium E SI1 0 61 0 3.89 3.84 2.31 3 0 Good E VS1 0 65 0 4.05 4.07 2.31 4 0 Premium I VS2 1 58 0 4.2 4.23 2.63 5 0 Good J SI2 1 58 0 4.34 4.35 2.75 6 0 Very Good J VVS2 1 57 0 3.94 3.96 2.48 7 0 Very Good I VVS1 1 57 0 3.95 3.98 2.47 8 0 Very Good H SI1 1 55 0 4.07 4.11 2.53 9 0 Fair E VS2 1 61 0 3.87 3.78 2.49 10 0 Very Good H VS1 0 61 0 4 4.05 2.39 # ... with 53,930 more rows However, working with lists is very useful, so let’s turn to that. Working with lists Aside from data frames, you may think you don’t have much need for list objects. However, list objects make it very easy to iterate some form of data processing. Let’s say you have models of increasing complexity, and you want to easily summarise and/or compare them. library(mgcv) # for gam mtcars$cyl = factor(mtcars$cyl) mod_lm = lm(mpg ~ wt, data=mtcars) mod_poly = lm(mpg ~ poly(wt, 2), data=mtcars) mod_inter = lm(mpg ~ wt*cyl, data=mtcars) mod_gam = gam(mpg ~ s(wt), data=mtcars) mod_gam_inter = gam(mpg ~ cyl + s(wt, by=cyl), data=mtcars) model_list = list(mod_lm = mod_lm, mod_poly = mod_poly, mod_inter = mod_inter, mod_gam = mod_gam, mod_gam_inter = mod_gam_inter) # lowest wins model_list %&gt;% map_dbl(AIC) %&gt;% sort() mod_gam_inter mod_inter mod_poly mod_gam mod_lm 150.6324 155.4811 158.0484 158.5717 166.0294 # highest wins model_list %&gt;% map_dbl(function(x) if_else(inherits(x, &#39;gam&#39;), summary(x)$r.sq, summary(x)$adj) ) %&gt;% sort(decreasing = TRUE) mod_gam_inter mod_inter mod_poly mod_gam mod_lm 0.8643020 0.8349382 0.8065828 0.8041651 0.7445939 Let’s go further and create a plot of these results. We’ll map to a data frame, use gather to melt it to two columns of model and value, then use ggplot2 to plot the results24. model_list %&gt;% map_df(function(x) if_else(inherits(x, &#39;gam&#39;), summary(x)$r.sq, summary(x)$adj) ) %&gt;% gather(key = &#39;model&#39;, value = `Adj. Rsq`) %&gt;% arrange(desc(`Adj. Rsq`)) %&gt;% mutate(model = factor(model, levels = model)) %&gt;% # sigh ggplot(aes(x=model, y=`Adj. Rsq`)) + geom_point(aes(color=model), size=10, show.legend = F) Why not throw in AIC also? mod_rsq = model_list %&gt;% map_df(function(x) if_else(inherits(x, &#39;gam&#39;), summary(x)$r.sq, summary(x)$adj) ) %&gt;% gather(key = &#39;model&#39;, value=&#39;Rsq&#39;) mod_aic = map_df(model_list, AIC) %&gt;% gather(key=&#39;model&#39;, value=&#39;AIC&#39;) left_join(mod_rsq, mod_aic) %&gt;% arrange(AIC) %&gt;% mutate(model = factor(model, levels = model)) %&gt;% gather(key = &#39;measure&#39;, value=&#39;value&#39;, -model) %&gt;% ggplot(aes(x=model, y=value)) + geom_point(aes(color=model), size=10, show.legend = F) + facet_wrap(~measure, scales = &#39;free&#39;) List columns As data.frames are lists, anything can be put into a column just as you would a list element. We’ll use pmap here, as it can take more than one argument, and we’re feeding all columns of the data.frame. mtcars2 = as.matrix(mtcars) mtcars2[sample(1:length(mtcars2),50)] = NA mtcars2 = as_data_frame(mtcars2) mtcars2 = mtcars2 %&gt;% mutate(newvar = pmap(., ~ data.frame( N = sum(!is.na(c(...))), Miss = sum(is.na(c(...))) ))) Now check out the list column. mtcars2 # A tibble: 32 x 12 mpg cyl disp hp drat wt qsec vs am gear carb newvar &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 &lt;NA&gt; &lt;data.frame [1 × 2]&gt; 2 21.0 6 160.0 &lt;NA&gt; 3.90 2.875 17.02 0 1 4 4 &lt;data.frame [1 × 2]&gt; 3 &lt;NA&gt; &lt;NA&gt; 108.0 &quot; 93&quot; 3.85 2.320 18.61 1 1 4 &lt;NA&gt; &lt;data.frame [1 × 2]&gt; 4 21.4 6 &lt;NA&gt; 110 &lt;NA&gt; 3.215 19.44 1 0 3 1 &lt;data.frame [1 × 2]&gt; 5 18.7 8 &lt;NA&gt; 175 3.15 3.440 17.02 0 0 &lt;NA&gt; 2 &lt;data.frame [1 × 2]&gt; 6 18.1 6 225.0 105 2.76 3.460 &lt;NA&gt; 1 0 3 1 &lt;data.frame [1 × 2]&gt; 7 &lt;NA&gt; 8 360.0 &lt;NA&gt; &lt;NA&gt; 3.570 &lt;NA&gt; 0 0 &lt;NA&gt; 4 &lt;data.frame [1 × 2]&gt; 8 24.4 4 146.7 &quot; 62&quot; 3.69 3.190 20.00 &lt;NA&gt; &lt;NA&gt; 4 2 &lt;data.frame [1 × 2]&gt; 9 22.8 &lt;NA&gt; 140.8 &quot; 95&quot; 3.92 3.150 22.90 1 0 4 2 &lt;data.frame [1 × 2]&gt; 10 19.2 6 167.6 123 3.92 &lt;NA&gt; 18.30 1 0 4 4 &lt;data.frame [1 × 2]&gt; # ... with 22 more rows mtcars2$newvar %&gt;% head(3) [[1]] N Miss 1 10 1 [[2]] N Miss 1 10 1 [[3]] N Miss 1 8 3 Unnest it with the tidyr function. mtcars2 %&gt;% unnest(newvar) # A tibble: 32 x 13 mpg cyl disp hp drat wt qsec vs am gear carb N Miss &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 &lt;NA&gt; 10 1 2 21.0 6 160.0 &lt;NA&gt; 3.90 2.875 17.02 0 1 4 4 10 1 3 &lt;NA&gt; &lt;NA&gt; 108.0 &quot; 93&quot; 3.85 2.320 18.61 1 1 4 &lt;NA&gt; 8 3 4 21.4 6 &lt;NA&gt; 110 &lt;NA&gt; 3.215 19.44 1 0 3 1 9 2 5 18.7 8 &lt;NA&gt; 175 3.15 3.440 17.02 0 0 &lt;NA&gt; 2 9 2 6 18.1 6 225.0 105 2.76 3.460 &lt;NA&gt; 1 0 3 1 10 1 7 &lt;NA&gt; 8 360.0 &lt;NA&gt; &lt;NA&gt; 3.570 &lt;NA&gt; 0 0 &lt;NA&gt; 4 6 5 8 24.4 4 146.7 &quot; 62&quot; 3.69 3.190 20.00 &lt;NA&gt; &lt;NA&gt; 4 2 9 2 9 22.8 &lt;NA&gt; 140.8 &quot; 95&quot; 3.92 3.150 22.90 1 0 4 2 10 1 10 19.2 6 167.6 123 3.92 &lt;NA&gt; 18.30 1 0 4 4 10 1 # ... with 22 more rows This is a pretty esoteric demonstration, and not something you’d normally want to do, as mutate or other approaches would be far more efficient and sensical. However, the idea is that you might want to retain the information you might otherwise store in a list with the data that was used to create it. Once you have that, you can use that colum as you would any list for iterative programming. Iterative Programming Exercises Exercise 1 With the following matrix, use apply and the sum function to get row or column sums. x = matrix(1:9, 3, 3) Exercise 2 With the following list, use lapply and sapply and the sum function to get sums for the elements. There is no margin to specify for a list, so just supply the list and the sum function. x = list(1:3, 4:6, 7:9) Exercise 3 As in the previous example, use a map function to create a data frame of the column means. d = data_frame( x = rnorm(100), y = rnorm(100, 10, 2), z = rnorm(100, 50, 10), ) sapply is actually just a wrapper for lapply. If you supply the argument simplified=F, it is identical. Otherwise, it attempts to return a vector or matrix. Note that weather data frame is originally a tbl (tibble) class object, and that loop code won’t work with it unless it’s converted to a real data frame, because tibbles are annoying like that. You would have to do something like weather %&gt;% pull(column).↩ You can have 3 or more dimensional arrays, and apply will work over those dimensions (or any combination of them), but this doesn’t come up too often.↩ Personally I find the help file description nebulous or completely lacking, and the examples unrealistic, and oftentimes can come up with a group_by-summarise approach before I can sort out why the map approach didn’t work.↩ Note that ggplot2 will change the ordering of the variable unless you coerce it by creating a factor. This pretty much defeats the entire purpose of retaining characters over factors everywhere else in the tidyverse.↩ "],
["3_03_functions.html", "Writing functions", " Writing functions You can’t do anything in R without using functions, but have you ever written your own? Why would you? Efficiency Customized functionality Reproducibility Extend the work that’s already been done There are many benefits to writing your own functions, and it’s actually easy to do. Once you get the basic concept down, you’ll likely find yourself using your own functions more and more. A starting point Let’s assume you want to calculate the mean, standard deviation, and number of missing values for a variable, called myvar. We could do something like the following mean(myvar) sd(myvar) sum(is.na(myvar)) Now let’s say you need to do it for several variables. Here’s what your custom function would look like. It takes a single input, the variable you want information about, and returns a data frame with that info. my_summary &lt;- function(myvar) { data.frame( mean = mean(myvar, na.rm = TRUE), sd = sd(myvar, na.rm = TRUE), N_missing = sum(is.na(myvar)) ) } Note that if there are actually missing values, we need to set na.rm = TRUE or the mean and sd will return NA. Let’s try it. I add missing values to test that aspect. my_summary(mtcars$mpg) mean sd N_missing 1 20.09062 6.026948 0 mtcars2 = mtcars mtcars2$wt[c(3,10)] = NA my_summary(mtcars2$wt) mean sd N_missing 1 3.239733 0.9966999 2 Seems to work fine. Let’s add how many total observations there are. my_summary &lt;- function(myvar) { data.frame( mean = mean(myvar, na.rm = TRUE), sd = sd(myvar, na.rm = TRUE), N_total = length(myvar), N_missing = sum(is.na(myvar)) ) } That was easy! Let’s try it. my_summary(mtcars2$wt) mean sd N_total N_missing 1 3.239733 0.9966999 32 2 This shows that writing the first part of any function can be straightforward. Then, once in place, you can usually add functionality without too much trouble. Eventually you could have something very complicated, but which will make sense to you because you built it from the ground up. Now let’s do it for every column! mtcars2 %&gt;% map_dfr(my_summary, .id = &#39;variable&#39;) variable mean sd N_total N_missing 1 mpg 20.090625 6.0269481 32 0 2 cyl 6.187500 1.7859216 32 0 3 disp 230.721875 123.9386938 32 0 4 hp 146.687500 68.5628685 32 0 5 drat 3.596563 0.5346787 32 0 6 wt 3.239733 0.9966999 32 2 7 qsec 17.848750 1.7869432 32 0 8 vs 0.437500 0.5040161 32 0 9 am 0.406250 0.4989909 32 0 10 gear 3.687500 0.7378041 32 0 11 carb 2.812500 1.6152000 32 0 The map_dfr function is just like our previous usage in the [iterative programming][iterative] section, just that it will create mini-data.frames then row bind them together. DRY An oft-quoted mantra in programming is Don’t Repeat Yourself. One context regards iterative programming, where we would rather write one line of code than one-hundred. More generally though, we would like to gain efficiency where possible. A good rule of thumb is, if you are writing the same set of code more than twice, you should write a function to do it instead. Consider the following example, where we want to subset the data given a set of conditions. Given the cylinder, engine displacement, and mileage, we’ll get different parts of the data. good_mileage_displ_low_cyl_4 = if_else(cyl == 4 &amp; displ &lt; mean(displ) &amp; hwy &gt; 30, &#39;yes&#39;, &#39;no&#39;) good_mileage_displ_low_cyl_6 = if_else(cyl == 6 &amp; displ &lt; mean(displ) &amp; hwy &gt; 30, &#39;yes&#39;, &#39;no&#39;) good_mileage_displ_low_cyl_8 = if_else(cyl == 8 &amp; displ &lt; mean(displ) &amp; hwy &gt; 30, &#39;yes&#39;, &#39;no&#39;) good_mileage_displ_high_cyl_4 = if_else(cyl == 4 &amp; displ &gt; mean(displ) &amp; hwy &gt; 30, &#39;yes&#39;, &#39;no&#39;) good_mileage_displ_high_cyl_6 = if_else(cyl == 6 &amp; displ &gt; mean(displ) &amp; hwy &gt; 30, &#39;yes&#39;, &#39;no&#39;) good_mileage_displ_high_cyl_8 = if_else(cyl == 8 &amp; displ &gt; mean(displ) &amp; hwy &gt; 30, &#39;yes&#39;, &#39;no&#39;) It was tedious, but that’s not much code. But now consider- what if you want to change the mpg cutoff? The mean to median? Something else? You have to change all of it. Let’s write a function instead! good_mileage &lt;- function(cylinder = 4, mpg_cutoff = 30, displ_fun = mean, displ_low = TRUE, cls = &#39;compact&#39;) { if (displ_low) mpg %&gt;% filter(cyl == cylinder &amp; displ &lt;= displ_fun(displ) &amp; hwy &gt;= mpg_cutoff &amp; class == cls) else mpg %&gt;% filter(cyl == cylinder &amp; displ &gt;= displ_fun(displ) &amp; hwy &gt;= mpg_cutoff &amp; class == cls) } What’s going on? The function takes five inputs: cyl: Which cylinder type we want mpg_cutoff: The cutoff for ‘good’ mileage displ_fun: Whether the displacement to be based on the mean or something else displ_low: Whether we are interested in low or high displacement vehicles cls: the class of the vehicle (e.g. compact or suv) Then it simply filters the data to observations that match the input criteria. We also put default values, which can be done to your discretion. Now we can do whatever we want as needed: good_mileage(mpg_cutoff = 40) # A tibble: 1 x 11 manufacturer model displ year cyl trans drv cty hwy fl class &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 volkswagen jetta 1.9 1999 4 manual(m5) f 33 44 d compact good_mileage(cylinder = 8, mpg_cutoff = 15, displ_low = F, cls=&#39;suv&#39;) # A tibble: 34 x 11 manufacturer model displ year cyl trans drv cty hwy fl class &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 chevrolet c1500 suburban 2wd 5.3 2008 8 auto(l4) r 14 20 r suv 2 chevrolet c1500 suburban 2wd 5.3 2008 8 auto(l4) r 11 15 e suv 3 chevrolet c1500 suburban 2wd 5.3 2008 8 auto(l4) r 14 20 r suv 4 chevrolet c1500 suburban 2wd 5.7 1999 8 auto(l4) r 13 17 r suv 5 chevrolet c1500 suburban 2wd 6 2008 8 auto(l4) r 12 17 r suv 6 chevrolet k1500 tahoe 4wd 5.3 2008 8 auto(l4) 4 14 19 r suv 7 chevrolet k1500 tahoe 4wd 5.7 1999 8 auto(l4) 4 11 15 r suv 8 chevrolet k1500 tahoe 4wd 6.5 1999 8 auto(l4) 4 14 17 d suv 9 dodge durango 4wd 4.7 2008 8 auto(l5) 4 13 17 r suv 10 dodge durango 4wd 4.7 2008 8 auto(l5) 4 13 17 r suv # ... with 24 more rows Let’s extend the functionality by adding a year argument (only values available are 2008 and 1999). good_mileage &lt;- function(cylinder = 4, mpg_cutoff = 30, displ_fun = mean, displ_low = TRUE, cls = &#39;compact&#39;, yr = 2008) { if (displ_low) mpg %&gt;% filter(cyl == cylinder &amp; displ &lt;= displ_fun(displ) &amp; hwy &gt;= mpg_cutoff &amp; class == cls &amp; year == yr) else mpg %&gt;% filter(cyl == cylinder &amp; displ &gt;= displ_fun(displ) &amp; hwy &gt;= mpg_cutoff &amp; class == cls &amp; year == yr) } good_mileage(cylinder = 8, mpg_cutoff = 19, displ_low = F, cls=&#39;suv&#39;, yr = 2008) # A tibble: 6 x 11 manufacturer model displ year cyl trans drv cty hwy fl class &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 chevrolet c1500 suburban 2wd 5.3 2008 8 auto(l4) r 14 20 r suv 2 chevrolet c1500 suburban 2wd 5.3 2008 8 auto(l4) r 14 20 r suv 3 chevrolet k1500 tahoe 4wd 5.3 2008 8 auto(l4) 4 14 19 r suv 4 ford explorer 4wd 4.6 2008 8 auto(l6) 4 13 19 r suv 5 jeep grand cherokee 4wd 4.7 2008 8 auto(l5) 4 14 19 r suv 6 mercury mountaineer 4wd 4.6 2008 8 auto(l6) 4 13 19 r suv So we now have something that is flexible, reusable, and extensible, and it took less code than writing out the individual lines of code Lambda functions Oftentimes we just need a quick and easy function for a one-off application, especially when using apply/map functions. For example, both the following would calculate standard deviations of columns. apply(mtcars, 2, sd) apply(mtcars, 2, function(x) sd(x)) The difference between the two is that for the latter, our function didn’t have to be something already available. To further illustrate this, we’ll create a robust standardization function that uses the median and median absolute deviation rather than the mean and standard deviation. # some variables have a mad = 0, and so return Inf, NaN # apply(mtcars, 2, function(x) (x - median(x))/mad(x)) %&gt;% # head() mtcars %&gt;% map_df(function(x) (x - median(x))/mad(x)) # A tibble: 32 x 11 mpg cyl disp hp drat wt qsec vs am gear carb &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.333 0 -0.258 -0.169 0.291 -0.919 -0.883 NaN Inf 0 1.35 2 0.333 0 -0.258 -0.169 0.291 -0.587 -0.487 NaN Inf 0 1.35 3 0.665 -0.674 -0.629 -0.389 0.220 -1.31 0.636 Inf Inf 0 -0.674 4 0.407 0 0.439 -0.169 -0.873 -0.143 1.22 Inf NaN -0.674 -0.674 5 -0.0924 0.674 1.17 0.674 -0.774 0.150 -0.487 NaN NaN -0.674 0 6 -0.203 0 0.204 -0.233 -1.33 0.176 1.77 Inf NaN -0.674 -0.674 7 -0.905 0.674 1.17 1.58 -0.689 0.319 -1.32 NaN NaN -0.674 1.35 8 0.961 -0.674 -0.353 -0.791 -0.00710 -0.176 1.62 Inf NaN 0 0 9 0.665 -0.674 -0.395 -0.363 0.319 -0.228 3.67 Inf NaN 0 0 10 0 0 -0.204 0 0.319 0.150 0.417 Inf NaN 0 1.35 # ... with 22 more rows Even if you don’t use lambda functions yourself, it’s important to understand them, because you’ll often see other people’s code using them. Writing Functions Exercises Excercise 1 Write a function that takes the log of the sum of two values (i.e. just two single numbers) using the log function. log_sum &lt;- function(a, b) { ? } Exercise 2 Let’s write a function that will take a numeric variable and convert it to a character string of ‘positive’ vs. ‘negative’. We can use if {}... else {} structure, ifelse, or dplyr::if_else- they all would accomplish this. In this case, the input is a single vector of numbers, and the output will recode any negative value to ‘negative’ and positive values to ‘positive’ (or whatever you want). Here is an example of how we would just do it as a one-off. set.seed(123) # so you get the exact same &#39;random&#39; result x &lt;- rnorm(10) if_else(x &lt; 0, &quot;negative&quot;, &quot;positive&quot;) Now try your hand at writing a function for that. pos_neg &lt;- function(?) { ? } Thinking Exercises How would you write a check for Exercise 1 to make sure the inputs are numeric and positive (can’t take the log of non-positive values)? See ?stop and ?message for starters. "],
["3_04_more.html", "More", " More Vectorization Boolean Indexing Assume x is a vector of numbers. How would we create an index representing any value greater than 2? x = c(-1, 2, 10, -5) idx = x &gt; 2 idx [1] FALSE FALSE TRUE FALSE x[idx] [1] 10 As mentioned previously, logicals are objects with values of TRUE or FALSE, like the idx variable above. While sometimes we want to deal with the logical object as an end, it is extremely commonly used as an index in data processing. Note how we don’t have to create an explicit index object first (though often you should), as R indexing is ridiculously flexible. Here are more examples, not necessarily recommended, but just to demonstrate the flexibility of Boolean indexing. x[x &gt; 2] x[x != 3] x[ifelse(x &gt; 2 &amp; x !=10, TRUE, FALSE)] x[{y = idx; y}] x[resid(lm(y ~ x)) &gt; 0] Vectorized operations Boolean indexing allows us to take vectorized approaches to dealing with data. Consider the following unfortunately coded loop, where we create a variable y, which takes on the value of Yes if the variable x is greater than 2, and No if otherwise. for (i in 1:nrow(mydf)) { check = mydf$x[i] &gt; 2 if (check==TRUE) { mydf$y[i] = &#39;Yes&#39; } else { mydf$y[i] = &#39;No&#39; } } Compare25: mydf$y = &#39;No&#39; mydf$y[mydf$x &gt; 2] = &#39;Yes&#39; This gets us the same thing, and would be much faster than the looped approach. Boolean indexing is an example of a vectorized operation. The whole vector is considered, rather than each element individually. The result is that any preprocessing is done once rather than the n iterations of the loop. In R, this will always faster. Example: Log all values in a matrix. mymatrix_log = log(mymatrix) This is way faster than looping over elements, rows or columns. Here we’ll let the apply function stand in for our loop, logging the elements of each column. mymatrix = matrix(runif(100), 10, 10) identical(apply(mymatrix, 2, log), log(mymatrix)) [1] TRUE library(microbenchmark) microbenchmark(apply(mymatrix, 2, log), log(mymatrix)) Unit: nanoseconds expr min lq mean median uq max neval cld apply(mymatrix, 2, log) 20791 21229.5 23006.17 21507 21763.0 77567 100 b log(mymatrix) 621 652.5 778.53 715 753.5 6517 100 a Many vectorized functions already exist in R. They are often written in C, Fortran etc., and so even faster. Not all programming languages lean toward vectorized operations, and may not see much speed gain from it. In R however, you’ll want to prefer it. Even without, it’s cleaner/clearer code, another reason to use the approach. Timings The previous demonstrates how to use apply. However, there is a scale function in base R that uses a more vectorized approach under the hood. The following demonstrates various approaches to standardizing the columns of the matrix, even using a parallelized approach. As you’ll see, the base R function requires very little code and beats the others. mymat = matrix(rnorm(100000), ncol=1000) stdize &lt;- function(x) { (x-mean(x)) / sd(x) } doubleloop = function() { for (i in 1:ncol(mymat)) { x = mymat[, i] for (j in 1:length(x)) { x[j] = (x[j] - mean(x)) / sd(x) } } } singleloop = function() { for (i in 1:ncol(mymat)) { x = mymat[, i] x = (x - mean(x)) / sd(x) } } library(parallel) cl = makeCluster(8) clusterExport(cl, c(&#39;stdize&#39;, &#39;mymat&#39;)) doParallel::registerDoParallel(cl) test = microbenchmark::microbenchmark(doubleloop=doubleloop(), singleloop=singleloop(), apply=apply(mymat, 2, stdize), parApply=parApply(cl, mymat, 2, stdize), vectorized=scale(mymat), times=25) stopCluster(cl) test Unit: milliseconds expr min lq mean median uq max neval cld doubleloop 2882.148089 2904.739076 2938.21281 2946.57065 2952.25532 3013.29805 25 c singleloop 28.989806 29.512434 33.08177 31.17037 32.42606 84.79185 25 b apply 32.167901 33.350945 33.96146 34.13204 34.64055 36.18983 25 b parApply 17.691611 18.213638 21.91323 19.63893 21.12156 73.60683 25 ab vectorized 8.085578 8.491433 10.25583 10.66990 11.02532 12.62173 25 a Regular Expressions A regular expression, regex for short, is a sequence of characters that can be used as a search pattern for a string. Common operations are to merely detect, extract, or replace the matching string. There are actually many different flavors of regex for different programming languages, which are all flavors that originate with the Perl approach, or can enable the Perl approach to be used. However, knowing one means you pretty much know the others with only minor modifications if any. To be clear, not only is regex another language, it’s nigh on indecipherable. You will not learn much regex, but what you do learn will save a potentially enormous amount of time you’d otherwise spend trying to do things in a more haphazard fashion. Furthermore, practically every situation that will come up has already been asked and answered on Stack Overflow, so you’ll almost always be able to search for what you need. Here is an example: ^r.*shiny[0-9]$ What is that you may ask? Well here is an example of strings it would and wouldn’t match. string = c(&#39;r is the shiny&#39;, &#39;r is the shiny1&#39;, &#39;r shines brightly&#39;) grepl(string, pattern=&#39;^r.*shiny[0-9]$&#39;) What the regex is esoterically attempting to match is any string that starts with ‘r’ and ends with ‘shiny_’ where _ is some single digit. Specifically it breaks down as follows: ^ : starts with, so ^r means starts with r . : any character * : match the preceding zero or more times shiny : match ‘shiny’ [0-9] : any digit 0-9 (note that we are still talking about strings, not actual numbered values) $ : ends with preceding Typical Uses None of it makes sense, so don’t attempt to do so. Just try to remember a couple key approaches, and search the web for the rest. Along with ^ . * [0-9] $, a couple more common ones are: [a-z] : letters a-z [A-Z] : capital letters + : match the preceding one or more times () : groupings | : logical or e.g. [a-z]|[0-9] (a lower case letter or a number) ? : preceding item is optional, and will be matched at most once. Typically used for ‘look ahead’ and ‘look behind’ \\ : escape a character, like if you actually wanted to search for a period instead of using it as a regex pattern, you’d use \\., though in R you need \\\\, i.e. double slashes, for escape. In addition, in R there are certain predefined characters that can be called: [:punct:] : punctuation [:blank:] : spaces and tabs [:alnum:] : alphanumeric characters Those are just a few. The key functions can be found by looking at the help file for the grep function (?grep). However, the stringr package has the same functionality with perhaps a slightly faster processing (though that’s due to the underlying stringi package). See if you can guess which of the following will turn up TRUE. grepl(c(&#39;apple&#39;, &#39;pear&#39;, &#39;banana&#39;), pattern=&#39;a&#39;) grepl(c(&#39;apple&#39;, &#39;pear&#39;, &#39;banana&#39;), pattern=&#39;^a&#39;) grepl(c(&#39;apple&#39;, &#39;pear&#39;, &#39;banana&#39;), pattern=&#39;^a|a$&#39;) Scraping the web, munging data, just finding things in your scripts … you can potentially use this all the time, and not only with text analysis, as we’ll now see. Vector Exercises Exercise 1 Show a non-vectorized and a vectorized way to add a two to the numbers 1 through 3. ? Exercise 2 Of the following matrix, which do you think is faster? Test it. x = matrix(rpois(100000, lambda = 5), ncol = 100) colSums(x) apply(x, 2, sum) microbenchmark::microbenchmark( cs = colSums(x), app = apply(x, 2, sum) ) Regex Exercises Exercise 1 Using stringr and str_replace, replace all the states a’s with nothing. library(stringr) str_replace(state.name, pattern = ?, replacement = ?) For those familiar with ifelse, that would be applicable, but is not the point of the example.↩ "],
["999_summary.html", "Summary", " Summary With the right tools, data exploration can be: easier faster more efficient more fun! Use them to wring your data dry of what it has to offer. Recommended next steps: R for Data Science Advanced R Embrace a richer understanding of your data! "]
]
