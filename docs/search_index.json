[
["index.html", "Data Processing &amp; Visualization", " Data Processing &amp; Visualization Michael Clark https://m-clark.github.io 2017-10-26 "],
["01_intro.html", "Intro Outline", " Intro This document is the basis for multiple workshops, whose commmon goal is to provide some tools, tips, packages etc. that make data processing and visualization in R easier. It is oriented toward those who have had some exposure to R in an applied fashion, but would also be useful to someone coming to R from another programming language. It is not an introduction to R, nor does this document have anything to do with statistical analysis. The goal here is primarily to instill awareness, specifically of tools that will make your data exploration easier, and to understand some of the why behind the tools, so that one can better implement them. Outline Part 1: Data Processing Understanding Base R Approaches to Data Processing Overview of Data Structures Input/Output Vectorization and Apply functions Getting Acquainted with Other Approaches to Data Processing Pipes, and how to use them tidyverse data.table Misc. Part 2: Visualization Thinking Visually in progress Information Color Contrast and more… ggplot2 Aesthetics Layers Themes and more… Adding Interactivity Package demos Shiny Future… ? Part 3: Programming Basics Part 4: Presentation Part 5: Modeling Color coding in text: emphasis package function object/class link Some key packages used in the following demonstrations and exercises: dplyr, tidyr, ggplot2, data.table, plotly, ggplot2movies Many others are also used, feel free to install as we come across them. Here are a few. DT, highcharter, magrittr, maps, mgcv, plotly, quantmod, readr, visNetwork "],
["02_dataStructures.html", "Part 1: Data Processing Data Structures", " Part 1: Data Processing Data Structures R has several core data structures, and we’ll take a look at each. Vectors Factors Lists Matrices/arrays Data frames The more you know about R data structures, the more you’ll know how to use them, the more you’ll know why things go wrong if they do, and the further you’ll be able to go with your data. Vectors Vectors form the basis of R data structures. Two main types are atomic and lists, but we’ll talk about lists separately. Here is an R vector. The elements of the vector are numeric values. x = c(1, 3, 2, 5, 4) x [1] 1 3 2 5 4 All elements of an atomic vector are the same type. Example types include: character numeric (double) integer logical Character strings When dealing with text, objects of class character are what you’d typically be dealing with. x = c(&#39;... Of Your Fake Dimension&#39;, &#39;Ephemeron&#39;, &#39;Dryswch&#39;, &#39;Isotasy&#39;, &#39;Memory&#39;) x Not much to it, but be aware there is no real limit to what is represented as a character vector. For example, in a data frame, you could have a column where each entry is one of the works of Shakespeare. Factors A important type of vector is a factor. Factors are used to represent categorical data structures. Although not exactly precise, one can think of factors as integers with labels. So the underlying representation of a variable for sex is 1:2 with labels ‘Male’ and ‘Female’. They are a special class with attributes, or metadata, that contains the information about the levels. x = factor(rep(letters[1:3], e=10)) x [1] a a a a a a a a a a b b b b b b b b b b c c c c c c c c c c Levels: a b c attributes(x) $levels [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; $class [1] &quot;factor&quot; The underlying representation is numeric, but it is important to remember that factors are categorical. Thus, they can’t be used as numbers would be, as the following demonstrates. x_num = as.numeric(x) # convert to a numeric object sum(x_num) [1] 60 sum(x) Error in Summary.factor(structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, : &#39;sum&#39; not meaningful for factors Strings vs. Factors The main thing to note is that factors are generally a statistical phenomenon, and are required to do statistical things with data that would otherwise be a simple character string. If you know the relatively few levels the data can take, you’ll generally want to use factors, or at least know that statistical packages and methods may require them. In addition, factors allow you to easily overcome the silly default alphabetical ordering of category levels in some very popular visualization packages. For other things, such as text analysis, you’ll almost certainly want character strings instead, and in many cases it will be required. It’s also worth noting that a lot of base R and other behavior will coerce strings to factors. This made a lot more sense in the early days of R, but is not really necessary these days. Some packages to note to help you with processing strings and factors: forcats stringr Matrices With multiple dimensions, we are dealing with arrays. Matrices are 2-d arrays, and extremely commonly used for scientific computing1. The vectors making up a matrix must all be of the same type. For example, all values in a matrix might be numeric, or all character strings. Creating a matrix Creating a matrix can be done in a variety of ways. # create vectors x = 1:4 y = 5:8 z = 9:12 rbind(x, y, z) # row bind [,1] [,2] [,3] [,4] x 1 2 3 4 y 5 6 7 8 z 9 10 11 12 cbind(x, y, z) # column bind x y z [1,] 1 5 9 [2,] 2 6 10 [3,] 3 7 11 [4,] 4 8 12 matrix(c(x, y, z), nrow=3, ncol=4, byrow=TRUE) [,1] [,2] [,3] [,4] [1,] 1 2 3 4 [2,] 5 6 7 8 [3,] 9 10 11 12 Lists Lists in R are highly flexible objects, and probably the most commonly used for data science. Unlike vectors, whose elements must be of the same type, lists can contain anything as their elements, even other lists. Here is a list. We use the list function to create it. x = list(1, &quot;apple&quot;, list(3, &quot;cat&quot;)) x [[1]] [1] 1 [[2]] [1] &quot;apple&quot; [[3]] [[3]][[1]] [1] 3 [[3]][[2]] [1] &quot;cat&quot; We often want to loop some function over a list. for (elem in x) print(class(elem)) [1] &quot;numeric&quot; [1] &quot;character&quot; [1] &quot;list&quot; Lists can, and often do, have named elements. x = list(&quot;a&quot; = 25, &quot;b&quot; = -1, &quot;c&quot; = 0) x[&quot;b&quot;] $b [1] -1 Python has similar structures, lists and dictionaries, where the latter works similarly to R’s named list. Data Frames Data frames are a very commonly used data structure. Elements of a data frame can be different types, and this is because the data.frame class is actually just a list. As such, everything about lists applies to them. But they can also be indexed by row or column as well, just like matrices. There are other very common types of object classes associated with packages that are both a data.frame and some other type of structure (e.g. tibbles in the tidyverse). Usually you’re data frame will come directly from import or manipulation of other R objects (e.g. matrices). However, you should know how to create one from scratch. Creating a data frame The following will create a data frame with two columns, a and b. mydf = data.frame(a = c(1,5,2), b = c(3,8,1)) Much to the disdain of the tidyverse, we can add row names also. rownames(mydf) = paste0(&#39;row&#39;, 1:3) mydf a b row1 1 3 row2 5 8 row3 2 1 Everything about lists applies to data.frames, so we can add, select, and remove elements of a data frame just like lists. However, we’ll visit this later, because we’ll have much more flexibility with data frames than we would lists. Data Structure Exercises Excercise #1 Create an object that is a matrix and/or a data.frame, and inspect its class or structure (use the class or str functions). Exercise #2 Create a list of 3 elements, the first of which contains character strings, the second numbers, and the third, the data.frame or matrix you just created. Thinking Exercises How is a factor different from a character vector? How is a data.frame the same as and different from a matrix? How is a data.frame the same as and different from a list? Despite what the tidyverse would have you believe.↩ "],
["03_io.html", "Input/Output", " Input/Output Until you get comfortable getting data into R, you’re not going to use it as much as you would. You should at least be able to read in common data formats like comma/tab-separated, Excel, etc. Standard methods of reading in tabular data include the following functions: read.table read.csv readLines Base R also comes with the foreign package for reading in other types of files, especially other statistical packages. However, while you may see it still in use, it’s not as useful as what’s found in other packages. Reading in data is usually a one-off event, such that you’ll never need to use the package again after the data is loaded. In that case, you might use the following approach, so that you don’t need to attach the whole package. readr::read_csv(&#39;fileloc/filename.csv&#39;) You can use that for any package, which can help avoid naming conflicts by not loading a bunch of different packages. Furthermore, if you need packages that do have a naming conflict, using this approach will ensure the function from the package you want will be used. Other approaches There are some better and faster ways to read in data than the base R approach. A package for reading in foreign statistical files is haven, which has functions like read_spss and read_dta for SPSS and Stata files respectively. The package readxl is a clean way to read Excel files that doesn’t require any outside packages or languages. The package rio: uses haven, readxl etc., but with just two functions for everything: import, export (also convert). Faster approaches For faster versions of base R functions, readr has read_csv, read_delim, and others. These make assumptions after an initial scan of the data, but if you don’t have ‘big’ data, this won’t help much. However, they actually can be used as a diagnostic to pick up potential data entry errors. The data.table package provides a faster version read.table, and is typically faster than readr approaches (fread). Other Data Be aware that R can handle practically any type of data you want to throw at it. Some examples include: JSON SQL XML YAML MongoDB NETCDF text (e.g. a novel) shapefiles (e.g. for geographic data) google spreadsheets And many, many others. On the horizon feather is designed to make reading/writing data frames efficient, and the really nice thing about it is that it works in both Python and R. It’s still in early stages of development on the R side though. Big Data You may come across the situation where your data cannot be held in memory. One of the first things to be aware of for data processing is that you may not need to have the data all in memory at once. Before shifting to a hardward solution, consider if the following is possible. Chunking: reading and processing the data in chunks Line at a time: dealing with individual lines of data Other data formats: for example SQL databases (sqldf package, src_dbi in dplyr) However, it may be that the end result is still to large. In that case you’ll have to consider a cluster-based or distributed data situation. Of course R will have tools for that. DBI sparklyr RHadoop And more. I/O Exercises Exercise 1 Use readr and haven to read the following files. Use the url just like you would any filename. The latter is a Stata file. You can use the RStudio’s menu approach to import the file if you want. ‘https://raw.githubusercontent.com/m-clark/data-processing-and-visualization/master/data/cars.csv’ ‘https://raw.githubusercontent.com/m-clark/data-processing-and-visualization/master/data/presvote.dta’ Thinking Exercises Why might you use read_csv from the readr package rather than read.csv in base R? What is your definition of ‘big’ data? "],
["04_indexing.html", "Indexing", " Indexing Base R Indexing Refresher Slicing vectors letters[4:6] [1] &quot;d&quot; &quot;e&quot; &quot;f&quot; letters[c(13,10,3)] [1] &quot;m&quot; &quot;j&quot; &quot;c&quot; Slicing matrices/data.frames myMatrix[1, 2:3] Label-based indexing mydf[&#39;row1&#39;, &#39;b&#39;] Position-based indexing mydf[1, 2] Mixed indexing mydf[&#39;row1&#39;, 2] If the row/column value is empty, all rows/columns are retained. mydf[&#39;row1&#39;,] mydf[,&#39;b&#39;] Non-contiguous mydf[c(1,3),] Boolean mydf[mydf$a &gt;= 2,] List/Data.frame extraction [ : grab a slice of elements/columns [[ : grab specific elements/columns $ : grab specific elements/columns @: extract slot for S4 objects my_list_or_df[2:4] my_list_or_df[[&#39;name&#39;]] my_list_or_df$name Indexing Exercises This following is a refresher of base R indexing only. Here is a matrix, a data.frame and a list. mymatrix = matrix(rnorm(100), 10, 10) mydf = cars mylist = list(mymatrix, thisdf = mydf) Exercise 1 For the matrix, in separate operations, take a slice of rows, a selection of columns, and a single element. Exercise 2 For the data.frame, grab a column in 3 different ways. Exercise 3 For the list grab an element by number and by name. "],
["05_vectorapp.html", "Vectorization &amp; Apply", " Vectorization &amp; Apply Boolean Indexing Logicals are objects with values of TRUE or FALSE. Assume x is a vector of numbers. x = c(-1, 2, 10, -5) idx = x &gt; 2 idx [1] FALSE FALSE TRUE FALSE x[idx] [1] 10 Flexiblity We don’t have to create an explicit index object first, as R indexing is ridiculously flexible. x[x &gt; 2] x[x != 3] x[ifelse(x &gt; 2 &amp; x !=10, T, F)] x[{y = idx; y}] x[resid(lm(y ~ x)) &gt; 0] Vectorized operations Consider the following unfortunately coded loop: for (i in 1:nrow(mydf)) { check = mydf$x[i] &gt; 2 if (check==TRUE) { mydf$y[i] = &#39;Yes&#39; } else { mydf$y[i] = &#39;No&#39; } } Compare: mydf$y = &#39;No&#39; mydf$y[mydf$x &gt; 2] = &#39;Yes&#39; This gets us the same thing, and would be much faster than the looped approach. Boolean indexing is an example of a vectorized operation. The whole vector is considered: Rather than each element individually Any preprocessing is done once rather than n times This is always faster. Example: Log all values in a matrix. mymatrix_log = log(mymatrix) Way faster than looping over elements, rows or columns. Here we’ll let the apply function stand in for our loop, logging the elements of each column. mymatrix = matrix(runif(100), 10, 10) identical(apply(mymatrix, 2, log), log(mymatrix)) [1] TRUE library(microbenchmark) microbenchmark(apply(mymatrix, 2, log), log(mymatrix)) Unit: microseconds expr min lq mean median uq max neval apply(mymatrix, 2, log) 44.428 45.329 47.24382 45.929 46.529 121.876 100 log(mymatrix) 2.701 3.002 3.22721 3.002 3.302 9.306 100 Many vectorized functions already exist in R. They are often written in C, Fortran etc., and so even faster. Apply functions A family of functions allows for a succinct way of looping. Common ones include: apply arrays, matrices, data.frames lapply, sapply, vapply lists, data.frames, vectors tapply grouped operations (table apply) mapply multivariate version of sapply replicate similar to sapply Example Standardizing variables. for (i in 1:ncol(mydf)){ x = mydf[,i] for (j in 1:length(x)){ x[j] = (x[j] - mean(x))/sd(x) } } The above would be a really bad way to use R. The following is much cleaner and now you’d have a function you can use elsewhere. apply will take a matrix, and apply a function over the margin, row or column, you want. stdize &lt;- function(x) { (x-mean(x))/sd(x) } apply(mydf, 2, stdize) # 1 for rows, 2 for columnwise application Timings The previous demonstrates how to use apply. However, there is a scale function in base R that uses a more vectorized approach under the hood. The following demonstrates various approaches to standardizing the columns of the matrix, even using a parallelized approach. The base R function requires very little code and beats the others. mymat = matrix(rnorm(100000), ncol=1000) stdize &lt;- function(x) { (x-mean(x)) / sd(x) } doubleloop = function() { for (i in 1:ncol(mymat)) { x = mymat[, i] for (j in 1:length(x)) { x[j] = (x[j] - mean(x)) / sd(x) } } } singleloop = function() { for (i in 1:ncol(mymat)) { x = mymat[, i] x = (x - mean(x)) / sd(x) } } library(parallel) cl = makeCluster(8) clusterExport(cl, c(&#39;stdize&#39;, &#39;mymat&#39;)) doParallel::registerDoParallel(cl) test = microbenchmark::microbenchmark(doubleloop=doubleloop(), singleloop=singleloop(), apply=apply(mymat, 2, stdize), parApply=parApply(cl, mymat, 2, stdize), vectorized=scale(mymat), times=25) stopCluster(cl) test Unit: milliseconds expr min lq mean median uq max neval doubleloop 2882.148089 2904.739076 2938.21281 2946.57065 2952.25532 3013.29805 25 singleloop 28.989806 29.512434 33.08177 31.17037 32.42606 84.79185 25 apply 32.167901 33.350945 33.96146 34.13204 34.64055 36.18983 25 parApply 17.691611 18.213638 21.91323 19.63893 21.12156 73.60683 25 vectorized 8.085578 8.491433 10.25583 10.66990 11.02532 12.62173 25 Apply functions Benefits Cleaner/simpler code Environment kept clear of unnecessary objects Potentially more reproducible more likely to use generalizable functions Might be faster Parallelizable NOT necessarily faster than explicit loops. single loop over columns was as fast as apply Replicate and mapply are especially slow ALWAYS can potentially be faster than loops. Parallelization: parApply, parLapply etc. Personal experience I use R every day, and rarely use explicit loops. Note: no speed difference for a for loop vs. using while If you must use an explicit loop, create an empty object of the dimension/form you need, and then fill it in. Notably faster I pretty much never use an explicit double loop, as a little more thinking about the problem will usually provide a more efficient path to solving the problem. Apply functions Apply functions and similar approaches should be a part of your regular R experience. Other versions we’ll talk about have been optimized, but you need to know the basics in order to use those. Any you still may need parallel versions. Vector/Apply Exercises Exercise 1 With the following matrix, use apply and the sum function to get row or column sums. x = matrix(1:9, 3, 3) Exercise 2 With the following list, use lapply and sapply and the sum function to get sums for the elements. There is no margin to specify with on a list, so just supply the list and the sum function. x = list(1:3, 4:6, 7:9) sapply is actually just a wrapper for lapply. If you supply the argument simplified=F, it is identical. Otherwise, it attempts to return a vector or matrix. "],
["06_pipes.html", "Pipes", " Pipes Note: More detail on much of the following is given in another workshop. Pipes are operators that send what comes before the pipe to what comes after. There are many different pipes, and some packages that use their own. However, the vast majority of packages use the same pipe: %&gt;% Here, we’ll focus on their use with the dplyr package. Later, we’ll use it for visualizations. Example: mydf %&gt;% select(var1, var2) %&gt;% filter(var1 == &#39;Yes&#39;) %&gt;% summary Start with a data.frame %&gt;% select columns from it %&gt;% filter/subset it %&gt;% get a summary Using variables as they are created One nice thing about pipelines is that we can use variables as soon as they are created, with out having to break out separate objects/steps. mydf %&gt;% mutate(newvar1 = var1 + var2, newvar2 = newvar1/var3) %&gt;% summarise(newvar2avg = mean(newvar2)) Pipes for Visualization (more later) The following provides a means to think about pipes for visualization. It’s just a generic example for now, but we’ll see more later. basegraph %&gt;% points %&gt;% lines %&gt;% layout The dot Most functions are not ‘pipe-aware’ by default. Example: pipe to a modeling function. mydf %&gt;% lm(y ~ x) # error Other pipes could work in this situation. e.g. %$% in magrittr. But generally, when you come upon this situation, you can use a dot. The dot refers to the object before the previous pipe. mydf %&gt;% lm(y ~ x, data=.) # . == mydf Flexibility Piping is not just for data.frames. The following starts with a character vector. Sends it to a recursive function (named ..). .. is created on-the-fly, and has a single argument (.). After the function is created, it’s used on ., whcih represents the string before the pipe. Result: pipes between the words. c(&#39;Ceci&#39;, &quot;n&#39;est&quot;, &#39;pas&#39;, &#39;une&#39;, &#39;pipe!&#39;) %&gt;% { .. &lt;- . %&gt;% if (length(.) == 1) . else paste(.[1], &#39;%&gt;%&#39;, ..(.[-1])) ..(.) } [1] &quot;Ceci %&gt;% n&#39;est %&gt;% pas %&gt;% une %&gt;% pipe!&quot; Put that in your pipe and smoke it René Magritte! Summary Pipes are best used interactively, and they are extremely useful for data exploration. Nowadays, more and more packages are being made that are ‘pipe-aware’, especially many visualization packages. See the magrittr package for more pipes. "],
["07_tidyverse.html", "Tidyverse", " Tidyverse What is the tidyverse? The tidyverse consists of a few key packages- ggplot2: data visualization dplyr: data manipulation tidyr: data tidying readr: data import purrr: functional programming, e.g. alternate approaches to apply tibble: tibbles, a modern re-imagining of data frames And of course the tidyverse package which will load all of the above. See also: lubridate, rvest, stringr and others in the ‘hadleyverse’. What is tidy? Tidy data refers to data arranged in a way that makes data processing, analysis, and visualization simpler. In a tidy data set: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Think long before wide. dplyr dplyr provides a grammar of data manipulation (like ggplot2 does for visualization). It is the next iteration of plyr, but there is no longer any need for plyr really. It’s focused on tools for working with data frames. Over 100 functions It has three main goals: Make the most important data manipulation tasks easier. Do them faster. Use the same interface to work with data frames, data tables or database. Some key operations: select: grab columns select helpers: one_of, starts_with, num_range etc. filter/slice: grab rows group_by: grouped operations mutate/transmute: create new variables summarize: summarize/aggregate do: arbitrary operations Various join/merge functions: inner_join, left_join etc. Little things like: n, n_distinct, nth, n_groups, count, recode, between No need to quote variable names. An example Let’s say we want to select from our data the following variables: Start with the ID variable The variables X1:X10, which are not all together, and there are many more X columns The variables var1 and var2, which are the only var variables in the data Any variable that starts with XYZ How might we go about this? Some base R approaches Tedious, or multiple objects just to get the columns you want. # numeric indexes; not conducive to readibility or reproducibility newData = oldData[,c(1,2,3,4, etc.)] # explicitly by name; fine if only a handful; not pretty newData = oldData[,c(&#39;ID&#39;,&#39;X1&#39;, &#39;X2&#39;, etc.)] # two step with grep; regex difficult to read/understand cols = c(&#39;ID&#39;, paste0(&#39;X&#39;, 1:10), &#39;var1&#39;, &#39;var2&#39;, grep(colnames(oldData), &#39;^XYZ&#39;, value=T)) newData = oldData[,cols] # or via subset newData = subset(oldData, select = cols) More What if you also want observations where Z is Yes, Q is No, and only the observations with the top 50 values of var2, ordered by var1 (descending)? # three operations and overwriting or creating new objects if we want clarity newData = newData[oldData$Z == &#39;Yes&#39; &amp; oldData$Q == &#39;No&#39;,] newData = newData[order(newData$var2, decreasing=T)[1:50],] newData = newData[order(newData$var1, decreasing=T),] And this is for fairly straightforward operations. The dplyr way newData = oldData %&gt;% filter(Z == &#39;Yes&#39;, Q == &#39;No&#39;) %&gt;% select(num_range(&#39;X&#39;, 1:10), contains(&#39;var&#39;), starts_with(&#39;XYZ&#39;)) %&gt;% top_n(n=50, var2) %&gt;% arrange(desc(var1)) An alternative dplyr and piping is an alternative. You can do all this sort of stuff with base R, for example, with with, within, subset, transform, etc. Even though the initial base R approach depicted is fairly concise, it still can potentially be: noisier less legible less amenable to additional data changes requires esoteric knowledge (e.g. regular expressions) often requires creation of new objects (even if we just want to explore) often slower, possibly greatly Running example The following data was scraped initially scraped from the web as follows. However you can just load it into your workspace. library(rvest); url = &quot;http://www.basketball-reference.com/leagues/NBA_2017_totals.html&quot; bball = read_html(url) %&gt;% html_nodes(&quot;#totals_stats&quot;) %&gt;% html_table %&gt;% data.frame save(bball, file=&#39;data/bball.RData&#39;) When initially downloaded, the data is all character strings. We’ll fix this later. load(&#39;data/bball.RData&#39;) glimpse(bball[,1:5]) Observations: 619 Variables: 5 $ Rk &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;13&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;, &quot;16&quot;, &quot;17&quot;, &quot;18&quot;, &quot;19&quot;, &quot;20&quot;, &quot;Rk&quot;, &quot;21&quot;, &quot;22&quot;, &quot;23&quot;, &quot;24&quot;, &quot;25&quot;, &quot;26&quot;, &quot;27&quot;, &quot;28&quot;, &quot;29&quot;, &quot;29&quot;, &quot;... $ Player &lt;chr&gt; &quot;Alex Abrines&quot;, &quot;Quincy Acy&quot;, &quot;Quincy Acy&quot;, &quot;Quincy Acy&quot;, &quot;Steven Adams&quot;, &quot;Arron Afflalo&quot;, &quot;Alexis Ajinca&quot;, &quot;Cole Aldrich&quot;, &quot;LaMarcus Aldridge&quot;, &quot;Lavoy Allen&quot;, &quot;Tony Allen&quot;, &quot;Al-Farouq Aminu&quot;, &quot;Chris ... $ Pos &lt;chr&gt; &quot;SG&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;C&quot;, &quot;SG&quot;, &quot;C&quot;, &quot;C&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;SG&quot;, &quot;SF&quot;, &quot;C&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SG&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;C&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;Pos&quot;, &quot;C&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;SG&quot;, &quot;PG&quot;, &quot;SG&quot;, &quot;PG&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;... $ Age &lt;chr&gt; &quot;23&quot;, &quot;26&quot;, &quot;26&quot;, &quot;26&quot;, &quot;23&quot;, &quot;31&quot;, &quot;28&quot;, &quot;28&quot;, &quot;31&quot;, &quot;27&quot;, &quot;35&quot;, &quot;26&quot;, &quot;38&quot;, &quot;34&quot;, &quot;23&quot;, &quot;23&quot;, &quot;23&quot;, &quot;23&quot;, &quot;28&quot;, &quot;22&quot;, &quot;32&quot;, &quot;34&quot;, &quot;31&quot;, &quot;28&quot;, &quot;Age&quot;, &quot;30&quot;, &quot;29&quot;, &quot;27&quot;, &quot;23&quot;, &quot;20&quot;, &quot;34&quot;, &quot;32&quot;, &quot;24&quot;, &quot;... $ Tm &lt;chr&gt; &quot;OKC&quot;, &quot;TOT&quot;, &quot;DAL&quot;, &quot;BRK&quot;, &quot;OKC&quot;, &quot;SAC&quot;, &quot;NOP&quot;, &quot;MIN&quot;, &quot;SAS&quot;, &quot;IND&quot;, &quot;MEM&quot;, &quot;POR&quot;, &quot;CLE&quot;, &quot;LAC&quot;, &quot;TOT&quot;, &quot;DAL&quot;, &quot;PHI&quot;, &quot;SAS&quot;, &quot;HOU&quot;, &quot;MIL&quot;, &quot;NYK&quot;, &quot;SAS&quot;, &quot;HOU&quot;, &quot;DEN&quot;, &quot;Tm&quot;, &quot;NOP&quot;, &quot;ORL&quot;, &quot;MIA&quot;, &quot;NYK&quot;... Selecting Columns Often you do not need the entire data set. While this is easily handled in base R (as shown earlier), it can be more clear to use select in dplyr. Now we won’t have to create separate objects, use quotes or $, etc. bball %&gt;% select(Player, Tm, Pos) %&gt;% head Player Tm Pos 1 Alex Abrines OKC SG 2 Quincy Acy TOT PF 3 Quincy Acy DAL PF 4 Quincy Acy BRK PF 5 Steven Adams OKC C 6 Arron Afflalo SAC SG What if we want to drop some variables? bball %&gt;% select(-Player, -Tm, -Pos) %&gt;% head Rk Age G GS MP FG FGA FG. X3P X3PA X3P. X2P X2PA X2P. eFG. FT FTA FT. ORB DRB TRB AST STL BLK TOV PF PTS 1 1 23 68 6 1055 134 341 .393 94 247 .381 40 94 .426 .531 44 49 .898 18 68 86 40 37 8 33 114 406 2 2 26 38 1 558 70 170 .412 37 90 .411 33 80 .413 .521 45 60 .750 20 95 115 18 14 15 21 67 222 3 2 26 6 0 48 5 17 .294 1 7 .143 4 10 .400 .324 2 3 .667 2 6 8 0 0 0 2 9 13 4 2 26 32 1 510 65 153 .425 36 83 .434 29 70 .414 .542 43 57 .754 18 89 107 18 14 15 19 58 209 5 3 23 80 80 2389 374 655 .571 0 1 .000 374 654 .572 .571 157 257 .611 281 332 613 86 89 78 146 195 905 6 4 31 61 45 1580 185 420 .440 62 151 .411 123 269 .457 .514 83 93 .892 9 116 125 78 21 6 42 104 515 Helper functions Sometimes, we have a lot of variables to select, and if they have a common naming scheme, this can be very easy. bball %&gt;% select(Player, contains(&quot;3P&quot;), ends_with(&quot;RB&quot;)) %&gt;% arrange(desc(TRB)) %&gt;% head Player X3P X3PA X3P. ORB DRB TRB 1 Player 3P 3PA 3P% ORB DRB TRB 2 Player 3P 3PA 3P% ORB DRB TRB 3 Player 3P 3PA 3P% ORB DRB TRB 4 Player 3P 3PA 3P% ORB DRB TRB 5 Player 3P 3PA 3P% ORB DRB TRB 6 Player 3P 3PA 3P% ORB DRB TRB I probably don’t even need to explain what’s being done above, and this is the power of the tidyverse way. Here are the list of helper functions to be aware of: starts_with: starts with a prefix ends_with: ends with a prefix contains: contains a literal string matches: matches a regular expression num_range: a numerical range like x01, x02, x03. one_of: variables in character vector. everything: all variables. Filtering Rows There are repeated header rows in this data2, so we need to drop them. This is also why everything was character string when we first read it in, because having any character strings makes everything a character string. Filtering by rows requires the basic indexing knowledge we talked about before, especially boolean indexing. In the following, Rk, or rank, is really just a row id, but if it equals ‘Rk’, we know it’s a header row, so we’ll drop it. bball = bball %&gt;% filter(Rk != &quot;Rk&quot;) filter returns rows with matching conditions. slice allows for a numeric indexing approach. Say we want too look at forwards (SF or PF) over the age of 35. The following will do this, and since some players play on multiple teams, we’ll want only the unique information on the variables of interest. The function distinct allows us to do this. bball %&gt;% filter(Age &gt; 35, Pos == &quot;SF&quot; | Pos == &quot;PF&quot;) %&gt;% distinct(Player, Pos, Age) Player Pos Age 1 Matt Barnes SF 36 2 Vince Carter SF 40 3 Nick Collison PF 36 4 Mike Dunleavy SF 36 5 Richard Jefferson SF 36 6 Dahntay Jones SF 36 7 James Jones SF 36 8 Mike Miller SF 36 9 Dirk Nowitzki PF 38 10 Paul Pierce SF 39 11 Luis Scola PF 36 12 Metta World Peace SF 37 or the first 10 rows… bball %&gt;% slice(1:10) # A tibble: 10 x 30 Rk Player Pos Age Tm G GS MP FG FGA FG. X3P X3PA X3P. X2P X2PA X2P. eFG. FT FTA FT. ORB DRB TRB AST STL BLK TOV PF PTS &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1 Alex Abrines SG 23 OKC 68 6 1055 134 341 .393 94 247 .381 40 94 .426 .531 44 49 .898 18 68 86 40 37 8 33 114 406 2 2 Quincy Acy PF 26 TOT 38 1 558 70 170 .412 37 90 .411 33 80 .413 .521 45 60 .750 20 95 115 18 14 15 21 67 222 3 2 Quincy Acy PF 26 DAL 6 0 48 5 17 .294 1 7 .143 4 10 .400 .324 2 3 .667 2 6 8 0 0 0 2 9 13 4 2 Quincy Acy PF 26 BRK 32 1 510 65 153 .425 36 83 .434 29 70 .414 .542 43 57 .754 18 89 107 18 14 15 19 58 209 5 3 Steven Adams C 23 OKC 80 80 2389 374 655 .571 0 1 .000 374 654 .572 .571 157 257 .611 281 332 613 86 89 78 146 195 905 6 4 Arron Afflalo SG 31 SAC 61 45 1580 185 420 .440 62 151 .411 123 269 .457 .514 83 93 .892 9 116 125 78 21 6 42 104 515 7 5 Alexis Ajinca C 28 NOP 39 15 584 89 178 .500 0 4 .000 89 174 .511 .500 29 40 .725 46 131 177 12 20 22 31 77 207 8 6 Cole Aldrich C 28 MIN 62 0 531 45 86 .523 0 0 45 86 .523 .523 15 22 .682 51 107 158 25 25 23 17 85 105 9 7 LaMarcus Aldridge PF 31 SAS 72 72 2335 500 1049 .477 23 56 .411 477 993 .480 .488 220 271 .812 172 351 523 139 46 88 98 158 1243 10 8 Lavoy Allen PF 27 IND 61 5 871 77 168 .458 0 1 .000 77 167 .461 .458 23 33 .697 105 114 219 57 18 24 29 78 177 This can be done with things that are created on the fly… bball %&gt;% unite(&quot;posTeam&quot;, Pos, Tm) %&gt;% # create a new variable filter(posTeam == &quot;PF_SAS&quot;) %&gt;% # use it for filtering arrange(desc(Age)) # order Rk Player posTeam Age G GS MP FG FGA FG. X3P X3PA X3P. X2P X2PA X2P. eFG. FT FTA FT. ORB DRB TRB AST STL BLK TOV PF PTS 1 259 David Lee PF_SAS 33 79 10 1477 248 420 .590 0 0 248 420 .590 .590 80 113 .708 149 292 441 124 31 40 82 125 576 2 7 LaMarcus Aldridge PF_SAS 31 72 72 2335 500 1049 .477 23 56 .411 477 993 .480 .488 220 271 .812 172 351 523 139 46 88 98 158 1243 3 44 Davis Bertans PF_SAS 24 67 6 808 103 234 .440 69 173 .399 34 61 .557 .588 28 34 .824 22 76 98 46 20 28 32 75 303 Generating New Data One of the most common data processing tasks is generating new variables. The function mutate takes a vector and returns one of the same dimension. In addition, there is mutate_at, mutate_if, and mutate_all to help with specific scenarios. To demonstrate, we’ll use mutate_at to make appropriate columns numeric, i.e. everything except Player, Pos, and Tm. It takes two inputs, variables and functions to apply. bball = bball %&gt;% mutate_at(vars(-Player, -Pos, -Tm), funs(as.numeric)) glimpse(bball[,1:7]) Observations: 595 Variables: 7 $ Rk &lt;dbl&gt; 1, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 13, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 29, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,... $ Player &lt;chr&gt; &quot;Alex Abrines&quot;, &quot;Quincy Acy&quot;, &quot;Quincy Acy&quot;, &quot;Quincy Acy&quot;, &quot;Steven Adams&quot;, &quot;Arron Afflalo&quot;, &quot;Alexis Ajinca&quot;, &quot;Cole Aldrich&quot;, &quot;LaMarcus Aldridge&quot;, &quot;Lavoy Allen&quot;, &quot;Tony Allen&quot;, &quot;Al-Farouq Aminu&quot;, &quot;Chris ... $ Pos &lt;chr&gt; &quot;SG&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;C&quot;, &quot;SG&quot;, &quot;C&quot;, &quot;C&quot;, &quot;PF&quot;, &quot;PF&quot;, &quot;SG&quot;, &quot;SF&quot;, &quot;C&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;SG&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;C&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;C&quot;, &quot;PG&quot;, &quot;SF&quot;, &quot;SG&quot;, &quot;PG&quot;, &quot;SG&quot;, &quot;PG&quot;, &quot;PF&quot;, &quot;SF&quot;, &quot;SF&quot;, &quot;S... $ Age &lt;dbl&gt; 23, 26, 26, 26, 23, 31, 28, 28, 31, 27, 35, 26, 38, 34, 23, 23, 23, 23, 28, 22, 32, 34, 31, 28, 30, 29, 27, 23, 20, 34, 32, 24, 36, 36, 36, 26, 31, 28, 28, 30, 27, 23, 20, 28, 30, 22, 19, 23, 21, 24, ... $ Tm &lt;chr&gt; &quot;OKC&quot;, &quot;TOT&quot;, &quot;DAL&quot;, &quot;BRK&quot;, &quot;OKC&quot;, &quot;SAC&quot;, &quot;NOP&quot;, &quot;MIN&quot;, &quot;SAS&quot;, &quot;IND&quot;, &quot;MEM&quot;, &quot;POR&quot;, &quot;CLE&quot;, &quot;LAC&quot;, &quot;TOT&quot;, &quot;DAL&quot;, &quot;PHI&quot;, &quot;SAS&quot;, &quot;HOU&quot;, &quot;MIL&quot;, &quot;NYK&quot;, &quot;SAS&quot;, &quot;HOU&quot;, &quot;DEN&quot;, &quot;NOP&quot;, &quot;ORL&quot;, &quot;MIA&quot;, &quot;NYK&quot;, &quot;MEM... $ G &lt;dbl&gt; 68, 38, 6, 32, 80, 61, 39, 62, 72, 61, 71, 61, 12, 30, 75, 51, 24, 72, 72, 80, 74, 19, 80, 41, 31, 78, 68, 52, 33, 67, 35, 79, 74, 54, 20, 60, 52, 77, 3, 75, 73, 77, 22, 56, 74, 38, 43, 23, 3, 67, 67,... $ GS &lt;dbl&gt; 6, 1, 0, 1, 80, 45, 15, 0, 72, 5, 66, 25, 0, 0, 10, 2, 8, 14, 72, 80, 74, 0, 80, 7, 19, 20, 55, 13, 1, 0, 6, 79, 18, 13, 5, 19, 0, 77, 1, 2, 64, 77, 1, 6, 0, 1, 0, 1, 0, 6, 67, 27, 1, 16, 66, 54, 54, ... The following demonstrates how we can create composites of existing variables. bball = bball %&gt;% mutate(trueShooting = PTS / (2 * (FGA + (.44 * FTA))), effectiveFG = (FG + (.5 * X3P)) / FGA, shootingDif = trueShooting - FG.) summary(select(bball, shootingDif)) # select and others don&#39;t have to be piped to use shootingDif Min. :-0.46809 1st Qu.: 0.05202 Median : 0.09072 Mean : 0.08555 3rd Qu.: 0.11760 Max. : 0.39787 NA&#39;s :2 Grouping and Summarizing Data A very common task is to look at group-based statistics, and we can use group_by and summarize to help us in this regard3. Base R has things like aggregate and tapply, but they should never be used, and this approach is much more straightforward. For this, I’m going to start putting together several things we’ve demonstrated thus far bball %&gt;% mutate(trueShooting = PTS / (2 * (FGA + (.44 * FTA))), effectiveFG = (FG + (.5 * X3P)) / FGA, shootingDif = trueShooting - FG.) %&gt;% select(Player, Tm, Pos, MP, trueShooting, effectiveFG, PTS) %&gt;% group_by(Pos) %&gt;% # can group by more than one variable summarize(meanTrueShooting = mean(trueShooting, na.rm = TRUE)) # potentially any function can work # A tibble: 6 x 2 Pos meanTrueShooting &lt;chr&gt; &lt;dbl&gt; 1 C 0.5646389 2 PF 0.5164029 3 PF-C 0.5093448 4 PG 0.5105353 5 SF 0.5295641 6 SG 0.5154494 Use do on grouped data to go further. The following will group data by position, then get the correlation between field-goal percentage and free-throw shooting percentage. For some reason, one individual was labeled as ‘PF-C’. bball %&gt;% mutate(Pos = if_else(Pos==&#39;PF-C&#39;, &#39;C&#39;, Pos)) %&gt;% group_by(Pos) %&gt;% do(FgFt_Corr=cor(.$FG., .$FT., use=&#39;complete&#39;)) %&gt;% unnest(FgFt_Corr) # A tibble: 5 x 2 Pos FgFt_Corr &lt;chr&gt; &lt;dbl&gt; 1 C -0.43286280 2 PF -0.08935286 3 PG 0.12087949 4 SF 0.02330231 5 SG -0.01293017 As a reminder, data frames are lists. As such, anything can go into the ‘columns’, even the results of analyses. library(nycflights13) carriers = group_by(flights, carrier) group_size(carriers) [1] 18460 32729 714 54635 48110 54173 685 3260 342 26397 32 58665 20536 5162 12275 601 mods = do(carriers, model = lm(arr_delay ~ dep_time, data = .)) # reminder that data frames are lists mods %&gt;% summarize(rsq = summary(model)$r.squared) %&gt;% head # A tibble: 6 x 1 rsq &lt;dbl&gt; 1 0.05135040 2 0.05041396 3 0.08277597 4 0.02407390 5 0.03472122 6 0.08360260 Merging Data tidyr Tidyr can be thought of as a more special subset of dplyr functionality. Two primary functions for manipulating data you’ll want ot be familiar with are: gather: wide to long spread: long to wide Other useful functions include: unite: paste together multiple columns into one separate: complement of unite unnest: unnest ‘list columns’ Example library(tidyr) stocks &lt;- data.frame( time = as.Date(&#39;2009-01-01&#39;) + 0:9, X = rnorm(10, 0, 1), Y = rnorm(10, 0, 2), Z = rnorm(10, 0, 4) ) stocks %&gt;% head time X Y Z 1 2009-01-01 1.7573283 -1.2974905 -5.987739 2 2009-01-02 0.6129562 2.0674955 -1.699401 3 2009-01-03 -0.3037085 0.5406812 1.278685 4 2009-01-04 0.6739817 0.7641690 1.967867 5 2009-01-05 -1.1760121 -0.5740989 -3.423910 6 2009-01-06 -0.1963246 0.7183890 -4.197356 stocks %&gt;% gather(stock, price, -time) %&gt;% head time stock price 1 2009-01-01 X 1.7573283 2 2009-01-02 X 0.6129562 3 2009-01-03 X -0.3037085 4 2009-01-04 X 0.6739817 5 2009-01-05 X -1.1760121 6 2009-01-06 X -0.1963246 Note that the latter is an example of tidy data while the former is not. Separate player into first and last names based on the space bball %&gt;% separate(Player, into=c(&#39;firstName&#39;, &#39;lastName&#39;), sep=&#39; &#39;) %&gt;% select(1:5) %&gt;% head Rk firstName lastName Pos Age 1 1 Alex Abrines SG 23 2 2 Quincy Acy PF 26 3 2 Quincy Acy PF 26 4 2 Quincy Acy PF 26 5 3 Steven Adams C 23 6 4 Arron Afflalo SG 31 Purrr Purr allows you to take the apply family approach to the tidyverse. Consider Exercise 2 from the Vectorization/Apply section. We’ll use the map function to map the sum function to each element in the list, the the same way we would with lapply. x = list(1:3, 4:6, 7:9) x %&gt;% map(sum) [[1]] [1] 6 [[2]] [1] 15 [[3]] [1] 24 Personal Opinion The dplyr grammar is clear for a lot of standard data processing tasks, and some not so common. Extremely useful for data exploration and visualization. No need to create/overwrite existing objects Can overwrite columns as they are created Makes it easy to look at anything, and do otherwise tedious data checks Drawbacks: Not as fast as data.table or even some base R approaches for many things4 The mindset can make for unnecessary complication e.g. There is no need to pipe to create one new variable Some approaches are not very intuitive Poor ability to work with some common data structures (e.g. matrices) dplyr Exercises Exercise 0 Install and load the dplyr ggplot2movies packages. Look at the help file for the movies data set, which contains data from IMDB. install.packages(&#39;ggplot2movies&#39;) library(ggplot2movies) Exercise 1 Using the movies data set, perform each of the following actions separately. Exercise 1a Use mutate to create a centered version of the rating variable. A centered variable is one whose mean has been subtracted from it. The process will take the following form: data %&gt;% mutate(newvar = &#39;?&#39;) # A tibble: 58,788 x 25 title year length budget rating votes r1 r2 r3 r4 r5 r6 r7 r8 r9 r10 mpaa Action Animation Comedy Drama Documentary Romance Short ratingCen &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 $ 1971 121 NA 6.4 348 4.5 4.5 4.5 4.5 14.5 24.5 24.5 14.5 4.5 4.5 0 0 1 1 0 0 0 0.46714976 2 $1000 a Touchdown 1939 71 NA 6.0 20 0.0 14.5 4.5 24.5 14.5 14.5 14.5 4.5 4.5 14.5 0 0 1 0 0 0 0 0.06714976 3 $21 a Day Once a Month 1941 7 NA 8.2 5 0.0 0.0 0.0 0.0 0.0 24.5 0.0 44.5 24.5 24.5 0 1 0 0 0 0 1 2.26714976 4 $40,000 1996 70 NA 8.2 6 14.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 34.5 45.5 0 0 1 0 0 0 0 2.26714976 5 $50,000 Climax Show, The 1975 71 NA 3.4 17 24.5 4.5 0.0 14.5 14.5 4.5 0.0 0.0 0.0 24.5 0 0 0 0 0 0 0 -2.53285024 6 $pent 2000 91 NA 4.3 45 4.5 4.5 4.5 14.5 14.5 14.5 4.5 4.5 14.5 14.5 0 0 0 1 0 0 0 -1.63285024 7 $windle 2002 93 NA 5.3 200 4.5 0.0 4.5 4.5 24.5 24.5 14.5 4.5 4.5 14.5 R 1 0 0 1 0 0 0 -0.63285024 8 &#39;15&#39; 2002 25 NA 6.7 24 4.5 4.5 4.5 4.5 4.5 14.5 14.5 14.5 4.5 14.5 0 0 0 0 1 0 1 0.76714976 9 &#39;38 1987 97 NA 6.6 18 4.5 4.5 4.5 0.0 0.0 0.0 34.5 14.5 4.5 24.5 0 0 0 1 0 0 0 0.66714976 10 &#39;49-&#39;17 1917 61 NA 6.0 51 4.5 0.0 4.5 4.5 4.5 44.5 14.5 4.5 4.5 4.5 0 0 0 0 0 0 0 0.06714976 # ... with 58,778 more rows Exercise 1b Use filter to create a new data frame that has only movies from the years 2000 and beyond. Use the greater than or equal operator &gt;=. # A tibble: 10,789 x 24 title year length budget rating votes r1 r2 r3 r4 r5 r6 r7 r8 r9 r10 mpaa Action Animation Comedy Drama Documentary Romance Short &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 $pent 2000 91 NA 4.3 45 4.5 4.5 4.5 14.5 14.5 14.5 4.5 4.5 14.5 14.5 0 0 0 1 0 0 0 2 $windle 2002 93 NA 5.3 200 4.5 0.0 4.5 4.5 24.5 24.5 14.5 4.5 4.5 14.5 R 1 0 0 1 0 0 0 3 &#39;15&#39; 2002 25 NA 6.7 24 4.5 4.5 4.5 4.5 4.5 14.5 14.5 14.5 4.5 14.5 0 0 0 0 1 0 1 4 &#39;R Xmas 2001 83 NA 4.9 288 14.5 4.5 4.5 4.5 14.5 24.5 14.5 4.5 4.5 4.5 R 0 0 0 1 0 0 0 5 (A)Torzija 2002 13 NA 7.2 71 4.5 0.0 4.5 4.5 4.5 4.5 14.5 24.5 14.5 24.5 0 0 0 1 0 0 1 6 (Entre nous) 2002 82 NA 4.8 22 14.5 4.5 4.5 14.5 24.5 14.5 4.5 4.5 0.0 0.0 0 0 0 1 0 0 0 7 (Paris: XY) 2001 80 NA 1.4 6 14.5 0.0 14.5 0.0 14.5 14.5 14.5 14.5 0.0 0.0 0 0 0 1 0 0 0 8 (T)Raumschiff Surprise - Periode 1 2004 87 NA 5.6 1275 14.5 4.5 4.5 4.5 14.5 14.5 14.5 14.5 4.5 14.5 0 0 1 0 0 0 0 9 *Corpus Callosum 2002 92 NA 4.9 36 24.5 14.5 4.5 4.5 0.0 4.5 14.5 4.5 4.5 14.5 0 0 0 1 0 0 0 10 ...Lost 2000 5 NA 6.2 13 4.5 0.0 0.0 4.5 24.5 34.5 24.5 4.5 0.0 0.0 0 0 0 0 0 0 1 # ... with 10,779 more rows Exercise 1c Use select to create a new data frame that only has the title, year, budget, length, rating and votes variables. There are at least 3 ways to do this. # A tibble: 58,788 x 6 title year budget length rating votes &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; 1 $ 1971 NA 121 6.4 348 2 $1000 a Touchdown 1939 NA 71 6.0 20 3 $21 a Day Once a Month 1941 NA 7 8.2 5 4 $40,000 1996 NA 70 8.2 6 5 $50,000 Climax Show, The 1975 NA 71 3.4 17 6 $pent 2000 NA 91 4.3 45 7 $windle 2002 NA 93 5.3 200 8 &#39;15&#39; 2002 NA 25 6.7 24 9 &#39;38 1987 NA 97 6.6 18 10 &#39;49-&#39;17 1917 NA 61 6.0 51 # ... with 58,778 more rows # A tibble: 58,788 x 6 title year length budget rating votes &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; 1 $ 1971 121 NA 6.4 348 2 $1000 a Touchdown 1939 71 NA 6.0 20 3 $21 a Day Once a Month 1941 7 NA 8.2 5 4 $40,000 1996 70 NA 8.2 6 5 $50,000 Climax Show, The 1975 71 NA 3.4 17 6 $pent 2000 91 NA 4.3 45 7 $windle 2002 93 NA 5.3 200 8 &#39;15&#39; 2002 25 NA 6.7 24 9 &#39;38 1987 97 NA 6.6 18 10 &#39;49-&#39;17 1917 61 NA 6.0 51 # ... with 58,778 more rows # A tibble: 58,788 x 6 title year length budget rating votes &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; 1 $ 1971 121 NA 6.4 348 2 $1000 a Touchdown 1939 71 NA 6.0 20 3 $21 a Day Once a Month 1941 7 NA 8.2 5 4 $40,000 1996 70 NA 8.2 6 5 $50,000 Climax Show, The 1975 71 NA 3.4 17 6 $pent 2000 91 NA 4.3 45 7 $windle 2002 93 NA 5.3 200 8 &#39;15&#39; 2002 25 NA 6.7 24 9 &#39;38 1987 97 NA 6.6 18 10 &#39;49-&#39;17 1917 61 NA 6.0 51 # ... with 58,778 more rows Exercise 2 Use group_by to group the data by year, and summarize to create a new variable that is the average budget. The summarize function works just like mutate in this case. Use the mean function to get the average, but you’ll also need to use the argument na.rm = TRUE within it because the earliest years have no budget recorded. # A tibble: 6 x 2 year AvgBudget &lt;int&gt; &lt;dbl&gt; 1 2000 23947684 2 2001 19235574 3 2002 19397139 4 2003 15868271 5 2004 13905717 6 2005 16468165 Exercise 3 Now put it all together in one set of piped operations. Filter movies released after 1990 select the same variables as before but also the mpaa, Action, and Drama variables group by mpaa and Action or Drama get the average rating # A tibble: 10 x 3 # Groups: mpaa [?] mpaa Drama AvgRating &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 0 5.940811 2 1 6.204495 3 NC-17 0 4.275000 4 NC-17 1 4.620000 5 PG 0 5.193590 6 PG 1 6.154913 7 PG-13 0 5.435729 8 PG-13 1 6.135092 9 R 0 4.855603 10 R 1 5.942602 You may be thinking- ‘its 2017 and why on earth would anyone do that?!’. Peruse most sports websites and you’ll see that fundamental web design basics escape them. See also, financial sites.↩ As Hadley Wickham is from New Zealand, and his examples use summarise, you’ll probably see it about as much as you do the other spelling.↩ There is multidplyr, but it doesn’t appear to have been updated since well before dplyr itself underwent major changes.↩ "],
["08_datatable.html", "data table", " data table data.table works in a notably different way than dplyr. However, you’d use it for the same reasons. Like dplyr, the data objects are both data.frames and a package specific class. Faster subset, grouping, update, ordered joins etc. In general, data.table works with brackets as in base R. However, the brackets work like a function call! Several key arguments x[i, j, by, keyby, with = TRUE, ...] Importantly: you don’t use the brackets as you would with data.frames. library(data.table) dt = data.table(x=sample(1:10, 6), g=1:3, y=runif(6)) class(dt) [1] &quot;data.table&quot; &quot;data.frame&quot; What i and j can be are fairly complex. In general, you use i for filtering by rows. dt[2] dt[2,] x g y 1: 4 2 0.0586233 x g y 1: 4 2 0.0586233 In general, you use j to select (by name!) or create new columns. Define a new variable with := dt[,x] dt[,z := x+y] # dt now has a new column dt[,z] dt[g&gt;1, mean(z), by=g] dt [1] 2 4 3 10 6 8 [1] 2.814280 4.058623 3.387284 10.160015 6.821022 8.266908 g V1 1: 2 5.439822 2: 3 5.827096 x g y z 1: 2 1 0.8142801 2.814280 2: 4 2 0.0586233 4.058623 3: 3 3 0.3872842 3.387284 4: 10 1 0.1600149 10.160015 5: 6 2 0.8210217 6.821022 6: 8 3 0.2669085 8.266908 Dropping columns is awkward. because j is an argument dt[,-y] # creates negative values of y dt[,-&#39;y&#39;, with=F] # drops y, but now needs quotes ## dt[,y:=NULL] # drops y, but this is just a base R approach ## dt$y = NULL [1] -0.8142801 -0.0586233 -0.3872842 -0.1600149 -0.8210217 -0.2669085 x g z 1: 2 1 2.814280 2: 4 2 4.058623 3: 3 3 3.387284 4: 10 1 10.160015 5: 6 2 6.821022 6: 8 3 8.266908 Grouped operations We can now attempt a ‘group-by’ operation, along with creation of a new variable. Note that these operations actually modify dt in place, a key distinction with dplyr. dt1 = dt2 = dt dt[,sum(x,y), by=g] # sum of all x and y values g V1 1: 1 12.97430 2: 2 10.87964 3: 3 11.65419 dt1[,newvar := sum(x), by=g] # add new variable to the original data We can also create groupings on the fly. For a new summary data set, we’ll take the following approach. dt2[, list(meanx = mean(x), sumx = sum(x)), by=g==1] g meanx sumx 1: TRUE 6.00 12 2: FALSE 5.25 21 Faster! joins: and easy to do (note that i can be a data.table) dt1[dt2] group operations: via setkey reading files: fread character matches: e.g. via chmatch Timings The following demonstrates some timings from here. Reproduced on my own machine based on 50 million observations Grouped operations are just a sum and length on a vector. By the way, never, ever use aggregate. For anything. fun elapsed 1: aggregate 114.35 2: by 24.51 3: sapply 11.62 4: tapply 11.33 5: dplyr 10.97 6: lapply 10.65 7: data.table 2.71 Ever. Really. Pipe with data.table Can be done but awkward at best. mydt[,newvar:=mean(x),][,newvar2:=sum(newvar), by=group][,-&#39;y&#39;, with=FALSE] mydt[,newvar:=mean(x), ][,newvar2:=sum(newvar), by=group ][,-&#39;y&#39;, with=FALSE ] Probably better to just use a pipe and dot approach. mydt[,newvar:=mean(x),] %&gt;% .[,newvar2:=sum(newvar), by=group] %&gt;% .[,-&#39;y&#39;, with=FALSE] My take Faster methods are great to have. Especially for group-by and joins. Drawbacks: Complex The syntax can be awkward It doesn’t work like a data.frame Piping with brackets Does not have its own ‘verse’ Compromise If speed and/or memory is (potentially) a concern, data.table For interactive exploration, dplyr Piping allows one to use both, so no need to choose. And on the horizon… dtplyr Coming soon to an R near you. This implements the data table back-end for ‘dplyr’ so that you can seamlessly use data table and ‘dplyr’ together. Or play with now. The following shows times for a grouped operation of a data frame of two variables, a random uniform draw of 5e7 values, and a grouping variable of 500k groups. package timing dplyr 10.97 data.table 2.71 dtplyr 2.7 Just for giggles I did the same in python with a pandas DataFrame and groupby operation, and it took 7+ seconds. data.table Exercises Exercise 0 Install and load the data.table package. Create the following data table. mydt = data.table(expand.grid(x=1:3, y=c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;)), z=sample(1:20, 9)) Exercise 1 Create a new object that contains only the ‘a’ group. Think back to how you use a logical to select rows. Exercise 2 Create a new object that is the sum of z grouped by x. You don’t need to name the sum variable. "],
["08b_Misc.html", "Miscellaneous", " Miscellaneous (DEVELOPMENT IN PROGRESS) Brief descriptions of other packages and tools that might be useful in data processing. MOAR TIDY dplyr functions: There are over a hundred functions that perform very common tasks. You really need to be aware of them as their use will come up often. broom: Convert statistical analysis objects from R into tidy data frames, so that they can more easily be combined, reshaped and otherwise processed with tools like dplyr, tidyr and ggplot2. tidy*: a lot of packages out there that are now ‘tidy’, though not a part of the official tidyverse. Some examples of the ones I’ve used: tidycensus tidybayes tidytext modelr Seriously, there’s a lot. Documents pander DT "],
["09_thinkingvis.html", "Part II: Visualization Thinking Visually", " Part II: Visualization Thinking Visually Information A starting point regarding data visualization regards the information you want to display and then how you want to display it. As in statistical modeling, parsimony is the goal, but not at the cost of the more compelling story. We don’t want to waste the time of the audience or be redundant, but we also want to avoid unnecessary clutter, chart junk, and the like. We’ll start with a couple examples. Consider the following. So what’s wrong with this? Plenty. Aside from being boring, the entire story can be said with a couple words- males are taller than females (even in the Star Wars universe). There is no reason to have a visualization. And if a simple group difference is the most exciting thing you have to say, not many are going to be interested. Minor issues include unnecessary border around the bars, unnecessary vertical gridlines, and an unnecessary X axis label. Even worse. Now the axis has been changed to distort the difference. Furthermore, color is used but the colors are chosen poorly, and add zero information, thus making the legend superfluous. And finally, the above doesn’t even convey the information people think it does, assuming they are even standard error bars5, which one typically has to guess about in many journal visualizations of this kind. Now we add more information, but more problems! The above has unnecessary border, gridlines, and emphasis. The labels, while possibly interesting, do not relate anything useful to the graph, and many are illegible. It imposes a straight (and too wide of a) line on a nonlinear relationship. And finally, color choice is both terrible and tends to draw one’s eye to the female data points. Here is what it looks like to someone with the most common form of colorblindness. If the points were less clumpy on gender, it would be very difficult to distinguish the groups. And here is what it might look like when printed. Now consider the following. We have six pieces of information in one graph- name (on hover), homeworld (shape), age (size), gender (color), mass (x), and height (y). The colors are evenly spaced from one another, and so do not draw one’s attention to one group over another, or even to the line over groups. Opacity allows the line to be added and points overlap without loss of information. We technically don’t need a caption, legend or gridlines, because hovering over the data tells us everything we’d want to know about a given data point. Whether this is something you’d prefer or not, the point is that we get quite a bit of information without being overwhelming, and the data is allowed to express itself cleanly. Here are some things to keep in mind when creating visualizations for scientific communication. Your audience isn’t dumb Assume your audience, which in academia is full of people with advanced degrees or those aspiring to obtain one, can handle more than a bar graph. If the visualization is good and well-explained6, they’ll be fine. See the data visualization and maps sections of 2016: The Year in Visual Stories and Graphics at the New York Times. Good data visualization of even complex relationships can be appreciated by more than an academic audience. Assume you can at least provide visualizations on that level of complexity and be okay. Clarity is key Sometimes the clearest message is a complicated one. That’s okay. Make sure your visualization tells the story you think is important, but don’t dumb it down via visualization. People will likely remember the graphic before they’ll remember the table of numbers. By the same token, don’t needlessly complicate something that is straightforward. Perhaps a scatter plot with some groupwise coloring is enough. That’s fine. All of this is easier said than done, and there is no right way to do data visualizations. Prepare to experiment. Avoid clutter Gridlines, 3d, unnecessary patterning, and chartjunk in general will only detract from the message. As an example, gridlines might even seem necessary, but even faint ones can hinder the pattern recognition you hope will take place, potentially imposing clumps of data that do not exist. In addition, they practically insist on a level of data precision that you simply don’t have. What’s more, with interactivity they literally convey nothing additional, as a simple hover over or click on a data point will reveal the precise values. Use sparingly, if at all. Color isn’t optional No modern outlet should be a print-first outfit, and if they are, you shouldn’t care to send your work there. The only thing you should be concerned with is how it will look online, because that’s how people will interact with your work first and foremost. That means that color is essentially a requirement for any visualization, so use it well in yours. Think interactively I would suggest you start by making the visualization you want to make, with interactivity and anything else you like. You can then reduce as necessary for publication, and keep the fancy one as supplemental, or accessible on your own website. Color Until recently, the default color schemes of most visualization packages were poor at best. Thankfully, ggplot2, its imitators and extenders, in both the R world and beyond, have made it much easier to have a decent color scheme by default7. However, the defaults are still potentially problematic, so you should be prepared to go with something else. In other cases, you may just simply prefer something else. For example, for me, the gray background of ggplot2 defaults is something I have to remove for every plot8. Viridis A couple packages will help you get started in choosing a decent color scheme. One is viridis. As stated in the package description: These color maps are designed in such a way that they will analytically be perfectly perceptually-uniform, both in regular form and also when converted to black-and-white. They are also designed to be perceived by readers with the most common form of color blindness. So basically you have something that will take care of your audience without having to do much. There are four palettes. These color schemes might seem a bit odd from what you’re used to. But recall that the point is scientific communication, and these will allow you to convey information accurately, without implicit bias, and be acceptable in different formats. In addition, there is ggplot2 functionality to boot, e.g. scale_color_viridis, and it will work for discrete or continuous valued data. For more, see the vignette. I also invite you to watch the introduction of the original module in Python, where you can learn more about the issues in color selection, and why viridis works. RColorBrewer Color Brewer offers another collection of palettes that will generally work well in a variety of situations. While there are print and color-blind friendly palettes, not all adhere to those restrictions. Specifically though, you have palettes for the following data situations: Qualitative (e.g. Dark29) Sequential (e.g. Reds) Diverging (e.g. RdBu) There is a ggplot2 function, scale_color_brewer, you can use as well. For more, see colorbrewer.org. There you can play around with the palettes to help make your decision. Contrast Thankfully, websites have mostly gotten past the phase where there text looks like this. The goal of scientific communication is to, well, communicate. Making text hard to read is pretty much antithetical to this. So contrast comes into play with text as well as color. In general, you should consider a 7 to 1 contrast ratio for text, minimally 4 to 1. -Here is text at 2 to 1 -Here is text at 4 to 1 -Here is text at 7 to 1 -Here is black I personally don’t like stark black, and find it visually irritating, but obviously that would be fine to use for most people. Scaling by Size You might not be aware, but there is more than one way to scale objects, e.g. in a scatterplot. Consider the following, where in both cases dots are scaled by the person’s body-mass index (BMI). The first plot scales the dots by their area, while the second scales the radius, but otherwise they are identical. It’s not generally recommended to scale the radius, as our perceptual system is more attuned to the area. Packages like ggplot2 and plotly will automatically do this, but some might not, so you should check. Transparency Using transparency is a great way to keep detailed information available to the audience without being overwhelming. Consider the following. One-hundred individual trajectories are shown, but it doesn’t cause any issue graphically. Without transparency, it just looks ugly and notably busier if nothing else. In addition, transparency can be used to add additional information, e.g. data density in scatter plots. Things to avoid pie charts and their cousins bar chart (and stacked versions) wind rose histograms use density using 3d without adding any communicative value which is almost always prefer interactivity Where is it useful: showing structure (e.g. molecular, geographical), continuous multi-way interactions using too many colors WebAxe Thinking Visually Exercises Exercise 1 The following uses the diamonds data set that comes with ggplot2. library(ggplot2); library(viridis) ggplot(aes(x=carat, y=price), data=diamonds) + geom_point(aes(color=price)) + ???? Exercise 2 Now color it by the cut. To use the scale_color_viridis approach, you’ll have to change one of the arguments to the function. Reproduce this but using one of the other viridis palettes. Thinking exercises For your upcoming presentation, who is your audience? Error bars for group means can overlap and still be statistically different (the test regards the difference in means). Furthermore most visuals of this sort don’t bother to say whether it is standard deviation, standard error, or 2*standard error, or even something else.↩ People seem to think there are text limits for captions. There are none.↩ Even Matlab finally joined the club, except that they still screwed up with their default coloring scheme.↩ Hadley states “The grey background gives the plot a similar colour (in a typographical sense) to the remainder of the text, ensuring that the graphics fit in with the flow of a text without jumping out with a bright white background. Finally, the grey background creates a continuous field of colour which ensures that the plot is perceived as a single visual entity.”. The part about it being ugly is apparently left out. ☺ Also, my opinion is that it has the opposite effect, making the visualization jump out because nothing on the web is gray by default. If anything the page background is white, and having a white/transparent background would perhaps be better, but honestly, don’t you want a visualization to jump out?↩ Don’t even think about asking what the Dark1 palette is.↩ "],
["10_ggplot2.html", "ggplot2", " ggplot2 ggplot2 is an extremely popular package for visualization in R. and copied in other languages/programs It entails a grammar of graphics. Every graph is built from the same few parts Key ideas: Aesthetics Layers (and geoms) Piping Facets Themes Extensions Strengths: Ease of getting a good looking plot Easy customization A lot of data processing is done for you Clear syntax Easy multidimensional approach Equally spaced colors as a default Aesthetics Aesthetics map data to aesthetic aspects of the plot. Size Color etc. The function used in ggplot to do this is aes Layers In general, we start with a base layer and add to it. In most cases you’ll start as follows. This would just produce a plot background. Piping Layers are added via piping. The first layers added are typically geoms: points lines density text ggplot2 was using pipes before it was cool, and so it has a different pipe (+). Otherwise, the concept is the same as before. Our base is provided via the ggplot2 functions and specifies the data along with x and y aesthetics. The geom_point function adds a layer of points, and now we would have a scatterplot. Alternatively, you could have specified the x and y aesthetic at the geom_point layer If you’re going to have the same x, y, color, etc. aesthetics regardless of layer, put it in the base. Otherwise, doing it by layer gives you more flexibility Examples In the following, one setting is not mapped to the data. Stats There are many statistical functions built in. Key strength: you don’t have to do much preprocessing. Quantile regression lines: Loess (or additive model) smooth: Bootstrapped confidence intervals: Facets Facets allow for paneled display, a very common operation. In general, we often want comparison plots. facet_grid will produce a grid. Often this is all that’s needed facet_wrap is more flexible. Both use a formula approach to specify the grouping. facet_grid facet_wrap Fine control ggplot2 makes it easy to get good looking graphs quickly. However the amount of fine control is extensive. The following plot is hideous (aside from the background, which is totaly rad), but illustrates the point. ggplot(aes(x=carat, y=price), data=diamonds) + annotation_custom(rasterGrob(lambosun, width=unit(1,&quot;npc&quot;), height=unit(1,&quot;npc&quot;), interpolate = FALSE), -Inf, Inf, -Inf, Inf) + geom_point(aes(color=clarity), alpha=.5) + scale_y_log10(breaks=c(1000,5000,10000)) + xlim(0, 10) + scale_color_brewer(type=&#39;div&#39;) + facet_wrap(~cut, ncol=3) + theme_minimal() + theme(axis.ticks.x=element_line(color=&#39;darkred&#39;), axis.text.x=element_text(angle=-45), axis.text.y=element_text(size=20), strip.text=element_text(color=&#39;forestgreen&#39;), strip.background=element_blank(), panel.grid.minor=element_line(color=&#39;lightblue&#39;), legend.key=element_rect(linetype=4), legend.position=&#39;bottom&#39;) Themes In the last example you saw two uses of a theme. built-in specific customization Each argument takes on a specific value or an element function: element_rect element_line element_text element_blank The base theme is not too good. not for web doesn’t look good for print either You will almost invariably need to tweak it. Extensions ggplot2 now has its own extension system, and there is even a website to track the extensions. http://www.ggplot2-exts.org/ Examples include: additional themes interactivity animations marginal plots network graphs Summary ggplot2 ggplot2 is an easy to use, but powerful visualization tool. Allows one to think in many dimensions for any graph: x y color size opacity facet 2d graphs are only useful for conveying the simplest of ideas. Use ggplot2 to easily create more interesting visualizations. ggplot2 Exercises Exercise 0 Install and load the ggplot2 package. Exercise 1 Create two plots, one a scatterplot (e.g. with geom_point) and one with lines (e.g. geom_line) with a data set of your choosing (all of the following are base R or available after loading ggplot2. Some suggestions: faithful: Waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. msleep: mammals sleep dataset with sleep times and weights etc. diamonds: used in the slides economics: US economic time series. txhousing: Housing sales in TX. midwest: Midwest demographics. mpg: Fuel economy data from 1999 and 2008 for 38 popular models of car Recall the basic form for ggplot. ggplot(aes(x=*, y=*, other), data=*) + geom_*() + otherLayers, theme etc. Themes to play with: theme_bw theme_classic theme_dark theme_gray theme_light theme_linedraw theme_minimal Exercise 2 Play around and change the arguments to the following. You’ll need to install the maps package. For example, do points for all county midpoints, with different colors. "],
["11_interactive.html", "Interactive Visualization", " Interactive Visualization Packages ggplot2 is the most widely used package for visualization in R. However, it is not interactive by default. Many packages use htmlwidgets, d3 (JavaScript library) etc. to provide interactive graphics. General: plotly used also in Python, Matlab, Julia, can convert ggplot2 images to interactive ones. highcharter also very general wrapper for highcharts.js and works with some R packages out of the box rbokeh like plotly, it also has cross program support Specific functionality: DT interactive data tables leaflet maps with OpenStreetMap visNetwork Network visualization Piping for Visualization One of the advantages to piping is that it’s not limited to dplyr style data management functions. Any R function can be potentially piped to. several examples have already been shown. This facilitates data exploration, especially visually. don’t have to create objects new variables are easily created and subsequently manipulated just for vis data manipulation not separated from visualization htmlwidgets Many newer visualization packages take advantage of piping. htmlwidgets is a package that makes it easy to create javascript visualizations. i.e. what you see everywhere on the web. The packages using it typically are pipe-oriented and produce interactive plots. Plotly A couple demonstrations with plotly. Note the layering as with ggplot2. Piping used before plotting. plotly has modes, which allow for points, lines, text and combinations. Traces work similar to geoms. library(mgcv); library(modelr) mtcars %&gt;% mutate(amFactor = factor(am, labels=c(&#39;auto&#39;, &#39;manual&#39;)), hovertext = paste(&#39;weight:&#39;,wt, &#39;&lt;br&gt;&#39;, &#39;mgp:&#39;, mpg, &#39;&lt;br&gt;&#39;, amFactor)) %&gt;% add_predictions(gam(mpg~s(wt, am, bs=&#39;fs&#39;), data=mtcars)) %&gt;% arrange(wt) %&gt;% plot_ly() %&gt;% add_markers(x=~wt, y=~mpg, color=~amFactor, text=~hovertext, hoverinfo=&#39;text&#39;) %&gt;% add_lines(x=~wt, y=~pred, color=~amFactor, alpha=.5, name=&#39;gam prediction&#39;, showlegend=F) ggplotly The nice thing about plotly is that we can feed a ggplot to it. It would have been easy to use geom_smooth to get a similar result, so let’s do so. Highcharter Highcharter is also fairly useful for a wide variety of plots. Uses the highcharts.js library visNetwork visNetwork allows for network visualizations Uses the vis.js library library(visNetwork) visNetwork(nodes, edges) %&gt;% #, height=600, width=800 visNodes(shape=&#39;circle&#39;, font=list(), scaling=list(min=10, max=50, label=list(enable=T))) %&gt;% visLegend() data table Use the DT package for interactive data frames. Shiny Shiny is a framework that can essentially allow you to build an interactive website. Provided by RStudio developers Most of the more recently developed visualization packages will work specifically within the shiny and rmarkdown settings. Interactive and Visual Data Exploration Interactivity allows for even more dimensions to be brought to a graphic. Interactive graphics are more fun too! But they must serve a purpose Too often they are simply distraction, and detract from the data story Just a couple visualization packages can go a very long way. Interactive Visualization Exercises Exercise 0 Install and load the plotly package. Load the dplyr and ggplot2 packages if necessary. Exercise 1 Using dplyr group by year, and summarize to create a new variable that is the Average rating. Then create a plot with plotly for a line or scatter plot (for the latter, use the add_markers function). It will take the following form: data %&gt;% group_by() %&gt;% summarize() %&gt;% plot_ly() %&gt;% add_markers() Exercise 2 This time group by year and Drama. In the summarize create average rating again, but also a variable representing the average number of votes. In your plotly line, use the size and color arguments to represent whether the average number of votes and whether it was drama or not respectively. Use add_markers. Exercise 3 Create a ggplot of your design and then use ggplotly to make it interactive. "],
["999_summary.html", "Summary", " Summary With the right tools, data exploration can be: easier faster more efficient more fun! Use them to wring your data dry of what it has to offer. Recommended next steps: R for Data Science Advanced R Embrace a richer understanding of your data! "]
]
