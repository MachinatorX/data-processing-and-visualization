[
["index.html", "Data Processing &amp; Visualization", " Data Processing &amp; Visualization A Workshop in Two Parts Michael Clark https://m-clark.github.io 2017-10-21 "],
["01_intro.html", "Intro Outline", " Intro This document is for a workshop the goal of which is to provide some tools, tips, packages etc. that make data processing and visualization in R easier. It is oriented toward those who have had some exposure to R in an applied fashion, but would also be useful to someone coming to R from another programming language. It is not an introduction to R, nor does this document have anything to do with statistical analysis. The goal here is primarily to instill awareness, specifically of tools that will make your data exploration easier, and to understand some of the why behind the tools, so that one can better implement them. Outline Part 1: Data Processing Understanding Base R Approaches to Data Processing Overview of Data Structures Input/Output Vectorization and Apply functions Getting Acquainted with Newer Approaches to Data Processing Pipes, and how to use them tidyverse data.table Part 2: Visualization ggplot2 Visualization with ggplot2 Adding Interactivity Color coding in text: emphasis package function object/class link Some key packages used in the following demonstrations and exercises: dplyr, tidyr, ggplot2, data.table, plotly, ggplot2movies Many others are also used, feel free to install as we come across them. Here are a few. DT, highcharter, magrittr, maps, mgcv, plotly, quantmod, readr, visNetwork "],
["02_dataStructures.html", "Part 1: Data Processing Data Structures", " Part 1: Data Processing Data Structures R has several core data structures: Vectors Factors Lists Matrices/arrays Data frames Vectors Vectors form the basis of R data structures. Two main types- atomic and lists, but we’ll talk about lists separately. Here is an R vector. The elements of the vector are numeric values. x = c(1, 3, 2, 5, 4) x [1] 1 3 2 5 4 All elements of an atomic vector are the same type. Examples include: character numeric (double) integer logical Factors A important type of vector is a factor. Factors are used to represent categorical data structures. x = factor(1:3, labels=c(&#39;q&#39;, &#39;V&#39;, &#39;what the heck?&#39;)) x [1] q V what the heck? Levels: q V what the heck? The underlying representation is numeric, but it is important to remember that factors are categorical. They can’t be used as numbers would be, as the following demonstrates. as.numeric(x) [1] 1 2 3 sum(x) Error in Summary.factor(structure(1:3, .Label = c(&quot;q&quot;, &quot;V&quot;, &quot;what the heck?&quot;: &#39;sum&#39; not meaningful for factors Matrices With multiple dimensions, we are dealing with arrays. Matrices are 2-d arrays, and extremely commonly used for scientific computing. The vectors making up a matrix must all be of the same type. e.g. all values in a matrix must be numeric. Creating a matrix Creating a matrix can be done in a variety of ways. # create vectors x = 1:4 y = 5:8 z = 9:12 rbind(x, y, z) # row bind [,1] [,2] [,3] [,4] x 1 2 3 4 y 5 6 7 8 z 9 10 11 12 cbind(x, y, z) # column bind x y z [1,] 1 5 9 [2,] 2 6 10 [3,] 3 7 11 [4,] 4 8 12 matrix(c(x, y, z), nrow=3, ncol=4, byrow=TRUE) [,1] [,2] [,3] [,4] [1,] 1 2 3 4 [2,] 5 6 7 8 [3,] 9 10 11 12 Lists Lists in R are highly flexible objects, and probably the most commonly used for data science. They can contain anything as their elements, even other lists. unlike vectors, whose elements must be of the same type. Here is a list. We use the list function to create it. x = list(1, &quot;apple&quot;, list(3, &quot;cat&quot;)) x [[1]] [1] 1 [[2]] [1] &quot;apple&quot; [[3]] [[3]][[1]] [1] 3 [[3]][[2]] [1] &quot;cat&quot; We often want to loop some function over a list. for(elem in x) print(class(elem)) [1] &quot;numeric&quot; [1] &quot;character&quot; [1] &quot;list&quot; Lists can, and often do, have named elements. x = list(&quot;a&quot; = 25, &quot;b&quot; = -1, &quot;c&quot; = 0) x[&quot;b&quot;] $b [1] -1 Data Frames data.frames are a very commonly used data structure. They do not have to have the same type of element, and this is because the data.frame class is actually just a list. As such, everything about lists applies to them But they can also be indexed by row or column as well. like matrices There are other very common types of object classes associated with packages that are both a data.frame and other type of structure. Creating a data frame mydf = data.frame(a = c(1,5,2), b = c(3,8,1)) We can add row names also. rownames(mydf) = paste0(&#39;row&#39;, 1:3) mydf a b row1 1 3 row2 5 8 row3 2 1 Data Structure Exercises Excercise #1 Create an object that is a matrix and/or a data.frame, and inspect its class or structure (use the class or str functions). Exercise #2 Create a list of 3 elements, the first of which contains character strings, the second numbers, and the third, the data.frame or matrix you just created. Thinking Exercises How is a factor different from a character vector? How is a data.frame the same as and different from a matrix? How is a data.frame the same as and different from a list? "],
["03_io.html", "Input/Output", " Input/Output Standard methods of reading in tabular data include the following functions: read.table read.csv readLines Base R also comes with the foreign package for reading in other types of files, especially other statistical packages. However, while you may see it still in use, it’s not as useful as what’s found in other packages. Newer approaches haven: Package to read in foreign statistical files read_spss read_dta readxl: for excel files rio: uses haven, readxl etc. but with just two functions for everything import, export (also convert) Faster approaches readr: Faster versions of base R functions read_csv read_delim These make assumptions after an initial scan of the data, but if you don’t have ‘big’ data, this won’t help much. However, they actually can be used as a diagnostic. pick up potential data entry errors. data.table provides a faster version read.table, and is typically faster than readr approaches. fread Other Data Note that R can handle many types of data. Some examples: JSON SQL XML YAML MongoDB NETCDF text (e.g. a novel) shapefiles (e.g. for geographic data) google spreadsheets And many, many others. On the horizon feather: designed to make reading/writing data frames efficient Works in both Python and R. Still in early stages of development. Big Data You may come across the situation where your data cannot be held in memory. One of the first things to be aware of for data processing is that you may not need to have the data all in memory at once. Before shifting to a hardward solution, consider if the following is possible. Chunking: reading and processing the data in chunks Line at a time: dealing with individual lines of data Other data formats: for example SQL databases (sqldf package, src_dbi in dplyr) However, it may be that the end result is still to large. In that case you’ll have to consider a cluster-based or distributed data situation. Of course R will have tools for that. DBI sparklyr RHadoop And more. I/O Exercises Exercise 1 Use readr and haven to read the following files. Use the url just like you would any filename. The latter is a Stata file. You can use the RStudio’s menu approach to import the file if you want. ‘https://raw.githubusercontent.com/m-clark/data-processing-and-visualization/master/data/cars.csv’ ‘https://raw.githubusercontent.com/m-clark/data-processing-and-visualization/master/data/presvote.dta’ Thinking Exercises Why might you use read_csv from the readr package rather than read.csv in base R? What is your definition of ‘big’ data? "],
["04_indexing.html", "Indexing", " Indexing Base R Indexing Refresher Slicing vectors letters[4:6] [1] &quot;d&quot; &quot;e&quot; &quot;f&quot; letters[c(13,10,3)] [1] &quot;m&quot; &quot;j&quot; &quot;c&quot; Slicing matrices/data.frames myMatrix[1, 2:3] Label-based indexing mydf[&#39;row1&#39;, &#39;b&#39;] Position-based indexing mydf[1, 2] Mixed indexing mydf[&#39;row1&#39;, 2] If the row/column value is empty, all rows/columns are retained. mydf[&#39;row1&#39;,] mydf[,&#39;b&#39;] Non-contiguous mydf[c(1,3),] Boolean mydf[mydf$a &gt;= 2,] List/Data.frame extraction [ : grab a slice of elements/columns [[ : grab specific elements/columns $ : grab specific elements/columns @: extract slot for S4 objects my_list_or_df[2:4] my_list_or_df[[&#39;name&#39;]] my_list_or_df$name Indexing Exercises This following is a refresher of base R indexing only. Here is a matrix, a data.frame and a list. mymatrix = matrix(rnorm(100), 10, 10) mydf = cars mylist = list(mymatrix, thisdf = mydf) Exercise 1 For the matrix, in separate operations, take a slice of rows, a selection of columns, and a single element. Exercise 2 For the data.frame, grab a column in 3 different ways. Exercise 3 For the list grab an element by number and by name. "],
["05_vectorapp.html", "Vectorization &amp; Apply", " Vectorization &amp; Apply Boolean Indexing Logicals are objects with values of TRUE or FALSE. Assume x is a vector of numbers. x = c(-1, 2, 10, -5) idx = x &gt; 2 idx [1] FALSE FALSE TRUE FALSE x[idx] [1] 10 Flexiblity We don’t have to create an explicit index object first, as R indexing is ridiculously flexible. x[x &gt; 2] x[x != 3] x[ifelse(x &gt; 2 &amp; x !=10, T, F)] x[{y = idx; y}] x[resid(lm(y ~ x)) &gt; 0] Vectorized operations Consider the following unfortunately coded loop: for (i in 1:nrow(mydf)) { check = mydf$x[i] &gt; 2 if (check==TRUE) { mydf$y[i] = &#39;Yes&#39; } else { mydf$y[i] = &#39;No&#39; } } Compare: mydf$y = &#39;No&#39; mydf$y[mydf$x &gt; 2] = &#39;Yes&#39; This gets us the same thing, and would be much faster. Boolean indexing is an example of a vectorized operation. The whole vector is considered: Rather than each element individually Any preprocessing is done once rather than n times This is always faster. Example: Log all values in a matrix. mymatrix_log = log(mymatrix) Way faster than looping over elements, rows or columns. Here we’ll let the apply function stand in for our loop, logging the elements of each column. mymatrix = matrix(runif(100), 10, 10) identical(apply(mymatrix, 2, log), log(mymatrix)) [1] TRUE library(microbenchmark) microbenchmark(apply(mymatrix, 2, log), log(mymatrix)) Unit: microseconds expr min lq mean median uq max neval apply(mymatrix, 2, log) 45.928 47.430 48.98774 47.880 48.3300 150.694 100 log(mymatrix) 3.002 3.002 3.31125 3.302 3.4525 4.503 100 Many vectorized functions already exist in R. They are often written in C, Fortran etc., and so even faster. Apply functions A family of functions allows for a succinct way of looping. Common ones include: apply arrays, matrices, data.frames lapply, sapply, vapply lists, data.frames, vectors tapply grouped operations (table apply) mapply multivariate version of sapply replicate similar to sapply Example Standardizing variables. for (i in 1:ncol(mydf)){ x = mydf[,i] for (j in 1:length(x)){ x[j] = (x[j] - mean(x))/sd(x) } } The above would be a really bad way to use R. The following is much cleaner and now you’d have a function you can use elsewhere. apply will take a matrix, and apply a function over the margin, row or column, you want. stdize &lt;- function(x) { (x-mean(x))/sd(x) } apply(mydf, 2, stdize) # 1 for rows, 2 for columnwise application Timings The previous demonstrates how to use apply. However, there is a scale function in base R that uses a more vectorized approach under the hood. The following demonstrates various approaches to standardizing the columns of the matrix, even using a parallelized approach. The base R function requires very little code and beats the others. mymat = matrix(rnorm(100000), ncol=1000) stdize &lt;- function(x) { (x-mean(x)) / sd(x) } doubleloop = function() { for (i in 1:ncol(mymat)) { x = mymat[, i] for (j in 1:length(x)) { x[j] = (x[j] - mean(x)) / sd(x) } } } singleloop = function() { for (i in 1:ncol(mymat)) { x = mymat[, i] x = (x - mean(x)) / sd(x) } } library(parallel) cl = makeCluster(8) clusterExport(cl, c(&#39;stdize&#39;, &#39;mymat&#39;)) doParallel::registerDoParallel(cl) test = microbenchmark::microbenchmark(doubleloop=doubleloop(), singleloop=singleloop(), apply=apply(mymat, 2, stdize), parApply=parApply(cl, mymat, 2, stdize), vectorized=scale(mymat), times=25) stopCluster(cl) test Unit: milliseconds expr min lq mean median uq max neval doubleloop 2882.148089 2904.739076 2938.21281 2946.57065 2952.25532 3013.29805 25 singleloop 28.989806 29.512434 33.08177 31.17037 32.42606 84.79185 25 apply 32.167901 33.350945 33.96146 34.13204 34.64055 36.18983 25 parApply 17.691611 18.213638 21.91323 19.63893 21.12156 73.60683 25 vectorized 8.085578 8.491433 10.25583 10.66990 11.02532 12.62173 25 Apply functions Benefits Cleaner/simpler code Environment kept clear of unnecessary objects Potentially more reproducible more likely to use generalizable functions Might be faster Parallelizable NOT necessarily faster than explicit loops. single loop over columns was as fast as apply Replicate and mapply are especially slow ALWAYS can potentially be faster than loops. Parallelization: parApply, parLapply etc. Personal experience I use R every day, and rarely use explicit loops. Note: no speed difference for a for loop vs. using while If you must use an explicit loop, create an empty object of the dimension/form you need, and then fill it in. Notably faster I pretty much never use an explicit double loop, as a little more thinking about the problem will usually provide a more efficient path to solving the problem. Apply functions Apply functions and similar approaches should be a part of your regular R experience. Other versions we’ll talk about have been optimized, but you need to know the basics in order to use those. Any you still may need parallel versions. Vector/Apply Exercises Exercise 1 With the following matrix, use apply and the sum function to get row or column sums. x = matrix(1:9, 3, 3) Exercise 2 With the following list, use lapply and sapply and the sum function to get sums for the elements. There is no margin to specify with on a list, so just supply the list and the sum function. x = list(1:3, 4:6, 7:9) sapply is actually just a wrapper for lapply. If you supply the argument simplified=F, it is identical. Otherwise, it attempts to return a vector or matrix. "],
["06_pipes.html", "Pipes", " Pipes Note: More detail on much of the following is given in another workshop. Pipes are operators that send what comes before the pipe to what comes after. There are many different pipes, and some packages that use their own. However, the vast majority of packages use the same pipe: %&gt;% Here, we’ll focus on their use with the dplyr package. Later, we’ll use it for visualizations. Example: mydf %&gt;% select(var1, var2) %&gt;% filter(var1 == &#39;Yes&#39;) %&gt;% summary Start with a data.frame %&gt;% select columns from it %&gt;% filter/subset it %&gt;% get a summary Using variables as they are created One nice thing about pipelines is that we can use variables as soon as they are created, with out having to break out separate objects/steps. mydf %&gt;% mutate(newvar1 = var1 + var2, newvar2 = newvar1/var3) %&gt;% summarise(newvar2avg = mean(newvar2)) Pipes for Visualization (more later) The following provides a means to think about pipes for visualization. It’s just a generic example for now, but we’ll see more later. basegraph %&gt;% points %&gt;% lines %&gt;% layout The dot Most functions are not ‘pipe-aware’ by default. Example: pipe to a modeling function. mydf %&gt;% lm(y ~ x) # error Other pipes could work in this situation. e.g. %$% in magrittr. But generally, when you come upon this situation, you can use a dot. The dot refers to the object before the previous pipe. mydf %&gt;% lm(y ~ x, data=.) # . == mydf Flexibility Piping is not just for data.frames. The following starts with a character vector. Sends it to a recursive function (named ..). .. is created on-the-fly, and has a single argument (.). After the function is created, it’s used on ., whcih represents the string before the pipe. Result: pipes between the words. c(&#39;Ceci&#39;, &quot;n&#39;est&quot;, &#39;pas&#39;, &#39;une&#39;, &#39;pipe!&#39;) %&gt;% { .. &lt;- . %&gt;% if (length(.) == 1) . else paste(.[1], &#39;%&gt;%&#39;, ..(.[-1])) ..(.) } [1] &quot;Ceci %&gt;% n&#39;est %&gt;% pas %&gt;% une %&gt;% pipe!&quot; Put that in your pipe and smoke it René Magritte! Summary Pipes are best used interactively. Extremely useful for data exploration. Common in many visualization packages. See the magrittr package for more pipes. "],
["07_tidyverse.html", "Tidyverse", " Tidyverse What is the tidyverse? The tidyverse consists of a few key packages- ggplot2: data visualization dplyr: data manipulation tidyr: data tidying readr: data import purrr: functional programming, e.g. alternate approaches to apply tibble: tibbles, a modern re-imagining of data frames And of course the tidyverse package which will load all of the above. See also: lubridate, rvest, stringr and others in the ‘hadleyverse’. What is tidy? Tidy data refers to data arranged in a way that makes data processing, analysis, and visualization simpler. In a tidy data set: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Think long before wide. dplyr dplyr provides a grammar of data manipulation (like ggplot2 does for visualization). It is the next iteration of plyr, but there is no longer any need for plyr really. It’s focused on tools for working with data frames. Over 100 functions It has three main goals: Make the most important data manipulation tasks easier. Do them faster. Use the same interface to work with data frames, data tables or database. Some key operations: select: grab columns select helpers: one_of, starts_with, num_range etc. filter/slice: grab rows group_by: grouped operations mutate/transmute: create new variables summarize: summarize/aggregate do: arbitrary operations Various join/merge functions: inner_join, left_join etc. Little things like: n, n_distinct, nth, n_groups, count, recode, between No need to quote variable names. An example Let’s say we want to select from our data the following variables: Start with the ID variable The variables X1:X10, which are not all together, and there are many more X columns The variables var1 and var2, which are the only var variables in the data Any variable that starts with XYZ How might we go about this? Some base R approaches Tedious, or multiple objects just to get the columns you want. # numeric indexes; not conducive to readibility or reproducibility newData = oldData[,c(1,2,3,4, etc.)] # explicitly by name; fine if only a handful; not pretty newData = oldData[,c(&#39;ID&#39;,&#39;X1&#39;, &#39;X2&#39;, etc.)] # two step with grep; regex difficult to read/understand cols = c(&#39;ID&#39;, paste0(&#39;X&#39;, 1:10), &#39;var1&#39;, &#39;var2&#39;, grep(colnames(oldData), &#39;^XYZ&#39;, value=T)) newData = oldData[,cols] # or via subset newData = subset(oldData, select = cols) More What if you also want observations where Z is Yes, Q is No, and only the observations with the top 50 values of var2, ordered by var1 (descending)? # three operations and overwriting or creating new objects if we want clarity newData = newData[oldData$Z == &#39;Yes&#39; &amp; oldData$Q == &#39;No&#39;,] newData = newData[order(newData$var2, decreasing=T)[1:50],] newData = newData[order(newData$var1, decreasing=T),] And this is for fairly straightforward operations. The dplyr way newData = oldData %&gt;% filter(Z == &#39;Yes&#39;, Q == &#39;No&#39;) %&gt;% select(num_range(&#39;X&#39;, 1:10), contains(&#39;var&#39;), starts_with(&#39;XYZ&#39;)) %&gt;% top_n(n=50, var2) %&gt;% arrange(desc(var1)) An alternative dplyr and piping is an alternative you can do all this sort of stuff with base R with, within, subset, transform, etc. Even though the initial base R approach depicted is fairly concise, it still can potentially be: noisier less legible less amenable to additional data changes requires esoteric knowledge (e.g. regular expressions) often requires creation of new objects (even if we just want to explore) often slower, possibly greatly tidyr Two primary functions for manipulating data gather: wide to long spread: long to wide Other useful functions include: unite: paste together multiple columns into one separate: complement of unite Example library(tidyr) stocks &lt;- data.frame( time = as.Date(&#39;2009-01-01&#39;) + 0:9, X = rnorm(10, 0, 1), Y = rnorm(10, 0, 2), Z = rnorm(10, 0, 4) ) stocks %&gt;% head time X Y Z 1 2009-01-01 -1.00003515 1.6001824 5.06918845 2 2009-01-02 1.71214620 1.5388574 0.07021022 3 2009-01-03 -0.57923533 3.3789660 7.89330904 4 2009-01-04 1.13448631 0.1137021 2.68071180 5 2009-01-05 -1.22065077 1.2024585 3.56703167 6 2009-01-06 -0.07526508 -0.5296092 0.59128843 stocks %&gt;% gather(stock, price, -time) %&gt;% head time stock price 1 2009-01-01 X -1.00003515 2 2009-01-02 X 1.71214620 3 2009-01-03 X -0.57923533 4 2009-01-04 X 1.13448631 5 2009-01-05 X -1.22065077 6 2009-01-06 X -0.07526508 Note that the latter is an example of tidy data while the former is not. Personal Opinion The dplyr grammar is clear for a lot of standard data processing tasks, and some not so common. Extremely useful for data exploration and visualization. No need to create/overwrite existing objects Can overwrite columns as they are created Makes it easy to look at anything, and do otherwise tedious data checks Drawbacks: Not as fast as data.table or even some base R approaches for many things The mindset can make for unnecessary complication e.g. There is no need to pipe to create one new variable On the horizon multidplyr Partitions the data across a cluster. Faster than data.table (after partitioning) dplyr Exercises Exercise 0 Install and load the dplyr ggplot2movies packages. Look at the help file for the movies data set, which contains data from IMDB. install.packages(&#39;ggplot2movies&#39;) library(ggplot2movies) Exercise 1 Using the movies data set, perform each of the following actions separately. Exercise 1a Use mutate to create a centered version of the rating variable. A centered variable is one whose mean has been subtracted from it. The process will take the following form: data %&gt;% mutate(newvar = &#39;?&#39;) Exercise 1b Use filter to create a new data frame that has only movies from the years 2000 and beyond. Use the greater than or equal operator &gt;=. Exercise 1c Use select to create a new data frame that only has the title, year, budget, length, rating and votes variables. There are at least 3 ways to do this. Exercise 2 Use group_by to group the data by year, and summarize to create a new variable that is the average budget. The summarize function works just like mutate in this case. Use the mean function to get the average, but you’ll also need to use the argument na.rm = TRUE within it because the earliest years have no budget recorded. Exercise 3 Now put it all together in one set of piped operations. Filter movies released after 1990 select the same variables as before but also the mpaa, Action, and Drama variables group by mpaa and Action or Drama get the average rating "],
["08_datatable.html", "data table", " data table data.table works in a notably different way than dplyr. However, you’d use it for the same reasons. Like dplyr, the data objects are both data.frames and a package specific class. Faster subset, grouping, update, ordered joins etc. In general, data.table works with brackets as in base R. However, the brackets work like a function call! Several key arguments x[i, j, by, keyby, with = TRUE, ...] Importantly: you don’t use the brackets as you would with data.frames. library(data.table) dt = data.table(x=sample(1:10, 6), g=1:3, y=runif(6)) class(dt) [1] &quot;data.table&quot; &quot;data.frame&quot; What i and j can be are fairly complex. In general, you use i for filtering by rows. dt[2] dt[2,] x g y 1: 4 2 0.1195698 x g y 1: 4 2 0.1195698 In general, you use j to select (by name!) or create new columns. Define a new variable with := dt[,x] dt[,z := x+y] # dt now has a new column dt[,z] dt[g&gt;1, mean(z), by=g] dt [1] 10 4 6 9 2 3 [1] 10.372553 4.119570 6.332344 9.751102 2.109955 3.948890 g V1 1: 2 3.114762 2: 3 5.140617 x g y z 1: 10 1 0.3725530 10.372553 2: 4 2 0.1195698 4.119570 3: 6 3 0.3323439 6.332344 4: 9 1 0.7511015 9.751102 5: 2 2 0.1099548 2.109955 6: 3 3 0.9488896 3.948890 Dropping columns is awkward. because j is an argument dt[,-y] # creates negative values of y dt[,-&#39;y&#39;, with=F] # drops y, but now needs quotes ## dt[,y:=NULL] # drops y, but this is just a base R approach ## dt$y = NULL [1] -0.3725530 -0.1195698 -0.3323439 -0.7511015 -0.1099548 -0.9488896 x g z 1: 10 1 10.372553 2: 4 2 4.119570 3: 6 3 6.332344 4: 9 1 9.751102 5: 2 2 2.109955 6: 3 3 3.948890 Grouped operations We can now attempt a ‘group-by’ operation, along with creation of a new variable. Note that these operations actually modify dt in place, a key distinction with dplyr. dt1 = dt2 = dt dt[,sum(x,y), by=g] # sum of all x and y values g V1 1: 1 20.123655 2: 2 6.229525 3: 3 10.281233 dt1[,newvar := sum(x), by=g] # add new variable to the original data We can also create groupings on the fly. For a new summary data set, we’ll take the following approach. dt2[, list(meanx = mean(x), sumx = sum(x)), by=g==1] g meanx sumx 1: TRUE 9.50 19 2: FALSE 3.75 15 Faster! joins: and easy to do (note that i can be a data.table) dt1[dt2] group operations: via setkey reading files: fread character matches: e.g. via chmatch Timings The following demonstrates some timings from here. Reproduced on my own machine based on 50 million observations Grouped operations are just a sum and length on a vector. By the way, never, ever use aggregate. For anything. fun elapsed 1: aggregate 114.35 2: by 24.51 3: sapply 11.62 4: tapply 11.33 5: dplyr 10.97 6: lapply 10.65 7: data.table 2.71 Ever. Really. Pipe with data.table Can be done but awkward at best. mydt[,newvar:=mean(x),][,newvar2:=sum(newvar), by=group][,-&#39;y&#39;, with=FALSE] mydt[,newvar:=mean(x), ][,newvar2:=sum(newvar), by=group ][,-&#39;y&#39;, with=FALSE ] Probably better to just use a pipe and dot approach. mydt[,newvar:=mean(x),] %&gt;% .[,newvar2:=sum(newvar), by=group] %&gt;% .[,-&#39;y&#39;, with=FALSE] My take Faster methods are great to have. Especially for group-by and joins. Drawbacks: Complex The syntax can be awkward It doesn’t work like a data.frame Piping with brackets Does not have its own ‘verse’ Compromise If speed and/or memory is (potentially) a concern, data.table For interactive exploration, dplyr Piping allows one to use both, so no need to choose. And on the horizon… dtplyr Coming soon to an R near you. This implements the data table back-end for ‘dplyr’ so that you can seamlessly use data table and ‘dplyr’ together. Or play with now. The following shows times for a grouped operation of a data frame of two variables, a random uniform draw of 5e7 values, and a grouping variable of 500k groups. package timing dplyr 10.97 data.table 2.71 dtplyr 2.7 Just for giggles I did the same in python with a pandas DataFrame and groupby operation, and it took 7+ seconds. data.table Exercises Exercise 0 Install and load the data.table package. Create the following data table. mydt = data.table(expand.grid(x=1:3, y=c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;)), z=sample(1:20, 9)) Exercise 1 Create a new object that contains only the ‘a’ group. Think back to how you use a logical to select rows. Exercise 2 Create a new object that is the sum of z grouped by x. You don’t need to name the sum variable. "],
["09_thinkingvis.html", "Part II: Visualization Thinking Visually", " Part II: Visualization Thinking Visually Information A starting point regarding data visualization regards the information you want to display and then how you want to display it. As in statistical modeling, parsimony is the goal, but not at the cost of the more compelling story. We don’t want to waste the time of the audience or be redundant, but we also want to avoid unnecessary clutter, chart junk, and the like. We’ll start with a couple examples. Consider the following. So what’s wrong with this? Plenty. Aside from being boring, the entire story can be said with a couple words- males are taller than females (even in the Star Wars universe). There is no reason to have a visualization. And if a simple group difference is the most exciting thing you have to say, not many are going to be interested. Minor issues include unnecessary border around the bars, unnecessary vertical gridlines, and an unnecessary X axis label. Even worse. Now the axis has been changed to distort the difference. Furthermore, color is used but the colors are chosen poorly, add zero information. And finally, the above doesn’t even convey the information people think it does, assuming they are even standard error bars1, which one typically has to guess about in many journal visualizations of this kind. Now we add more information, but more problems! The above has unnecessary border, gridlines, and emphasis. The labels, while possibly interesting, do not relate anything useful to the graph, and many are illegible. It imposes a straight (and too wide of a) line on a nonlinear relationship. And finally, color choice is both terrible and tends to draw one’s eye to the female data points. Here is what it looks like to someone with the most common form of colorblindness. If the points were less clumpy on gender, it would be very difficult to distinguish the groups. And here is what it might look like when printed. Now consider the following. We have six pieces of information in one graph- name (on hover), homeworld (shape), age (size), gender (color), mass (x), and height (y). The colors are evenly spaced from one another, and so do not draw one’s attention to one group over another. Opacity allows the line to be added and points overlap without loss of information. We technically don’t need a caption, legend or gridlines, because hovering over the data tells us everything we’d want to know about a given data point. Whether this is something you’d prefer or not, the point is that we get quite a bit of information without being overwhelming, and the data is allowed to express itself cleanly. Here are some things to keep in mind when creating visualizations for scientific communication. Your audience isn’t dumb Assume your audience, which in academia is full of people with advanced degrees or those aspiring to obtain one, can handle more than a bar graph. If the visualization is good and well-explained2, they’ll be fine. See the data visualization and maps sections of 2016: The Year in Visual Stories and Graphics at the New York Times. Good data visualization can be appreciated by more than an academic audience. Assume you can at least provide visualizations on that level of complexity and be fine with your audience. Clarity is key Sometimes the clearest message is a complicated one. That’s okay. Make sure your visualization tells the story you think is important, but don’t dumb it down via visualization. People will likely remember the graphic before they’ll remember the table of numbers. By the same token, don’t needlessly complicate something that is straightforward. Perhaps a scatter plot with some groupwise coloring is enough. That’s fine. All of this is easier said than done, and there is no right way to do data visualizations. Prepare to experiment. Avoid clutter Gridlines, 3d, unnecessary patterning, and chartjunk in general will only detract from the message. As an example, gridlines might even seem necessary, but even faint ones can hinder the pattern recognition you hope will take place, potentially imposing clumps of data that do not exist. In addition, they practically insist on a level of data precision that you simply don’t have. What’s more, with interactivity they literally convey nothing additional, as a simple hover over or click on a data point will reveal the precise values. Use sparingly, if at all. Color isn’t optional No modern journal should be a print-first outfit, and if they are, you shouldn’t care to send your work there. The only thing you should be concerned with is how it will look online, because that’s how people will interact with your work first and foremost. That means that color is essentially a requirement for any visualization, so use it well in yours. Think interactively I would suggest you start by making the visualization you want to make, with interactivity and anything else you like. You can then reduce as necessary for publication, and keep the fancy one as supplemental, or accessible on your own website. Color Until recently, the default color schemes of most visualization packages were poor at best. Thankfully, ggplot2, its imitators and extenders in both the R world and beyond have made it much easier to have a decent color scheme by default. Finally, the grey background creates a continuous field of colour which ensures that the plot is perceived as a single visual entity. Viridis RColorBrewer Contrast WebAxe Error bars for group means can overlap and still be statistically different (the test regards the difference in means). Furthermore most visuals of this sort don’t bother to say whether it is standard deviation, standard error, or 2*standard error, or even something else.↩ People seem to think there are text limits for captions. There are none.↩ "],
["10_ggplot2.html", "ggplot2", " ggplot2 ggplot2 is an extremely popular package for visualization in R. and copied in other languages/programs It entails a grammar of graphics. Every graph is built from the same few parts Key ideas: Aesthetics Layers (and geoms) Piping Facets Themes Extensions Strengths: Ease of getting a good looking plot Easy customization A lot of data processing is done for you Clear syntax Easy multidimensional approach Equally spaced colors as a default Aesthetics Aesthetics map data to aesthetic aspects of the plot. Size Color etc. The function used in ggplot to do this is aes aes(x=myvar, y=myvar2, color=myvar3, group=g) Layers In general, we start with a base layer and add to it. In most cases you’ll start as follows. ggplot(aes(x=myvar, y=myvar2), data=mydata) This would just produce a plot background. Piping Layers are added via piping. The first layers added are typically geoms: points lines density text ggplot2 was using pipes before it was cool, and so it has a different pipe (+). Otherwise, the concept is the same as before. ggplot(aes(x=myvar, y=myvar2), data=mydata) + geom_point() Our base is provided via the ggplot2 functions and specifies the data along with x and y aesthetics. The geom_point function adds a layer of points, and now we would have a scatterplot. Alternatively, you could have specified the x and y aesthetic at the geom_point layer If you’re going to have the same x, y, color, etc. aesthetics regardless of layer, put it in the base. Otherwise, doing it by layer gives you more flexibility Examples library(ggplot2) data(&quot;diamonds&quot;); data(&#39;economics&#39;) ggplot(aes(x=carat, y=price), data=diamonds) + geom_point() ggplot(aes(x=date, y=unemploy), data=economics) + geom_line() In the following, one setting is not mapped to the data. ggplot(aes(x=carat, y=price), data=diamonds) + geom_point(aes(size=carat, color=clarity), alpha=.25) Stats There are many statistical functions built in. Key strength: you don’t have to do much preprocessing. Quantile regression lines: ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_quantile() Loess (or additive model) smooth: ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth() Bootstrapped confidence intervals: ggplot(mtcars, aes(cyl, mpg)) + geom_point() + stat_summary(fun.data = &quot;mean_cl_boot&quot;, colour = &quot;orange&quot;, alpha=.75, size = 1) Facets Facets allow for paneled display, a very common operation. In general, we often want comparison plots. facet_grid will produce a grid. Often this is all that’s needed facet_wrap is more flexible. Both use a formula approach to specify the grouping. facet_grid ggplot(mtcars, aes(wt, mpg)) + geom_point() + facet_grid(vs ~ cyl, labeller = label_both) facet_wrap ggplot(mtcars, aes(wt, mpg)) + geom_point() + facet_wrap(vs ~ cyl, labeller = label_both, ncol=2) Fine control ggplot2 makes it easy to get good looking graphs quickly. However the amount of fine control is extensive. The following plot is hideous (aside from the background, which is totaly rad), but illustrates the point. ggplot(aes(x=carat, y=price), data=diamonds) + annotation_custom(rasterGrob(lambosun, width=unit(1,&quot;npc&quot;), height=unit(1,&quot;npc&quot;), interpolate = FALSE), -Inf, Inf, -Inf, Inf) + geom_point(aes(color=clarity), alpha=.5) + scale_y_log10(breaks=c(1000,5000,10000)) + xlim(0, 10) + scale_color_brewer(type=&#39;div&#39;) + facet_wrap(~cut, ncol=3) + theme_minimal() + theme(axis.ticks.x=element_line(color=&#39;darkred&#39;), axis.text.x=element_text(angle=-45), axis.text.y=element_text(size=20), strip.text=element_text(color=&#39;forestgreen&#39;), strip.background=element_blank(), panel.grid.minor=element_line(color=&#39;lightblue&#39;), legend.key=element_rect(linetype=4), legend.position=&#39;bottom&#39;) Themes In the last example you saw two uses of a theme. built-in specific customization Each argument takes on a specific value or an element function: element_rect element_line element_text element_blank The base theme is not too good. not for web doesn’t look good for print either You will almost invariably need to tweak it. Extensions ggplot2 now has its own extension system, and there is even a website to track the extensions. http://www.ggplot2-exts.org/ Examples include: additional themes interactivity animations marginal plots network graphs Summary ggplot2 ggplot2 is an easy to use, but powerful visualization tool. Allows one to think in many dimensions for any graph: x y color size opacity facet 2d graphs are only useful for conveying the simplest of ideas. Use ggplot2 to easily create more interesting visualizations. ggplot2 Exercises Exercise 0 Install and load the ggplot2 package. Exercise 1 Create two plots, one a scatterplot (e.g. with geom_point) and one with lines (e.g. geom_line) with a data set of your choosing (all of the following are base R or available after loading ggplot2. Some suggestions: faithful: Waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. msleep: mammals sleep dataset with sleep times and weights etc. diamonds: used in the slides economics: US economic time series. txhousing: Housing sales in TX. midwest: Midwest demographics. mpg: Fuel economy data from 1999 and 2008 for 38 popular models of car Recall the basic form for ggplot. ggplot(aes(x=*, y=*, other), data=*) + geom_*() + otherLayers, theme etc. Themes to play with: theme_bw theme_classic theme_dark theme_gray theme_light theme_linedraw theme_minimal Exercise 2 Play around and change the arguments to the following. You’ll need to install the maps package. For example, do points for all county midpoints, with different colors. library(maps) mi = map_data(&quot;county&quot;, &quot;michigan&quot;) seats = mi %&gt;% group_by(subregion) %&gt;% summarise_at(vars(lat, long), function(x) median(range(x))) ggplot(mi, aes(long, lat)) + geom_polygon(aes(group = group), fill = NA, colour = &quot;grey60&quot;) + geom_text(aes(label = subregion), data = seats, size = 1, angle = 45) + geom_point(y=42.281389, x=-83.748333, color=&#39;#1e90ff&#39;, size=3) + theme_minimal() + theme(panel.grid=element_blank()) "],
["11_interactive.html", "Interactive Visualization", " Interactive Visualization Packages ggplot2 is the most widely used package for visualization in R. However, it is not interactive by default. Many packages use htmlwidgets, d3 (JavaScript library) etc. to provide interactive graphics. General: plotly used also in Python, Matlab, Julia, can convert ggplot2 images to interactive ones. highcharter also very general wrapper for highcharts.js and works with some R packages out of the box rbokeh like plotly, it also has cross program support Specific functionality: DT interactive data tables leaflet maps with OpenStreetMap visNetwork Network visualization Piping for Visualization One of the advantages to piping is that it’s not limited to dplyr style data management functions. Any R function can be potentially piped to. several examples have already been shown. This facilitates data exploration, especially visually. don’t have to create objects new variables are easily created and subsequently manipulated just for vis data manipulation not separated from visualization htmlwidgets Many newer visualization packages take advantage of piping. htmlwidgets is a package that makes it easy to create javascript visualizations. i.e. what you see everywhere on the web. The packages using it typically are pipe-oriented and produce interactive plots. plotly example A couple demonstrations with plotly. Note the layering as with ggplot2. Piping used before plotting. library(plotly) midwest %&gt;% filter(inmetro==T) %&gt;% plot_ly(x=~percbelowpoverty, y=~percollege, mode=&#39;markers&#39;) plotly has modes, which allow for points, lines, text and combinations. Traces work similar to geoms. library(mgcv); library(modelr) mtcars %&gt;% mutate(amFactor = factor(am, labels=c(&#39;auto&#39;, &#39;manual&#39;)), hovertext = paste(&#39;weight:&#39;,wt, &#39;&lt;br&gt;&#39;, &#39;mgp:&#39;, mpg, &#39;&lt;br&gt;&#39;, amFactor)) %&gt;% add_predictions(gam(mpg~s(wt, am, bs=&#39;fs&#39;), data=mtcars)) %&gt;% arrange(wt) %&gt;% plot_ly() %&gt;% add_markers(x=~wt, y=~mpg, color=~amFactor, text=~hovertext, hoverinfo=&#39;text&#39;) %&gt;% add_lines(x=~wt, y=~pred, color=~amFactor, alpha=.5, name=&#39;gam prediction&#39;, showlegend=F) ggplotly The nice thing about plotly is that we can feed a ggplot to it. It would have been easy to use geom_smooth to get a similar result, so let’s do so. gp = mtcars %&gt;% mutate(amFactor = factor(am, labels=c(&#39;auto&#39;, &#39;manual&#39;)), hovertext = paste(wt, mpg, amFactor)) %&gt;% arrange(wt) %&gt;% ggplot(aes(x=wt, y=mpg, color=amFactor)) + geom_smooth(se=F) + geom_point(aes(color=amFactor)) ggplotly() Highcharter Highcharter is also fairly useful for a wide variety of plots. Uses the highcharts.js library library(highcharter); library(quantmod) x = getSymbols(&quot;GOOG&quot;, auto.assign = FALSE) hchart(x, width=1000) visNetwork visNetwork allows for network visualizations Uses the vis.js library library(visNetwork) visNetwork(nodes, edges) %&gt;% #, height=600, width=800 visNodes(shape=&#39;circle&#39;, font=list(), scaling=list(min=10, max=50, label=list(enable=T))) %&gt;% visLegend() data table Use the DT package for interactive data frames. library(DT) ggplot2movies::movies %&gt;% select(1:6) %&gt;% filter(rating&gt;8, !is.na(budget), votes &gt; 1000) %&gt;% datatable() Shiny Shiny is a framework that can essentially allow you to build an interactive website. Provided by RStudio developers Most of the more recently developed visualization packages will work specifically within the shiny and rmarkdown settings. Interactive and Visual Data Exploration Interactivity allows for even more dimensions to be brought to a graphic. Interactive graphics are more fun too! But they must serve a purpose Too often they are simply distraction, and detract from the data story Just a couple visualization packages can go a very long way. Interactive Visualization Exercises Exercise 0 Install and load the plotly package. Load the dplyr and ggplot2 packages if necessary. Exercise 1 Using dplyr group by year, and summarize to create a new variable that is the Average rating. Then create a plot with plotly for a line or scatter plot (for the latter, use the add_markers function). It will take the following form: data %&gt;% group_by() %&gt;% summarize() %&gt;% plot_ly() %&gt;% add_markers() Exercise 2 This time group by year and Drama. In the summarize create average rating again, but also a variable representing the average number of votes. In your plotly line, use the size and color arguments to represent whether the average number of votes and whether it was drama or not respectively. Use add_markers. Exercise 3 Create a ggplot of your design and then use ggplotly to make it interactive. "],
["999_summary.html", "Summary", " Summary With the right tools, data exploration can be: easier faster more efficient more fun! Use them to wring your data dry of what it has to offer. Recommended next steps: R for Data Science Advanced R Embrace a richer understanding of your data! "]
]
