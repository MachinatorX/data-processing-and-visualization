{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document is a Python exploration of this R-based document: http://m-clark.github.io/data-processing-and-visualization/.  It is intended for those new to modeling and related concepts.  Code is *not* optimized for anything but learning.  In addition, all the content is located with the main document, not here, so many sections may not be included.  I only focus on reproducing the code chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical way to depict a linear regression model is as follows:\n",
    "\n",
    "\n",
    "$$y = b_0 + b_1\\cdot x_1 + b_2\\cdot x_2 + ... +  + b_p\\cdot x_p + \\epsilon$$\n",
    "\n",
    "In the above, $b_0$ is the intercept, and the other $b_*$ are the regression coefficients that represent the relationship of the predictors $x$ to the target variable $y$.  The $\\epsilon$ represents the *error* or *residual*.  We don't have perfect prediction, and that represents the difference between what we can guess with our predictor relationships to the target and what we actually observe with it.\n",
    "\n",
    "In Python, we specify a linear model as we would in R using [the formula api](https://www.statsmodels.org/devel/example_formulas.html).  There are various inputs, typically starting with the formula. In the formula, The target variable is first, followed by the predictor variables, separated by a tilde (`~`). Additional predictor variables are added with a plus sign (`+`).  In this example, `y` is our target, and the predictors are `x` and `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y ~ x'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'y ~ x'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally in Python, model fitting is done in two steps, where the model is first specified, then subsequently fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = smf.ols(formula='y ~ x', data = df)\n",
    "\n",
    "# res = mod.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add things like interactions or convert variable types within the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = smf.ols(formula='y ~ x + z + x:z', data = df)\n",
    "# mod = smf.ols(formula='y ~ x + C(z)', data = df)  # convert z to categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of many estimation approaches is the reduction of *loss*, conceptually defined as the difference between the model predictions and the observed data, i.e. prediction error. In an introductory methods course, many are introduced to *ordinary least squares* as a means to estimate the coefficients for a linear regression model.  In this scenario, we are seeking to come up with estimates of the coefficients that minimize the (squared) difference between the observed target value and the fitted value based on the parameter estimates.  The loss in this case is defined as the sum of the squared errors.  Formally we can state it as follows.\n",
    "\n",
    "$$\\mathcal{Loss} = \\Sigma(y - \\hat{y})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how this works more clearly with some simple conceptual code.  In what follows, we create a function, allows us to move row by row through the data, calculating both our prediction based on the given model parameters- $\\hat{y}$, and the difference between that and our target variable $y$.  We sum these squared differences to get a total. In practice such a function is called the loss function, cost function, or objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_loss(X, y, beta):\n",
    "    N = X.shape[0]\n",
    "    loss  = np.repeat(0., N)\n",
    "    y_hat = np.repeat(0., N)\n",
    "    \n",
    "    for n in range(N):\n",
    "        y_hat[n] = np.sum(X[n]*beta)\n",
    "        loss[n]  = (y[n] - y_hat[n])**2\n",
    "    \n",
    "    return(np.sum(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need some data. Let's construct some data so that we know the true underlying values for the regression coefficients. Feel free to change the sample size `N` or the coefficient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.099239</td>\n",
       "      <td>-1.085631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.520785</td>\n",
       "      <td>0.997345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.853754</td>\n",
       "      <td>0.282978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.845157</td>\n",
       "      <td>-1.506295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.686074</td>\n",
       "      <td>-0.578600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y         x\n",
       "0  5.099239 -1.085631\n",
       "1  3.520785  0.997345\n",
       "2  5.853754  0.282978\n",
       "3  6.845157 -1.506295\n",
       "4  4.686074 -0.578600"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)           # for reproducibility\n",
    "\n",
    "N = 100                       # Sample size\n",
    "X = np.c_[np.repeat(1, N), np.random.normal(size = N)]  # a model matrix; first column represents the intercept\n",
    "y = 5 * X[:, 0] + .5 * X[:, 1] + np.random.normal(size = N)  # a target with some noise; truth is y = 5 +.5*x\n",
    "\n",
    "df = pd.DataFrame(np.c_[y, X[:,1]], columns = ['y', 'x'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2595.055066494441"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_loss(X, y, beta = [0., 1.])    # guess 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1939.0482563455128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_loss(X, y, beta = [1., 2.])    # guess 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198.46475152607633"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_loss(X, y, beta = [4., .25])   # guess 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = smf.ols('y ~ x', df)          # fit the model and obtain parameter estimates using OLS\n",
    "model = f.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    4.980914\n",
       "x            0.483407\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params                      # best guess given the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.06535118757606"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.resid**2)            # least squares loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.06535118757606"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_loss(X, y, model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some relatively rare cases, a known approach is available and we do not have to search for the best estimates, but simply have to perform the correct steps that will result in them.  For example, the following matrix operations will produce the best estimates for linear regression, which also happen to be the maximum likelihood estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.98091425, 0.48340745])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)  # 'normal equations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many statistical modeling techniques use *maximum likelihood* in some form or fashion, including Bayesian approaches, so you would do well to understand the basics. In this case, instead of minimizing the loss, we use an approach to maximize the probability of the observations of the target variable given the estimates of the parameters of the model (e.g. the coefficients in a regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, some simple conceptual code can help us. The next bit of code follows a similar approach to what we had with least squares regression, but the goal is instead to maximize the likelihood of the observed data. In this example, I fix the estimated variance, but in practice we'd need to estimate that parameter as well.  As probabilities are typically very small, we work with them on the log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def max_like(X, y, beta, sigma = 1):\n",
    "    N = X.shape[0]\n",
    "    likelihood  = np.repeat(0., N)\n",
    "    y_hat = np.repeat(0., N)\n",
    "    \n",
    "    for n in range(N):\n",
    "        y_hat[n] = np.sum(X[n]*beta)\n",
    "        likelihood[n]  = norm.logpdf(y[n], y_hat[n], 1)\n",
    "    \n",
    "    return(np.sum(likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1389.4213865676875"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_like(X, y, beta = [0., 1.])    # guess 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1061.4179814932236"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_like(X, y, beta = [1., 2.])    # guess 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-191.12622908350542"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_like(X, y, beta = [4., .25])   # guess 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-138.8348323266888"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.llf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-138.9265289142553"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_like(X, y, model.params)  # difference as estimated sigma is not 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `statsmodels` the primary components required to fit a model are the model formula, and a data frame that contains the variables specified in that formula.  Consider the following models. In general the syntax is the similar regardless of package, with special considerations for the type of model. The data argument is not included in these examples, but would be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glm\n",
    "# model = smf.glm(formula, family = sm.families.Binomial())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poisson with offset\n",
    "# model = smf.glm(\n",
    "#     formula, \n",
    "#     family = sm.families.Poisson(), \n",
    "#     exposure = np.asarray(data['exposure_var']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed models\n",
    "# model = smf.mixedlm(formula, groups = data[\"group_var\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally get our hands dirty and run an example. We'll use the world happiness dataset. This is country level data based on surveys taken at various years, and the scores are averages or proportions, along with other values like GDP.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>life_ladder</th>\n",
       "      <th>log_gdp_per_capita</th>\n",
       "      <th>social_support</th>\n",
       "      <th>healthy_life_expectancy_at_birth</th>\n",
       "      <th>freedom_to_make_life_choices</th>\n",
       "      <th>generosity</th>\n",
       "      <th>perceptions_of_corruption</th>\n",
       "      <th>positive_affect</th>\n",
       "      <th>negative_affect</th>\n",
       "      <th>confidence_in_national_government</th>\n",
       "      <th>democratic_quality</th>\n",
       "      <th>delivery_quality</th>\n",
       "      <th>gini_index_world_bank_estimate</th>\n",
       "      <th>happiness_score</th>\n",
       "      <th>dystopia_residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1704.000000</td>\n",
       "      <td>1704.000000</td>\n",
       "      <td>1676.000000</td>\n",
       "      <td>1691.000000</td>\n",
       "      <td>1676.000000</td>\n",
       "      <td>1675.000000</td>\n",
       "      <td>1622.000000</td>\n",
       "      <td>1608.000000</td>\n",
       "      <td>1685.000000</td>\n",
       "      <td>1691.000000</td>\n",
       "      <td>1530.000000</td>\n",
       "      <td>1558.000000</td>\n",
       "      <td>1559.000000</td>\n",
       "      <td>643.000000</td>\n",
       "      <td>554.000000</td>\n",
       "      <td>554.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>2012.332160</td>\n",
       "      <td>5.437155</td>\n",
       "      <td>9.222456</td>\n",
       "      <td>0.810570</td>\n",
       "      <td>63.111971</td>\n",
       "      <td>0.733829</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.751315</td>\n",
       "      <td>0.709368</td>\n",
       "      <td>0.265679</td>\n",
       "      <td>0.481973</td>\n",
       "      <td>-0.136053</td>\n",
       "      <td>-0.001390</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>5.410409</td>\n",
       "      <td>2.059861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>3.688072</td>\n",
       "      <td>1.121149</td>\n",
       "      <td>1.185794</td>\n",
       "      <td>0.119210</td>\n",
       "      <td>7.583622</td>\n",
       "      <td>0.144115</td>\n",
       "      <td>0.163365</td>\n",
       "      <td>0.186074</td>\n",
       "      <td>0.107984</td>\n",
       "      <td>0.084707</td>\n",
       "      <td>0.192059</td>\n",
       "      <td>0.876074</td>\n",
       "      <td>0.975849</td>\n",
       "      <td>0.083232</td>\n",
       "      <td>1.130121</td>\n",
       "      <td>0.550848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>2005.000000</td>\n",
       "      <td>2.661718</td>\n",
       "      <td>6.457201</td>\n",
       "      <td>0.290184</td>\n",
       "      <td>32.299999</td>\n",
       "      <td>0.257534</td>\n",
       "      <td>-0.336385</td>\n",
       "      <td>0.035198</td>\n",
       "      <td>0.362498</td>\n",
       "      <td>0.083426</td>\n",
       "      <td>0.068769</td>\n",
       "      <td>-2.448228</td>\n",
       "      <td>-2.144974</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>2.693000</td>\n",
       "      <td>0.291651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>4.610970</td>\n",
       "      <td>8.304428</td>\n",
       "      <td>0.747512</td>\n",
       "      <td>58.299999</td>\n",
       "      <td>0.638436</td>\n",
       "      <td>-0.115534</td>\n",
       "      <td>0.696083</td>\n",
       "      <td>0.621855</td>\n",
       "      <td>0.205414</td>\n",
       "      <td>0.334735</td>\n",
       "      <td>-0.790461</td>\n",
       "      <td>-0.711416</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>4.513250</td>\n",
       "      <td>1.723459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>5.339557</td>\n",
       "      <td>9.406206</td>\n",
       "      <td>0.833098</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>0.752731</td>\n",
       "      <td>-0.022080</td>\n",
       "      <td>0.805775</td>\n",
       "      <td>0.718541</td>\n",
       "      <td>0.254544</td>\n",
       "      <td>0.464109</td>\n",
       "      <td>-0.227386</td>\n",
       "      <td>-0.218633</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>5.312500</td>\n",
       "      <td>2.064439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>6.273522</td>\n",
       "      <td>10.193060</td>\n",
       "      <td>0.904432</td>\n",
       "      <td>68.300003</td>\n",
       "      <td>0.848155</td>\n",
       "      <td>0.093522</td>\n",
       "      <td>0.876458</td>\n",
       "      <td>0.801530</td>\n",
       "      <td>0.314896</td>\n",
       "      <td>0.614862</td>\n",
       "      <td>0.650468</td>\n",
       "      <td>0.699971</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>6.323525</td>\n",
       "      <td>2.436582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>8.018934</td>\n",
       "      <td>11.770276</td>\n",
       "      <td>0.987343</td>\n",
       "      <td>76.800003</td>\n",
       "      <td>0.985178</td>\n",
       "      <td>0.677743</td>\n",
       "      <td>0.983276</td>\n",
       "      <td>0.943621</td>\n",
       "      <td>0.704590</td>\n",
       "      <td>0.993604</td>\n",
       "      <td>1.575009</td>\n",
       "      <td>2.184725</td>\n",
       "      <td>0.634000</td>\n",
       "      <td>7.632100</td>\n",
       "      <td>3.837715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              year  life_ladder  log_gdp_per_capita  social_support  \\\n",
       "count  1704.000000  1704.000000         1676.000000     1691.000000   \n",
       "mean   2012.332160     5.437155            9.222456        0.810570   \n",
       "std       3.688072     1.121149            1.185794        0.119210   \n",
       "min    2005.000000     2.661718            6.457201        0.290184   \n",
       "25%    2009.000000     4.610970            8.304428        0.747512   \n",
       "50%    2012.000000     5.339557            9.406206        0.833098   \n",
       "75%    2015.000000     6.273522           10.193060        0.904432   \n",
       "max    2018.000000     8.018934           11.770276        0.987343   \n",
       "\n",
       "       healthy_life_expectancy_at_birth  freedom_to_make_life_choices  \\\n",
       "count                       1676.000000                   1675.000000   \n",
       "mean                          63.111971                      0.733829   \n",
       "std                            7.583622                      0.144115   \n",
       "min                           32.299999                      0.257534   \n",
       "25%                           58.299999                      0.638436   \n",
       "50%                           65.000000                      0.752731   \n",
       "75%                           68.300003                      0.848155   \n",
       "max                           76.800003                      0.985178   \n",
       "\n",
       "        generosity  perceptions_of_corruption  positive_affect  \\\n",
       "count  1622.000000                1608.000000      1685.000000   \n",
       "mean      0.000079                   0.751315         0.709368   \n",
       "std       0.163365                   0.186074         0.107984   \n",
       "min      -0.336385                   0.035198         0.362498   \n",
       "25%      -0.115534                   0.696083         0.621855   \n",
       "50%      -0.022080                   0.805775         0.718541   \n",
       "75%       0.093522                   0.876458         0.801530   \n",
       "max       0.677743                   0.983276         0.943621   \n",
       "\n",
       "       negative_affect  confidence_in_national_government  democratic_quality  \\\n",
       "count      1691.000000                        1530.000000         1558.000000   \n",
       "mean          0.265679                           0.481973           -0.136053   \n",
       "std           0.084707                           0.192059            0.876074   \n",
       "min           0.083426                           0.068769           -2.448228   \n",
       "25%           0.205414                           0.334735           -0.790461   \n",
       "50%           0.254544                           0.464109           -0.227386   \n",
       "75%           0.314896                           0.614862            0.650468   \n",
       "max           0.704590                           0.993604            1.575009   \n",
       "\n",
       "       delivery_quality  gini_index_world_bank_estimate  happiness_score  \\\n",
       "count       1559.000000                      643.000000       554.000000   \n",
       "mean          -0.001390                        0.370000         5.410409   \n",
       "std            0.975849                        0.083232         1.130121   \n",
       "min           -2.144974                        0.240000         2.693000   \n",
       "25%           -0.711416                        0.305000         4.513250   \n",
       "50%           -0.218633                        0.352000         5.312500   \n",
       "75%            0.699971                        0.428000         6.323525   \n",
       "max            2.184725                        0.634000         7.632100   \n",
       "\n",
       "       dystopia_residual  \n",
       "count         554.000000  \n",
       "mean            2.059861  \n",
       "std             0.550848  \n",
       "min             0.291651  \n",
       "25%             1.723459  \n",
       "50%             2.064439  \n",
       "75%             2.436582  \n",
       "max             3.837715  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy = pd.read_csv('../data/world_hapiness.csv')\n",
    "\n",
    "happy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a model with statsmodels and a pandas data frame is pretty easy, and at a minimum requires the two key ingredients mentioned before, the formula and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_model_base = smf.ols(\n",
    "  'happiness_score ~ democratic_quality + generosity + log_gdp_per_capita',\n",
    "  data = happy\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike R, many if not most Python modeling approaches will expect matrix input rather than a formula + data frame, so you should feel comfortable with that approach. For example, we can use sm.ols as before, and as we did, we'd  need to create the required components.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<statsmodels.regression.linear_model.RegressionResultsWrapper at 0x1c1ac82438>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.OLS(y, X).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a model, we'll want to summarize the results of it. Most modeling packages have a summary method we can apply, which will provide parameter estimates, some notion of model fit, inferential statistics, and other output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>happiness_score</td> <th>  R-squared:         </th> <td>   0.695</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.693</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   309.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 24 Feb 2020</td> <th>  Prob (F-statistic):</th> <td>1.24e-104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:34:57</td>     <th>  Log-Likelihood:    </th> <td> -390.15</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   411</td>      <th>  AIC:               </th> <td>   788.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   407</td>      <th>  BIC:               </th> <td>   804.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "           <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>          <td>   -1.0105</td> <td>    0.314</td> <td>   -3.214</td> <td> 0.001</td> <td>   -1.628</td> <td>   -0.393</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>democratic_quality</th> <td>    0.1704</td> <td>    0.046</td> <td>    3.714</td> <td> 0.000</td> <td>    0.080</td> <td>    0.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>generosity</th>         <td>    1.1608</td> <td>    0.195</td> <td>    5.938</td> <td> 0.000</td> <td>    0.777</td> <td>    1.545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_gdp_per_capita</th> <td>    0.6934</td> <td>    0.033</td> <td>   20.792</td> <td> 0.000</td> <td>    0.628</td> <td>    0.759</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 3.428</td> <th>  Durbin-Watson:     </th> <td>   0.809</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.180</td> <th>  Jarque-Bera (JB):  </th> <td>   2.731</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.075</td> <th>  Prob(JB):          </th> <td>   0.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.630</td> <th>  Cond. No.          </th> <td>    96.7</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:        happiness_score   R-squared:                       0.695\n",
       "Model:                            OLS   Adj. R-squared:                  0.693\n",
       "Method:                 Least Squares   F-statistic:                     309.6\n",
       "Date:                Mon, 24 Feb 2020   Prob (F-statistic):          1.24e-104\n",
       "Time:                        17:34:57   Log-Likelihood:                -390.15\n",
       "No. Observations:                 411   AIC:                             788.3\n",
       "Df Residuals:                     407   BIC:                             804.4\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "======================================================================================\n",
       "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------\n",
       "Intercept             -1.0105      0.314     -3.214      0.001      -1.628      -0.393\n",
       "democratic_quality     0.1704      0.046      3.714      0.000       0.080       0.261\n",
       "generosity             1.1608      0.195      5.938      0.000       0.777       1.545\n",
       "log_gdp_per_capita     0.6934      0.033     20.792      0.000       0.628       0.759\n",
       "==============================================================================\n",
       "Omnibus:                        3.428   Durbin-Watson:                   0.809\n",
       "Prob(Omnibus):                  0.180   Jarque-Bera (JB):                2.731\n",
       "Skew:                           0.075   Prob(JB):                        0.255\n",
       "Kurtosis:                       2.630   Cond. No.                         96.7\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_model_base_sum = happy_model_base.summary()\n",
    "\n",
    "happy_model_base_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary provides several pieces of information: the coefficients or weights (`coef`), the standard errors (`std err`), the t-statistic (which is just the coefficient divided by the standard error), and the corresponding p-value.  The main thing to look at are the actual coefficients and the direction of their relationship, positive or negative. For example, with regard to the effect of democratic quality, moving one point on democratic quality results in roughly .17 units of happiness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we must also have in order to understand our results is to get a sense of the uncertainty in the effects.  The summary provides confidence intervals for each of the coefficients, and with them, we have a sense of the range of plausible values for the coefficients. We can also obtain it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Intercept</td>\n",
       "      <td>-1.628455</td>\n",
       "      <td>-0.392500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>democratic_quality</td>\n",
       "      <td>0.080188</td>\n",
       "      <td>0.260559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>generosity</td>\n",
       "      <td>0.776562</td>\n",
       "      <td>1.545131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>log_gdp_per_capita</td>\n",
       "      <td>0.627862</td>\n",
       "      <td>0.758981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0         1\n",
       "Intercept          -1.628455 -0.392500\n",
       "democratic_quality  0.080188  0.260559\n",
       "generosity          0.776562  1.545131\n",
       "log_gdp_per_capita  0.627862  0.758981"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_model_base.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value we actually estimate is the best guess given our circumstances, but slight changes in the data, the way we collect it, the time we collect it, etc., all would result in a slightly different result. The confidence interval provides a range of what we could expect given the uncertainty, and, given its importance, you should always report it.  In fact, just showing the coefficient and the interval would be better than typical reporting of the statistical test results, though you can do both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming variables can provide a few benefits, whether applied to the target, covariates, or both, and should regularly be used for most models. Some of these benefits include:\n",
    "\n",
    "- Interpretable intercepts\n",
    "- More comparable covariate effects\n",
    "- Faster estimation\n",
    "- Easier convergence\n",
    "- Help with heteroscedasticity\n",
    "\n",
    "For example, merely centering predictor variables, i.e. subtracting the mean, provides a more interpretable intercept that will fall within the actual range of the target variable, telling us what the value of the target variable is when the covariates are at their means (or reference value if categorical).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two extremely common transformations applied to numeric variables- logging and scaling.  We can, and typically should apply these to the data beforehand, and this will usually be done with `numpy` and other tools.  You can do them in statsmodels formulas though also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/micl/anaconda3/lib/python3.6/site-packages/pandas/core/series.py:853: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<statsmodels.regression.linear_model.OLS at 0x1c1b3b6748>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.ols(\n",
    "  'happiness_score ~ np.log(democratic_quality) + generosity + log_gdp_per_capita',\n",
    "  data = happy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A raw character string is not an analyzable unit, so character strings and labeled variables like factors must be converted for analysis to be conducted on them. For categorical variables, we can employ what is called *effects coding* to test for specific types of group differences.  Far and away the most common approach is called *dummy coding* or *one-hot encoding*.  In the next example, we will use dummy coding via pandas.  statsmodels will recognize the categorical type from the pandas data frame and do the coding behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>life_ladder</th>\n",
       "      <th>log_gdp_per_capita</th>\n",
       "      <th>social_support</th>\n",
       "      <th>healthy_life_expectancy_at_birth</th>\n",
       "      <th>freedom_to_make_life_choices</th>\n",
       "      <th>generosity</th>\n",
       "      <th>perceptions_of_corruption</th>\n",
       "      <th>positive_affect</th>\n",
       "      <th>negative_affect</th>\n",
       "      <th>confidence_in_national_government</th>\n",
       "      <th>democratic_quality</th>\n",
       "      <th>delivery_quality</th>\n",
       "      <th>gini_index_world_bank_estimate</th>\n",
       "      <th>happiness_score</th>\n",
       "      <th>dystopia_residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2005</td>\n",
       "      <td>7.418048</td>\n",
       "      <td>10.608347</td>\n",
       "      <td>0.961552</td>\n",
       "      <td>71.300003</td>\n",
       "      <td>0.957306</td>\n",
       "      <td>0.244575</td>\n",
       "      <td>0.502681</td>\n",
       "      <td>0.838544</td>\n",
       "      <td>0.233278</td>\n",
       "      <td>0.442818</td>\n",
       "      <td>1.168249</td>\n",
       "      <td>1.748819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2007</td>\n",
       "      <td>7.481753</td>\n",
       "      <td>10.636994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.660004</td>\n",
       "      <td>0.930341</td>\n",
       "      <td>0.241932</td>\n",
       "      <td>0.405608</td>\n",
       "      <td>0.871604</td>\n",
       "      <td>0.256810</td>\n",
       "      <td>0.637824</td>\n",
       "      <td>1.195769</td>\n",
       "      <td>1.795603</td>\n",
       "      <td>0.338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2008</td>\n",
       "      <td>7.485604</td>\n",
       "      <td>10.636126</td>\n",
       "      <td>0.938707</td>\n",
       "      <td>71.839996</td>\n",
       "      <td>0.926315</td>\n",
       "      <td>0.254044</td>\n",
       "      <td>0.369588</td>\n",
       "      <td>0.890220</td>\n",
       "      <td>0.202175</td>\n",
       "      <td>0.590371</td>\n",
       "      <td>1.224689</td>\n",
       "      <td>1.813375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2009</td>\n",
       "      <td>7.487824</td>\n",
       "      <td>10.594738</td>\n",
       "      <td>0.942845</td>\n",
       "      <td>72.019997</td>\n",
       "      <td>0.915058</td>\n",
       "      <td>0.238822</td>\n",
       "      <td>0.412622</td>\n",
       "      <td>0.867433</td>\n",
       "      <td>0.247633</td>\n",
       "      <td>0.608264</td>\n",
       "      <td>1.264292</td>\n",
       "      <td>1.827809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2010</td>\n",
       "      <td>7.650346</td>\n",
       "      <td>10.613968</td>\n",
       "      <td>0.953765</td>\n",
       "      <td>72.199997</td>\n",
       "      <td>0.933949</td>\n",
       "      <td>0.223002</td>\n",
       "      <td>0.412660</td>\n",
       "      <td>0.878868</td>\n",
       "      <td>0.233113</td>\n",
       "      <td>0.551076</td>\n",
       "      <td>1.144457</td>\n",
       "      <td>1.838644</td>\n",
       "      <td>0.336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country  year  life_ladder  log_gdp_per_capita  social_support  \\\n",
       "258  Canada  2005     7.418048           10.608347        0.961552   \n",
       "259  Canada  2007     7.481753           10.636994             NaN   \n",
       "260  Canada  2008     7.485604           10.636126        0.938707   \n",
       "261  Canada  2009     7.487824           10.594738        0.942845   \n",
       "262  Canada  2010     7.650346           10.613968        0.953765   \n",
       "\n",
       "     healthy_life_expectancy_at_birth  freedom_to_make_life_choices  \\\n",
       "258                         71.300003                      0.957306   \n",
       "259                         71.660004                      0.930341   \n",
       "260                         71.839996                      0.926315   \n",
       "261                         72.019997                      0.915058   \n",
       "262                         72.199997                      0.933949   \n",
       "\n",
       "     generosity  perceptions_of_corruption  positive_affect  negative_affect  \\\n",
       "258    0.244575                   0.502681         0.838544         0.233278   \n",
       "259    0.241932                   0.405608         0.871604         0.256810   \n",
       "260    0.254044                   0.369588         0.890220         0.202175   \n",
       "261    0.238822                   0.412622         0.867433         0.247633   \n",
       "262    0.223002                   0.412660         0.878868         0.233113   \n",
       "\n",
       "     confidence_in_national_government  democratic_quality  delivery_quality  \\\n",
       "258                           0.442818            1.168249          1.748819   \n",
       "259                           0.637824            1.195769          1.795603   \n",
       "260                           0.590371            1.224689          1.813375   \n",
       "261                           0.608264            1.264292          1.827809   \n",
       "262                           0.551076            1.144457          1.838644   \n",
       "\n",
       "     gini_index_world_bank_estimate  happiness_score  dystopia_residual  \n",
       "258                             NaN              NaN                NaN  \n",
       "259                           0.338              NaN                NaN  \n",
       "260                             NaN              NaN                NaN  \n",
       "261                             NaN              NaN                NaN  \n",
       "262                           0.336              NaN                NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nafta = happy[happy.country.isin(['United States', 'Canada', 'Mexico'])]\n",
    "nafta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/micl/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1450: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=12\n",
      "  \"anyway, n=%i\" % int(n))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>happiness_score</td> <th>  R-squared:         </th> <td>   0.692</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.624</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   10.11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 24 Feb 2020</td> <th>  Prob (F-statistic):</th>  <td>0.00499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:34:57</td>     <th>  Log-Likelihood:    </th> <td>  4.4602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    12</td>      <th>  AIC:               </th> <td>  -2.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     9</td>      <th>  BIC:               </th> <td>  -1.466</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>                <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                <td>    7.3689</td> <td>    0.096</td> <td>   76.493</td> <td> 0.000</td> <td>    7.151</td> <td>    7.587</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country[T.Mexico]</th>        <td>   -0.6111</td> <td>    0.136</td> <td>   -4.485</td> <td> 0.002</td> <td>   -0.919</td> <td>   -0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country[T.United States]</th> <td>   -0.3434</td> <td>    0.136</td> <td>   -2.520</td> <td> 0.033</td> <td>   -0.652</td> <td>   -0.035</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 6.533</td> <th>  Durbin-Watson:     </th> <td>   1.796</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.038</td> <th>  Jarque-Bera (JB):  </th> <td>   2.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.937</td> <th>  Prob(JB):          </th> <td>   0.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.384</td> <th>  Cond. No.          </th> <td>    3.73</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:        happiness_score   R-squared:                       0.692\n",
       "Model:                            OLS   Adj. R-squared:                  0.624\n",
       "Method:                 Least Squares   F-statistic:                     10.11\n",
       "Date:                Mon, 24 Feb 2020   Prob (F-statistic):            0.00499\n",
       "Time:                        17:34:57   Log-Likelihood:                 4.4602\n",
       "No. Observations:                  12   AIC:                            -2.920\n",
       "Df Residuals:                       9   BIC:                            -1.466\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "============================================================================================\n",
       "                               coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------------\n",
       "Intercept                    7.3689      0.096     76.493      0.000       7.151       7.587\n",
       "country[T.Mexico]           -0.6111      0.136     -4.485      0.002      -0.919      -0.303\n",
       "country[T.United States]    -0.3434      0.136     -2.520      0.033      -0.652      -0.035\n",
       "==============================================================================\n",
       "Omnibus:                        6.533   Durbin-Watson:                   1.796\n",
       "Prob(Omnibus):                  0.038   Jarque-Bera (JB):                2.715\n",
       "Skew:                           0.937   Prob(JB):                        0.257\n",
       "Kurtosis:                       4.384   Cond. No.                         3.73\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.ols('happiness_score ~ country', data = nafta).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the coefficient represents the difference in means on the target variable between the reference group and the group in question.  In this case, the U.S. is .34 less on the happy score than the reference country (Canada).\n",
    "\n",
    "Other codings are possible, and these would allow for specific group comparisons or types of comparisons.  This is sometimes called *contrast coding*.  For example, we could compare each group to the overall mean using [Sum coding](https://www.statsmodels.org/dev/contrasts.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [-1. -1.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>happiness_score</td> <th>  R-squared:         </th> <td>   0.692</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.624</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   10.11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 24 Feb 2020</td> <th>  Prob (F-statistic):</th>  <td>0.00499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:34:57</td>     <th>  Log-Likelihood:    </th> <td>  4.4602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    12</td>      <th>  AIC:               </th> <td>  -2.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     9</td>      <th>  BIC:               </th> <td>  -1.466</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                 <td>    7.0507</td> <td>    0.056</td> <td>  126.769</td> <td> 0.000</td> <td>    6.925</td> <td>    7.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(country, Sum)[S.Canada]</th> <td>    0.3181</td> <td>    0.079</td> <td>    4.045</td> <td> 0.003</td> <td>    0.140</td> <td>    0.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(country, Sum)[S.Mexico]</th> <td>   -0.2929</td> <td>    0.079</td> <td>   -3.724</td> <td> 0.005</td> <td>   -0.471</td> <td>   -0.115</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 6.533</td> <th>  Durbin-Watson:     </th> <td>   1.796</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.038</td> <th>  Jarque-Bera (JB):  </th> <td>   2.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.937</td> <th>  Prob(JB):          </th> <td>   0.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.384</td> <th>  Cond. No.          </th> <td>    1.73</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:        happiness_score   R-squared:                       0.692\n",
       "Model:                            OLS   Adj. R-squared:                  0.624\n",
       "Method:                 Least Squares   F-statistic:                     10.11\n",
       "Date:                Mon, 24 Feb 2020   Prob (F-statistic):            0.00499\n",
       "Time:                        17:34:57   Log-Likelihood:                 4.4602\n",
       "No. Observations:                  12   AIC:                            -2.920\n",
       "Df Residuals:                       9   BIC:                            -1.466\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=============================================================================================\n",
       "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------\n",
       "Intercept                     7.0507      0.056    126.769      0.000       6.925       7.177\n",
       "C(country, Sum)[S.Canada]     0.3181      0.079      4.045      0.003       0.140       0.496\n",
       "C(country, Sum)[S.Mexico]    -0.2929      0.079     -3.724      0.005      -0.471      -0.115\n",
       "==============================================================================\n",
       "Omnibus:                        6.533   Durbin-Watson:                   1.796\n",
       "Prob(Omnibus):                  0.038   Jarque-Bera (JB):                2.715\n",
       "Skew:                           0.937   Prob(JB):                        0.257\n",
       "Kurtosis:                       4.384   Cond. No.                         1.73\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from patsy.contrasts import Sum\n",
    "\n",
    "levels = [1,2,3]\n",
    "contrast = Sum().code_without_intercept(levels)\n",
    "\n",
    "print(contrast.matrix)\n",
    "\n",
    "smf.ols('happiness_score ~ C(country, Sum)', data = nafta).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Extracting Output\n",
    "\n",
    "The better you get at modeling, the more often you are going to need to get at certain parts of the model output easily.  For example, we can extract the coefficients, residuals, model data and other parts from standard linear model objects from a statsmodels object.\n",
    "\n",
    "\n",
    "Why would you want to do this?  A simple example would be to compare effects across different settings. We can collect the values, put them in a data frame, and then to a table or visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical modeling methods/attributes you might want to use:\n",
    "\n",
    "- **summary**: print results in a legible way\n",
    "- **predict**: make predictions, possibly on new data\n",
    "- **conf_int**: get confidence intervals for parameters\n",
    "- **params**: extract coefficients\n",
    "- **fittedvalues**: extract fitted values\n",
    "- **resid**: extract residuals\n",
    "- **aic**: extract AIC\n",
    "- **rsquared_adj**: adjusted R**2\n",
    "- **mse_resid**: mean squared error residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "788.2967012572146"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_model_base.aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3.838179\n",
       "1    3.959046\n",
       "2    3.928180\n",
       "3    4.004129\n",
       "4    4.171624\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_model_base.predict(happy[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39472541113224174"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_model_base.mse_resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python and/or statsmodels doesn't have a lot going for visualizations of models unfortunately, but there are a few tools for exploration.  See a [list here](https://www.statsmodels.org/devel/graphics.html).  You can get partial regression and component plus residual plots, among others.  These are great for a quick look at what's going on, but you'll likely need to get your hands dirty to get anything presentable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9e93a6c18404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_regress_exog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhappy_model_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'generosity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sm' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "sm.graphics.plot_regress_exog(happy_model_base, 'generosity', fig = fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sm.graphics.plot_leverage_resid2(happy_model_base) # spot potential problematic observations your model fails to explain.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions to the Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of targets\n",
    "\n",
    "In many data situations, we do not have a continuous numeric target variable, or may want to use a different distribution to get a better fit, or adhere to some theoretical perspective.  For example, count data is not continuous and often notably skewed, so assuming a normal symmetric distribution may not work as well. From a data generating perspective we can use the Poisson distribution for the target variable instead.  \n",
    "\n",
    "$$\\ln{\\mu} =  X\\beta$$\n",
    "\n",
    "$$\\mu = e^{X\\beta}$$\n",
    "\n",
    "$$y \\sim \\mathcal{Pois}(\\mu)$$\n",
    "\n",
    "Conceptually nothing has really changed from what we were doing with the standard linear model, except for the distribution.  We still have a mean function determined by our predictors, and this is what we're typically mainly interested in from a theoretical perspective.  We do have an added step, a transformation of the mean (now usually called the *linear predictor*).  Poisson naturally works with the log of the target, but rather than do that explicitly, we instead exponentiate the linear predictor.  The *link function*, which is the natural log in this setting, has a corresponding  *inverse link* (or mean function)- exponentiation.\n",
    "\n",
    "In code we can demonstrate this as follows similar to how we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.085631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.997345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.282978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.506295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.578600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y         x\n",
       "0   2 -1.085631\n",
       "1  23  0.997345\n",
       "2  10  0.282978\n",
       "3   2 -1.506295\n",
       "4   1 -0.578600"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)                  # for reproducibility\n",
    "N = 1000                             # sample size\n",
    "beta = [2, 1]                        # the true coefficient values\n",
    "x = np.random.normal(size = N)       # a single predictor variable\n",
    "mu = np.exp(beta[0] + beta[1]*x)     # the linear predictor\n",
    "y = np.random.poisson(size = N, lam = mu)         # the target variable lambda = mean\n",
    "\n",
    "df = pd.DataFrame({'y': y, 'x':x})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td>  1000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   998</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>         <td>Poisson</td>     <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>          <td>log</td>       <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -2389.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 24 Feb 2020</td> <th>  Deviance:          </th> <td>  1066.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>17:34:58</td>     <th>  Pearson chi2:      </th> <td>1.03e+03</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>5</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    2.0028</td> <td>    0.013</td> <td>  156.789</td> <td> 0.000</td> <td>    1.978</td> <td>    2.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>         <td>    1.0027</td> <td>    0.009</td> <td>  108.427</td> <td> 0.000</td> <td>    0.985</td> <td>    1.021</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 1000\n",
       "Model:                            GLM   Df Residuals:                      998\n",
       "Model Family:                 Poisson   Df Model:                            1\n",
       "Link Function:                    log   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -2389.9\n",
       "Date:                Mon, 24 Feb 2020   Deviance:                       1066.3\n",
       "Time:                        17:34:58   Pearson chi2:                 1.03e+03\n",
       "No. Iterations:                     5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      2.0028      0.013    156.789      0.000       1.978       2.028\n",
       "x              1.0027      0.009    108.427      0.000       0.985       1.021\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.glm('y ~ x', data = df, family = sm.families.Poisson()).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common setting is the case where our target variable takes on only two values- yes vs. no, alive vs. dead, etc.  The most common model used in such settings is the logistic regression model.  In this case, it will have a different link to go with a different distribution.\n",
    "\n",
    "$$\\ln{\\frac{\\mu}{1-\\mu}} =  X\\beta$$\n",
    "\n",
    "$$\\mu = \\frac{1}{1+e^{-X\\beta}}$$\n",
    "\n",
    "$$y \\sim \\mathcal{Binom}(\\mathrm{prob}=\\mu, \\mathrm{size} = 1)$$\n",
    "\n",
    "\n",
    "Here our link function is called the *logit*, and it's inverse takes our linear predictor and puts it on the probability scale.\n",
    "\n",
    "Again, some code can help drive this home.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.085631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.282978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.506295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.578600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y         x\n",
       "0  1 -1.085631\n",
       "1  1  0.997345\n",
       "2  1  0.282978\n",
       "3  1 -1.506295\n",
       "4  1 -0.578600"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = 1 / (1 + np.exp(-(beta[0] + beta[1]*x)))\n",
    "y = np.random.binomial(size = N, n = 1, p = mu)\n",
    "\n",
    "df = pd.DataFrame({'y': y, 'x': x})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td>  1000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   998</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -391.14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 24 Feb 2020</td> <th>  Deviance:          </th> <td>  782.28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>17:34:58</td>     <th>  Pearson chi2:      </th>  <td>  978.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>5</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    1.9580</td> <td>    0.109</td> <td>   18.003</td> <td> 0.000</td> <td>    1.745</td> <td>    2.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>         <td>    0.9962</td> <td>    0.103</td> <td>    9.646</td> <td> 0.000</td> <td>    0.794</td> <td>    1.199</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 1000\n",
       "Model:                            GLM   Df Residuals:                      998\n",
       "Model Family:                Binomial   Df Model:                            1\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -391.14\n",
       "Date:                Mon, 24 Feb 2020   Deviance:                       782.28\n",
       "Time:                        17:34:58   Pearson chi2:                     978.\n",
       "No. Iterations:                     5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      1.9580      0.109     18.003      0.000       1.745       2.171\n",
       "x              0.9962      0.103      9.646      0.000       0.794       1.199\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.glm('y ~ x', data = df, family = sm.families.Binomial()).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll have noticed that when we fit these models we used `glm` instead of `ols`. The normal linear model is a special case of *generalized linear models*, which includes a specific class of distributions- normal, poisson, binomial, gamma, beta and more- collectively referred to as the [exponential family](https://en.wikipedia.org/wiki/Exponential_family).  While this family can cover a lot of ground, you do not have to restrict yourself to it, and other modules will provide access to more options.  The main point is that you have tools to deal with continuous, binary, count, ordinal, and other types of data.  Furthermore, not much necessarily changes conceptually from model to model besides the link function and/or distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlated data\n",
    "\n",
    "Often in standard regression modeling situations we have data that is correlated, like when we observe multiple observations for individuals (e.g. longitudinal studies), or observations are clustered within geographic units.  There are many ways to analyze all kinds of correlated data in the form of clustered data, time series, spatial data and similar. In terms of understanding the mean function and data generating distribution for our target variable, as we did in our previous models, not much changes.  However, we will want to utilize estimation techniques that take this correlation into account.  Examples of such models include:\n",
    "\n",
    "- Mixed models (e.g. random intercepts, 'multilevel' models)\n",
    "- Time series models (autoregressive)\n",
    "- Spatial models (e.g. conditional autoregressive)\n",
    "\n",
    "As demonstration is beyond the scope of this document, the main point here is awareness. But see the statsmodels docs on [mixed models](https://www.statsmodels.org/stable/examples/notebooks/generated/mixed_lm_example.html) and [many others](https://www.statsmodels.org/devel/user-guide.html#regression-and-linear-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other extensions\n",
    "\n",
    "There are many types of models that will take one well beyond the standard linear model.  In some cases, the focus is multivariate, trying to model many targets at once. Other models will even be domain-specific, tailored to a very narrow type of problem. Whatever the scenario, having a good understanding of the models we've been discussing will likely help you navigate these new waters much more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
