{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document is a Python exploration of this R-based document: https://m-clark.github.io/data-processing-and-visualization/reproducibility.html.  In addition, all the content is located with the main document, not here, so some sections may not be included.  I only focus on reproducing the code chunks and providing some useful context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Better Data-Driven Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we've covered many topics that will get you from data import and generation to visualizing model results.  What's left? To tell others about what you've discovered!  While there are any number of ways to present your *data-driven product*, there are a couple of things to keep in mind regardless of the chosen rendition. Chief among them is building a product that will be intimately connected with all the work that went before it, and which will be consistent across products and (hopefully) over time as well.\n",
    "\n",
    "We'll start our discussion of how to present one's work with some terminology you might have come across:\n",
    "\n",
    "- *Reproducible research*\n",
    "- *Repeatable research*\n",
    "- *Replicable science*\n",
    "- *Reproducible data analysis*\n",
    "- *Literate programming*\n",
    "- *Dynamic data analysis*\n",
    "- *Dynamic report generation*\n",
    "\n",
    "Each of these may mean slightly different things depending on the context and background of the person using them, so one should take care to note precisely what is meant.  We'll examine some of these concepts, or at least my particular version of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rep* Analysis\n",
    "\n",
    "Let's start with the notions of *replicability*, *repeatability*, and *reproducibility*, which are hot topics in various disciplines of late.  In our case, we are specifically concerned with programming and analytical results, visualizations, etc. (e.g. as opposed to running an experiment).\n",
    "\n",
    "To begin, these and related terms are often not precisely defined, and depending on the definition one selects, possibly unlikely, or even impossible!  I'll roughly follow the Association for Computing Machinery guidelines @acm2020 since they actually do define them, but mostly just to help get us organize our thinking about them, and so you can at least know what *I* mean when I use the terms.  In many cases, the concepts are best thought of as ideals to strive for, or goals for certain aspects of the data analysis process.  For example, in deference to Heraclitus, Cratylus, the Buddha, and others, nothing is exactly replicable, if only because time will have passed, and with it some things will have changed about the process since the initial analysis was conducted- the people involved, the data collection approach, the analytical tools, etc.  Indeed, even your thought processes regarding the programming and analysis are in constant flux while engaged with the data process.  However, we can replicate some things or some aspects of the process, possibly even exactly, and thus make the results reproducible.  In other cases, even when we can, we may not want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "As our focus will be on data analysis in particular, let's start with the following scenario. Various versions of a data set are used leading up to analysis, and after several iterations, `finaldata7` is now spread across the computers of the faculty advisor, two graduate students and one undergraduate student. Two of those `finaldata7` data sets, specifically named `finaldata7a` and `finaldata7b`, are slightly different from the other two and each other.  The undergraduate, who helped with the processing of `finaldata2` through `finaldata6`, has graduated and no longer resides in the same state, and has other things to occupy their time.  Some of the data processing was done with menus in a software package that shall not be named.\n",
    "\n",
    "The script that did the final analysis, called `results.C`, calls the data using a directory location which no longer exists (and refers only to `finaldata7`).  Though it is titled 'results', the script performs several more data processing steps, but without comments that would indicate why any of them are being done.  Some of the variables are named things like `PDQ` and `V3`, but there is no documentation that would say what those mean.\n",
    "\n",
    "When writing their research document in Microsoft Word, all the values from the analyses were copied and pasted into the tables and text[^intheback].  The variable names in the document have no exact match to any of the names in any of the data objects.  Furthermore, no reference was provided in the text regarding what software or specific packages were used for the analysis.  \n",
    "\n",
    "And now, several months later, after the final draft of the document was written and sent to the journal, the reviewers have eventually made their comments on the paper, and it's time to dive back into the analysis.  Now, what do you think the odds are that this research group could even reproduce the values reported in the main analysis of the paper?\n",
    "\n",
    "Sadly, up until recently this was not uncommon, and even certain issues just described are still very common.  Such an approach is essentially the antithesis of replicability and reproducible research[^notsci].  Anything that was only done with menus cannot be replicated for certain, and without sufficient documentation it's not clear what was done even when there is potentially reproducible code.  The naming of files, variables and other objects was done poorly, so it will take unnecessary effort to figure out what was done, to what, and when.  And even after most things get squared away, there is still a chance the numbers won't match what was in the paper anyway.  This scenario is exactly what we don't want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeatable\n",
    "\n",
    "*Repeatability* can simply be thought of as whether *you* can run the code and analysis again given the same circumstances, essentially producing the same results.  In the above scenario, even this is may not even possible.  But let's say that whoever did the analysis can run their code, it works, and produces a result very similar to what was published.  We could then say it's repeatable. This should only be seen as a minimum standard, though sometimes it is enough.\n",
    "\n",
    "The notion of repeatability also extends to a specific measure itself. This consistency of a measure across repeated observations is typically referred to as *reliability*. This is not our focus here, but I mention it for those who have the false belief that at least some data driven products are entirely replicable.  However, you can't escape measurement error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducible\n",
    "\n",
    "Now let someone else try the analytical process to see if they can *reproduce* the results. Assuming the same data starting point, they should get the same result using the same tools.  For our scenario, if we just start at the very last part, maybe this is possible, but at the least, it would require the data that went into the final analysis and the model being specified in a way that anyone could potentially understand.  However, it is entirely unlikely that if they start from the raw data import they would get the same results that are in the Word document.  If a research article does not provide the analytical data, nor specifies the model in code or math, it is not reproducible, and we can only take on faith what was done. \n",
    "\n",
    "Here are some typical non-reproducible situations:\n",
    "\n",
    "- data is not made available\n",
    "- code is not made available\n",
    "- model is not adequately represented (in math or code)\n",
    "- data processing and/or analysis was done with menus\n",
    "- visualizations were tweaked in other programs than the one that produced it\n",
    "- p-hacking efforts where 'outliers' are removed or other data transformations were undertaken to obtain a desired result, and are not reported or are not explained well enough to reproduce\n",
    "\n",
    "I find the lack of clear model explanation to be pervasive in some sciences. For example, I have seen articles in medical outlets where they ran a mixed model, yet none of the variance components or even a regression table is provided, nor is the model depicted in a formal fashion.  You can forget about the code or data being provided as well. I also tend to ignore analyses done using SPSS, because the only reason to use the program is to not have to use the syntax, making reproducibility difficult at best, if it's even possible.\n",
    "\n",
    "Tools like **Docker**, **packrat**, and others can ensure that the package environment is the same even if you're running the same code years from now, so that results should be reproduced if run on the same data, assuming other things are accounted for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicable\n",
    "\n",
    "*Replicability*, for our purposes, would be something like, if someone had the same type of data (e.g. same structure), and did the same analysis using their own setup (though with the same or similar tools), would they get the same result (to within some tolerance)?  \n",
    "\n",
    "For example, if I have some new data that is otherwise the same, install the same R packages etc. on my machine rather than yours, will I get a very similar result (on average)?  Similarly, if I do the exact same analysis using some other package (i.e. using the same estimation procedure even if the underlying code implementation is different), will the results be highly similar?\n",
    "\n",
    "Here are some typical non-replicable situations:\n",
    "\n",
    "- all of the non-reproducible/repeatable situations\n",
    "- new versions of the packages break old code, fix bugs that ultimately change results, etc.\n",
    "- small data and/or overfit models\n",
    "\n",
    "The last example is an interesting one, and yet it is also one that is driving a lot of so-called unreplicated findings.  Even with a clear model and method, if you're running a complex analysis on small data without any regularization, or explicit understanding of the uncertainty, the odds of seeing the same results in a new setting are not very strong.  While this has been well known and taught in every intro stats course people have taken, the concept evidently immediately gets lost in practice.  I see people regularly befuddled as to why they don't see the same thing when they only have a couple hundred or fewer observations[^hugeeffect].  However, the uncertainty in small samples, if reported, should make this no surprise.\n",
    "\n",
    "For my own work, I'm not typically as interested in analytical replicability, as I want my results to work now, not replicate precisely what I did two years ago.  No code is bug free, improvements in tools should typically lead to improvements in modeling approach, etc.  In the end, I don't mind the extra work to get my old code working with the latest packages, and there is a correlation between recency and relevancy.  However, if such replicability is desired, specific tools will need to be used, such as version control (Git), containers (e.g. Docker), and similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the stated ACM guidelines\n",
    "\n",
    "- Repeatability (Same team, same experimental setup)\n",
    "  - The measurement can be obtained with stated precision by the same team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same location on multiple trials. For computational experiments, this means that a researcher can reliably repeat her own computation.\n",
    "\n",
    "- Reproducibility (Different team, same experimental setup)\n",
    "  - The measurement can be obtained with stated precision by a different team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same or a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using the author’s own artifacts.\n",
    "\n",
    "- Replicability (Different team, different experimental setup)\n",
    "  - The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using artifacts which they develop completely independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of rep* analysis\n",
    "\n",
    "In summary, truly rep* data analysis requires:\n",
    "\n",
    "- Accessible data, or at least, essentially similar data[^hipaa]\n",
    "- Accessible, well written code\n",
    "- Clear documentation (of data and code)\n",
    "- Version control\n",
    "- Standard means of distribution\n",
    "- Literate programming practices\n",
    "- Possibly more depending on the stringency of desired replicability\n",
    "\n",
    "We've seen a poor example, what about a good one?  For instance, one could start their research as an RStudio project using Git for version control, write their research products using R Markdown, set seeds for random variables, and use <span class=\"pack\">packrat</span> to keep the packages used in analysis specific to the project.  Doing so would make it much more likely to reproduce the previous results at any stage, even years later.\n",
    "\n",
    "- [Reproducibility Guide by ROpenSci](https://ropensci.github.io/reproducibility-guide/)\n",
    "- [Ten Simple Rules for Reproducible Computational Research](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285) \n",
    "- [Recommendations to Funding Agencies for Supporting Reproducible Research](https://www.amstat.org/asa/files/pdfs/POL-ReproducibleResearchRecommendations.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literate Programming\n",
    "\n",
    "At this point we have an idea of what we want.  But how to we get it?  There is an additional concept to think about that will help us with regard to programming and data analysis. So let's now talk about *literate programming*, which is actually an [old idea](http://www.literateprogramming.com/knuthweb.pdf).\n",
    "\n",
    "> I believe that the time is ripe for significantly better documentation of programs, and that we can best achieve this by considering programs to be works of literature. <br> ~ Donald Knuth (1984)\n",
    "\n",
    "The interweaving of code and text is something many already do in normal scripting.  *Comments* in code are not only useful, they are practically required.  But in a program script, almost all the emphasis is on the code.  With literate programming, we instead focus on the text, and the code exists to help facilitate our ability to tell a (data-driven) story.\n",
    "\n",
    "In the early days, the idea was largely to communicate the idea of the computer program itself. Now, at least in the context we'll be discussing, our usage of literate programming is to generate results that tell the story in a completely human-oriented fashion, possibly without any reference to the code at all.  However, the document, in whatever format, does not exist independently of the code, and cannot be generated without it.\n",
    "\n",
    "Consider the following example. This code, which is clearly delimited from the text via background and font style, shows how to do an unordered list in *Markdown* using two different methods.  Either a `-` or a `*` will denote a list item.\n",
    "\n",
    "```\n",
    "- item 1\n",
    "- item 2\n",
    "\n",
    "* item 3\n",
    "* item 4\n",
    "```\n",
    "\n",
    "So, we have a statement explaining the code, followed by the code itself. We actually don't need a code comment, because the text explains the code in everyday language.  This is a simple example, but it gets at the essence of the approach.  In the document you're reading right now, code may be visible or not, but when visible, it's clear what the code part is and what the text explaining the code is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of literate programming, i.e. creating human-understandable programs, can extend beyond reports or slides that you might put together for an analysis, and in fact be used for any setting in which you have to write code at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R Markdown\n",
    "\n",
    "Now let's shift our focus from concepts to implementation.  Even for Python users, *R Markdown* provides a viable means for literate programming. It is a flavor of Markdown, a markup language used pervasively throughout the web.  Markdown can be converted to other formats like HTML, but is as easy to use as normal text. R Markdown allows one to combine normal R code with text to produce a wide variety of document formats.  This allows for a continuous transition from initial data import and processing to a finished product, whether journal article, software application, slide presentation, or even a website.\n",
    "\n",
    "To use R Markdown effectively, it helps to know why you'd even want to. So, in addition to literate programming, let's talk about some ideas, all of which are related, and which will give you some sense of the goals of effective document generation, and why this approach is superior to others you might try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read this chapter\n",
    "\n",
    "Any 'cell' or 'chunk' can be examined by double clicking it. Then you can see the underlying links, lists, etc. This chapter (and the rest of the document) is [available on GitHub](https://raw.githubusercontent.com/m-clark/data-processing-and-visualization/master/concepts.Rmd). Looking at the raw content (i.e. the R Markdown files *.Rmd) versus the finished product will get you well on your way to understanding how to use various tools at your disposal to produce a better data driven product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic data analysis & report generation\n",
    "\n",
    "Sometimes the goal is to create an expression of the analysis that is not only to be disseminated to a particular audience, but one which possibly will change over time, as the data itself evolves temporally.  In this dynamic setting, the document must be able to handle changes with minimal effort.  \n",
    "\n",
    "I can tell you from firsthand experience that R Markdown can allow one to automatically create custom presentation products for different audiences on a regular basis without even touching the data for explicit processing, nor the reports after the templates are created, even as the data continues to come in over time.  Furthermore, any academic effort that would fall under the heading of science is applicable here.\n",
    "\n",
    "The notion of *science as software development* is something you should get used to. Print has had its day, but is not the best choice for scientific advancement as it should take place.  Waiting months for feedback, or a year to get a paper published after it's first sent for review, and then hoping people have access to a possibly pay-walled outlet, is simply unacceptable.  Furthermore, what if more data comes in? A data or modeling bug is found? Other studies shed additional light on the conclusions?  In this day and age, are we supposed to just continue to cite a work that may no longer be applicable while waiting another year or so for updates?\n",
    "\n",
    "\n",
    "Consider [arxiv.org](https://arxiv.org/). Researchers will put papers there before they are published in journals, ostensibly to provide an openly available, if not *necessarily* 100% complete, work.  Others put working drafts or just use it as a place to float some ideas out there.  It is a serious outlet however, and a good chunk of the articles I read in the stats world can be found there.\n",
    "\n",
    "In a similar vein, many of my own documents on machine learning, Bayesian analysis, generalized additive models, etc. have been regularly updated for several years now.\n",
    "\n",
    "Research is never complete.  Data can be augmented, analyses tweaked, visualizations improved. Will the products of your own efforts adapt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Modern Tools\n",
    "\n",
    "The main problem for other avenues you might use, like MS Word and $\\LaTeX$, is that they were created for printed documents.  However, not only is printing unnecessary (and environmentally problematic), contorting a document to the confines of print potentially distorts or hinders the meaning an author wishes to convey, as well as restricts the means with which they can convey it.  In addition, for academic outlets and well beyond, print is not the dominant format anymore.  Even avid print readers must admit they see much more text on a screen than they do on a page on a typical day.\n",
    "\n",
    "\n",
    "Let's recap the issues with traditional approaches:\n",
    "\n",
    "- Possibly not usable for rep* analysis\n",
    "- Syntax gets in the way of fluid text\n",
    "- Designed for print\n",
    "- Wasteful if printed\n",
    "- Often very difficult to get visualizations/tables to look as desired\n",
    "- No interactivity\n",
    "\n",
    "\n",
    "The case for using a markdown approach is now years old and well established. Unfortunately many, but not all, journals are still print-oriented, because their income depends on the assumption of print, not to mention a closed-source, broken, and anti-scientific system of review and publication that dates [back to the 17th century](https://www.npr.org/sections/health-shots/2018/02/24/586184355/scientists-aim-to-pull-peer-review-out-of-the-17th-century).  Consider the fact that you could blog about your research while conducting it, present preliminary results in your blog via Jupyter Notebook, get regular feedback from peers along the way via your site's comment system, and all this before you'd ever send it off to a journal.  Now ask yourself what a print-oriented journal actually offers you? When was the last time you actually opened a print version of a journal? How often do you go to a journal site to look for something as opposed to a simple web search or using something like Google Scholar?  How many journals do adequate retractions when problems are found? Is it possible you may actually get more eyeballs and clicks on your work just having it on your own website or [tweeting about it](https://www.altmetric.com/about-altmetrics/what-are-altmetrics/)?\n",
    "\n",
    "The current paradigm is going to change because it has to, and there is practically no justification for the traditional approach to academic publication. Indeed, the change has already been well underway, with some outlets requiring pre-registration, code, and other changes to the usual send-a-pdf-and-we'll-get-back-to-you approach. In non-academic settings, while there is the same sort of pushback there, even those used to print and powerpoints must admit they'd prefer an interactive document that works on their phone if needed.  As such, you might as well be using tools and an approach that accommodate the things we've talked about in order to produce a better data-driven product. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
