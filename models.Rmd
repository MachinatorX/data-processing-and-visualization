# (PART\*) Part IV: Modeling {-}

```{r progsetup, include=FALSE, eval=TRUE, cache=FALSE}
knitr::opts_chunk$set(eval=T, echo=T)

kable_df = function(data, digits=3, ...) {
  kableExtra::kable(
    data,
    digits = digits,
    format = 'html',
    booktabs = T,
    # longtable = F,
    linesep = "",
    ...,
  ) %>% 
    kableExtra::kable_styling(full_width = F)
}

perc = function(x, digits = 1) paste(rnd(x*100, digits = digits), '%')
```

# Model Exploration

The following shows how to get started with modeling in R generally, with a focus on concepts, tools, and syntax, rather than trying to understand the specifics of a given model.

## Model Taxonomy

We can broadly describe two classes of models

- *Supervised*
- *Unsupervised*
- Some combination

For supervised settings, there is a target or set of target variables which we aim to predict with a set of predictor variables or covariates.  This is far and away the most common case, and the one we will focus on here.

In the case of unsupervised models, the data is the target, and includes techniques such as principal components analyis, factor analysis, cluster analytic approaches, topic modeling, and many others. The key notion is dimension reduction, either of the columns or rows.  For example, we may have many items of a survey we wish to group together into a few key concepts, or cluster thousands of observations into a few simple categories.


Most statistical modeling techniques use maximum likelihood in some form or fashion, including 


## Fitting Models

The two components required to fit a model in R with practically every modern package are the model formula and a data frame.  Consider the following models, in general the syntax is the same or identical, with special considerations for the type of model. The data argument is not included.

```{r model-syntax, eval=FALSE}
lm(y ~ x + z)                                        # standard linear model/OLS
glm(y ~ x + z, family = 'binomial')                  # logistic regression with binary response
glm(y ~ x + z + offset(log(q)), family = 'poisson')  # count/rate model
pscl::hurdle(y ~ x + z, dist = "negbin")             # hurdle model with negative binomial response
lme4::glmer(y ~ x + (1|group), family = 'binomial')  # generalized linear mixed model
mgcv::gam(y ~ s(x))                                  # generalized additive model
survival::coxph(Surv(time = t, event = q) ~ x)       # Cox Proportional Hazards Regression

# Bayesian
brms::brm(
  y ~ x + (1 + x | group), 
  family = 'zero_one_inflated_beta', 
  prior = priors
)
```

Let's run an example. We'll use the world happiness dataset[^happy]. This is country level data based on surveys taken at various years, and the scores are averages or proportions, along with other values like GDP.  

```{r model-prep, echo=1:5}
library(tidyverse)  # load if you haven't already

load('data/world_happiness.RData')

# glimpse(happy)

tidyext::describe_all_num(happy) %>% 
  kable_df()

happy_score = tidyext::num_summary(happy$happiness_score)
```


The happiness score itself ranges from `r happy_score$Min` to `r happy_score$Max`, with a mean of `r happy_score$Mean` and standard deviation of `r happy_score$SD`.


Fitting a model with R is trivial, and at a minimum requires the two key ingredients mentioned before, the formula and data.

```{r model-fit}
happy_model_0 = lm(
  happiness_score ~ democratic_quality + generosity + log_gdp_per_capita,
  data = happy
)
```


### Matrix

Many packages still allow for, or even require (but shouldn't), matrices instead of specifying a model formula.  This requires separating data into a model or design matrix, and the vector or matrix of the target variable(s).  For example, if we needed a speed boost and weren't concerned about some typical output we could use <span class="func" style = "">lm.fit</span>.

First we need to create the required components.

```{r model-matrix}
X = model.matrix(
  happiness_score ~ democratic_quality + generosity + log_gdp_per_capita, 
  data = happy
)

head(X)
```

Note the column of ones in the model matrix `X`.  This represents our intercept, but won't mean much to you unless you understant matrix multiplication.  The other columns are just as they are in the data.  Note also that the missing values have been removed.


```{r na-omit}
nrow(happy)
nrow(X)
```


We can now fit the model as follows.  The response variable must contain the same values as in the model matrix.

```{r model-fit-model-matrix}
y = happy$happiness_score[as.integer(rownames(X))]
happy_model_matrix = lm.fit(X, y)
summary(happy_model_matrix)
```

In my experience, it is generally a bad sign if a package requires that you create the model matrix rather than doing so itself.  Such packages tend to skip out on many other things like using typical methods and so forth, making them even more difficult to work with.  In general, the only real time you should need to use model matrices is when you are creating your own modeling package, doing simulations, or otherwise know why you need them.


## Summarizing Models

```{r}
summary(happy_model_0)
```

The summary tells us that all these covariates are statistically significant, which doesn't tell us much, but serves as a starting point.  For example, moving one unit on 

## Scaling 

## Variable Importance

## Extracting Output

## Factor Variables

## Visualization


[^happy]: The World Happiness Report is a survey of the state of global happiness that ranks countries by how happy their citizens perceive themselves to be. Almost all the information here is gleaned from the report and appendices. This regards the report data from 2008-2018 included in the 2019 report.